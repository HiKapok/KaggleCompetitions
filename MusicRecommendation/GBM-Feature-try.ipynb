{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "get_ipython().magic('env CUDA_VISIBLE_DEVICES =  ')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp      # will come in handy due to the size of the data\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import io\n",
    "from datetime import datetime\n",
    "import gc # garbage collector\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import logging\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a pandas dataframe to disk as gunzip compressed csv\n",
    "- df.to_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Read from disk\n",
    "- df = pd.read_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Magic useful\n",
    "- %%timeit for the whole cell\n",
    "- %timeit for the specific line\n",
    "- %%latex to render the cell as a block of latex\n",
    "- %prun and %%prun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/WSDM/'\n",
    "HDF_FILENAME = DATASET_PATH + 'datas.h5'\n",
    "HDF_FILENAME_TEMPSAVE = DATASET_PATH + 'datas_temp.h5'\n",
    "SUBMISSION_FILENAME = DATASET_PATH + 'submission_{}.csv'\n",
    "VALIDATION_INDICE = DATASET_PATH + 'validation_indice.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_logging(logger_name, logger_file_name):\n",
    "    log = logging.getLogger(logger_name)\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create formatter and add it to the handlers\n",
    "    print_formatter = logging.Formatter('%(message)s')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s_%(levelname)s: %(message)s')\n",
    "\n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(logger_file_name, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(file_formatter)\n",
    "    log.addHandler(fh)\n",
    "    # both output to console and file\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(print_formatter)\n",
    "    log.addHandler(consoleHandler)\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = set_logging('MUSIC', DATASET_PATH + 'music_gbm.log')\n",
    "log.info('here is an info message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN_FILE = DATASET_PATH + 'train.csv'\n",
    "# TEST_FILE = DATASET_PATH + 'test.csv'\n",
    "# MEMBER_FILE = DATASET_PATH + 'members.csv'\n",
    "# SONG_FILE = DATASET_PATH + 'fix_songs.csv'\n",
    "# SONG_EXTRA_FILE = DATASET_PATH + 'song_extra_info.csv'\n",
    "\n",
    "# train_data = pd.read_csv(TRAIN_FILE)\n",
    "# test_data = pd.read_csv(TEST_FILE)\n",
    "# member_data = pd.read_csv(MEMBER_FILE)\n",
    "# song_data = pd.read_csv(SONG_FILE)\n",
    "# song_extra_data = pd.read_csv(SONG_EXTRA_FILE)\n",
    "\n",
    "# songs_all = pd.merge(left = song_data, right = song_extra_data, how = 'left', on='song_id')\n",
    "# train_with_mem = pd.merge(left = train_data, right = member_data, how = 'left', on='msno')\n",
    "# train_all = pd.merge(left = train_with_mem, right = songs_all, how = 'left', on='song_id')\n",
    "# test_with_mem = pd.merge(left = test_data, right = member_data, how = 'left', on='msno')\n",
    "# test_all = pd.merge(left = test_with_mem, right = songs_all, how = 'left', on='song_id')\n",
    "# del train_with_mem, test_with_mem; gc.collect()\n",
    "\n",
    "# def convert_unicode_to_str(df):\n",
    "#     df.columns = df.columns.astype(str)\n",
    "#     types = df.apply(lambda x: pd.api.types.infer_dtype(df.values))\n",
    "#     #print(types)#mixed-integer\n",
    "#     for col in types[types == 'mixed-integer'].index:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     for col in types[types == 'mixed'].index:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     return df\n",
    "\n",
    "# store = pd.HDFStore(HDF_FILENAME)\n",
    "# store['train_data'] = convert_unicode_to_str(train_all)\n",
    "# store['test_data'] = convert_unicode_to_str(test_all)\n",
    "# store['song_data'] = convert_unicode_to_str(songs_all)\n",
    "# store['test_id'] = test_data.id\n",
    "# store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store_test = pd.HDFStore(HDF_FILENAME)\n",
    "# train = store_test['train_data'][0:100]\n",
    "# test = store_test['test_data'][0:100]\n",
    "# test_id =  store_test['test_id'][0:100]\n",
    "# store_test.close()\n",
    "store_test = pd.HDFStore(HDF_FILENAME)\n",
    "train = store_test['train_data']\n",
    "test = store_test['test_data']\n",
    "test_id =  store_test['test_id']\n",
    "store_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_country(input_data):\n",
    "    def get_country(isrc):\n",
    "        if isinstance(isrc, str) and isrc != 'nan':\n",
    "            return isrc[0:2]\n",
    "        else:\n",
    "            return np.nan\n",
    "    countries = train['isrc'].apply(get_country)\n",
    "    country_list = list(countries.value_counts().index)\n",
    "    country_map = dict(zip(country_list, country_list))\n",
    "    country_map['QM'] = 'QZ'\n",
    "    country_map['US'] = 'QZ'\n",
    "    return countries.map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['country'] = split_country(train)\n",
    "test['country'] = split_country(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if isinstance(isrc, str) and isrc != 'nan':\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "train['song_year'] = train['isrc'].apply(isrc_to_year)\n",
    "test['song_year'] = test['isrc'].apply(isrc_to_year)\n",
    "train.drop(['isrc'], axis = 1, inplace = True)\n",
    "test.drop(['isrc'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_reg_date(input_data):\n",
    "    input_data['registration_year'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[0:4]))\n",
    "    input_data['registration_year'] = pd.to_numeric(input_data['registration_year'], downcast='unsigned')\n",
    "\n",
    "    input_data['registration_month'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[4:6]))\n",
    "    input_data['registration_month'] = pd.to_numeric(input_data['registration_month'], downcast='unsigned')\n",
    "\n",
    "    input_data['registration_day'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[6:8]))\n",
    "    input_data['registration_day'] = pd.to_numeric(input_data['registration_day'], downcast='unsigned')\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_expir_date(input_data):\n",
    "    input_data['expiration_year'] = input_data['expiration_date'].apply(lambda x : int(str(x)[0:4]))\n",
    "    input_data['expiration_year'] = pd.to_numeric(input_data['expiration_year'], downcast='unsigned')\n",
    "\n",
    "    input_data['expiration_month'] = input_data['expiration_date'].apply(lambda x : int(str(x)[4:6]))\n",
    "    input_data['expiration_month'] = pd.to_numeric(input_data['expiration_month'], downcast='unsigned')\n",
    "\n",
    "    input_data['expiration_day'] = input_data['expiration_date'].apply(lambda x : int(str(x)[6:8]))\n",
    "    input_data['expiration_day'] = pd.to_numeric(input_data['expiration_day'], downcast='unsigned')\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_to_day(input_data):\n",
    "    # 转换注册时间\n",
    "    input_data['registration_init_time'] = pd.to_datetime(input_data['registration_init_time'],format=\"%Y%m%d\")\n",
    "    input_data['expiration_date'] = pd.to_datetime(input_data['expiration_date'],format=\"%Y%m%d\")\n",
    "    days = input_data.expiration_date - input_data.registration_init_time\n",
    "    days = [d.days for d in days]\n",
    "    input_data['days']=days\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = split_reg_date(train)\n",
    "test = split_reg_date(test)\n",
    "train = split_expir_date(train)\n",
    "test = split_expir_date(test)\n",
    "\n",
    "train = date_to_day(train)\n",
    "test = date_to_day(test)\n",
    "\n",
    "train.drop('registration_init_time',axis=1,inplace=True)\n",
    "train.drop('expiration_date',axis=1,inplace=True)\n",
    "test.drop('registration_init_time',axis=1,inplace=True)\n",
    "test.drop('expiration_date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['song_length'] = pd.to_numeric(train['song_length'].replace('nan', '235415'), downcast='unsigned')\n",
    "test['song_length'] = pd.to_numeric(test['song_length'].replace('nan', '235415'), downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in train.columns: print(col, ':', train[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in [col for col in test.columns if col != 'id' ]:\n",
    "    if train[col].dtype == object:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode registered_via, the less number of occurrences are merged into the top item which has the max number of occurrences\n",
    "# registered_via_hist = pd.concat([train['registered_via'], test['registered_via']], axis = 0).value_counts()\n",
    "# registered_via_map = dict(zip(registered_via_hist.index, [int(s) for s in registered_via_hist.index.values]))\n",
    "# registered_via_map[registered_via_hist.index[-1]] = int(str(registered_via_hist.index.values[0]))\n",
    "# train['registered_via'] = train['registered_via'].map(registered_via_map)\n",
    "# test['registered_via'] = test['registered_via'].map(registered_via_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode language, fill nan with most occurrences item\n",
    "# language_hist = pd.concat([train['language'], test['language']], axis = 0).value_counts()\n",
    "# language_map = dict(zip(language_hist.index, [int(float(s)) for s in language_hist.index.values if s != 'nan']))\n",
    "# language_map['nan'] = int(float(str(language_hist.index.values[0])))\n",
    "# train['language'] = train['language'].map(language_map)\n",
    "# test['language'] = test['language'].map(language_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode country, fill nan with most occurrences item\n",
    "# country_hist = pd.concat([train['country'], test['country']], axis = 0).value_counts()\n",
    "# merge_per = 0.25\n",
    "# country_map = dict(zip(country_hist.index, list(range(len(country_hist)))))\n",
    "# for key in list(country_hist[-int(len(country_hist)*merge_per):].index):\n",
    "#     country_map[key] = int(len(country_hist)*(1-merge_per)) + 1\n",
    "# train['country'] = train['country'].map(country_map)\n",
    "# test['country'] = test['country'].map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# msno : category ; uinque values: 30755\n",
    "# song_id : category ; uinque values: 359966\n",
    "# - source_system_tab : category ; uinque values: 10\n",
    "# - source_screen_name : category ; uinque values: 21\n",
    "# - source_type : category ; uinque values: 13\n",
    "# - target : object ; uinque values: 2\n",
    "# - city : category ; uinque values: 21\n",
    "# - bd : category ; uinque values: 92\n",
    "# - gender : category ; uinque values: 3\n",
    "# - registered_via : category ; uinque values: 5\n",
    "# song_length : uint32 ; uinque values: 60271\n",
    "# genre_ids : category ; uinque values: 573\n",
    "# artist_name : category ; uinque values: 40587\n",
    "# composer : category ; uinque values: 76072\n",
    "# lyricist : category ; uinque values: 33895\n",
    "# - language : category ; uinque values: 11\n",
    "# name : category ; uinque values: 234144\n",
    "# - country : category ; uinque values: 107\n",
    "# - song_year : float64 ; uinque values: 100\n",
    "# - registration_year : uint16 ; uinque values: 14\n",
    "# - registration_month : uint8 ; uinque values: 12\n",
    "# - registration_date : uint8 ; uinque values: 31\n",
    "# - expiration_year : uint16 ; uinque values: 18\n",
    "# - expiration_month : uint8 ; uinque values: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_transform(input_train_data, input_test_data, columns_to_transform):\n",
    "    for col in columns_to_transform:\n",
    "        le = LabelEncoder()\n",
    "        train_values = list(input_train_data[col].unique())\n",
    "        test_values = list(input_test_data[col].unique())\n",
    "        le.fit(train_values + test_values)\n",
    "        input_train_data[col] = le.transform(input_train_data[col])\n",
    "        input_test_data[col] = le.transform(input_test_data[col])\n",
    "    return input_train_data, input_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train, test = one_hot_transform(train, test, ['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender', 'name'])#, 'artist_name', 'composer', 'lyricist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: wether song_id should be merged like this or not? 231475 reserved and 188364 merged\n",
    "def encode_with_merge(input_train, input_test, columns, merge_value):\n",
    "    for index, col in enumerate(columns):\n",
    "        values_hist = pd.concat([input_train[col], input_train[col]], axis = 0).value_counts()\n",
    "        reserve_rows = values_hist[values_hist!=merge_value[index]]\n",
    "        merge_rows = values_hist[values_hist==merge_value[index]]\n",
    "\n",
    "        reserve_dict = dict(zip(list(reserve_rows.index), list(range(len(reserve_rows)))))\n",
    "        merge_dict = dict(zip(list(merge_rows.index), [len(reserve_rows)+1]*len(merge_rows.index)))\n",
    "        \n",
    "        map_dict = {**reserve_dict, **merge_dict}\n",
    "        \n",
    "        language_map['nan'] = int(float(str(language_hist.index.values[0])))\n",
    "        input_train[col] = input_train[col].map(map_dict)\n",
    "        input_test[col] = input_test[col].map(map_dict)\n",
    "    return input_train, input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train, test = encode_with_merge(train, test, ['msno', 'song_id', 'genre_ids'], [1, 1, 1])\n",
    "# print(train.head())\n",
    "# print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_org, test_org = train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = train_org.copy(deep=True)\n",
    "#test = test_org.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_test = pd.HDFStore(VALIDATION_INDICE)\n",
    "validation_list = store_test['keep_index']['index'].values\n",
    "store_test.close()\n",
    "train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "#validation_use = train.iloc[validation_list].copy(deep=True).reset_index(drop=True)\n",
    "validation_use = train.iloc[list(range(7277417, 7377417))].copy(deep=True).reset_index(drop=True)\n",
    "#train_use = train.drop(validation_list)\n",
    "train_use = train.drop(list(range(7277417, 7377417)))\n",
    "# train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "# validation_use = train[50:].copy(deep=True).reset_index(drop=True)\n",
    "# train_use = train.drop(list(range(50,100)))\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_transform(train_data, validation_data, test_data):\n",
    "    train_data['song_length'] = np.log(pd.to_numeric(train_data['song_length'], downcast='float') + 1)\n",
    "    validation_data['song_length'] = np.log(pd.to_numeric(validation_data['song_length'], downcast='float') + 1)\n",
    "    test_data['song_length'] = np.log(pd.to_numeric(test_data['song_length'], downcast='float') + 1)\n",
    "    return train_data, validation_data, test_data\n",
    "train_use, validation_use, test = log_transform(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_composer_hot_rate(train_data, val_data, test_data):\n",
    "    \n",
    "    temp_data = pd.concat([train_data[['composer']], val_data[['composer']], test_data[['composer']]], axis=0, join=\"outer\")\n",
    "    temp_data['composer'] = temp_data['composer'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['composer'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['composer_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    temp_data = pd.concat([df_temp['composer_0'], df_temp['composer_1'], df_temp['composer_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "\n",
    "    composer_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    composer_hot['nan'] = 0.\n",
    "    composer_hot['nan'] = composer_hot.mean()\n",
    "    #print(composer_hot)\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['composer'] = input_data['composer'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['composer'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['composer_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['composer_0', 'composer_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_0')\n",
    "        hot_hist.columns = ['composer_1', 'composer_1_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_1')\n",
    "        hot_hist.columns = ['composer_2', 'composer_2_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_2')\n",
    "        df_temp['composer_score'] = df_temp[['composer_0_score','composer_1_score','composer_2_score']].max(axis=1)\n",
    "        #df_temp['composer_score'] = df_temp['composer_0_score']\n",
    "        \n",
    "        input_data['composer_score'] = df_temp['composer_score']\n",
    "        input_data.drop('composer', inplace=True, axis = 1)\n",
    "        #input_data = input_data.drop('composer', inplace=False, axis = 1)\n",
    "        input_data['composer'] = df_temp['composer_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, composer_hot)\n",
    "    val_data = encoder_each(val_data, composer_hot)\n",
    "    test_data = encoder_each(test_data, composer_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_composer_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_lyricist_hot_rate(train_data, val_data, test_data):\n",
    "    temp_data = pd.concat([train_data[['lyricist']], val_data[['lyricist']], test_data[['lyricist']]], axis=0, join=\"outer\")\n",
    "    temp_data['lyricist'] = temp_data['lyricist'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['lyricist'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['lyricist_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    #temp_data = df_temp['lyricist_0']\n",
    "    temp_data = pd.concat([df_temp['lyricist_0'], df_temp['lyricist_1'], df_temp['lyricist_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "    lyricist_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    lyricist_hot['nan'] = 0.\n",
    "    lyricist_hot['nan'] = lyricist_hot.mean()\n",
    "\n",
    "    #print(lyricist_hot)\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['lyricist'] = input_data['lyricist'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['lyricist'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['lyricist_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['lyricist_0', 'lyricist_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='lyricist_0')\n",
    "        \n",
    "        df_temp['lyricist_score'] = df_temp['lyricist_0_score']\n",
    "        \n",
    "        input_data['lyricist_score'] = df_temp['lyricist_score']\n",
    "        input_data.drop('lyricist', inplace=True, axis = 1)\n",
    "        input_data['lyricist'] = df_temp['lyricist_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, lyricist_hot)\n",
    "    val_data = encoder_each(val_data, lyricist_hot)\n",
    "    test_data = encoder_each(test_data, lyricist_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_lyricist_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_artist_hot_rate(train_data, val_data, test_data):\n",
    "    temp_data = pd.concat([train_data[['artist_name']], val_data[['artist_name']], test_data[['artist_name']]], axis=0, join=\"outer\")\n",
    "    temp_data['artist_name'] = temp_data['artist_name'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['artist_name'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['artist_name_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    #temp_data = df_temp['artist_name_0']\n",
    "    temp_data = pd.concat([df_temp['artist_name_0'], df_temp['artist_name_1'], df_temp['artist_name_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "    artist_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    artist_hot['nan'] = 0.\n",
    "    artist_hot['nan'] = artist_hot.mean()\n",
    "    #print(artist_hot)\n",
    "\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['artist_name'] = input_data['artist_name'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['artist_name'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['artist_name_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['artist_name_0', 'artist_name_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='artist_name_0')\n",
    "        \n",
    "        df_temp['artist_name_score'] = df_temp['artist_name_0_score']\n",
    "        \n",
    "        input_data['artist_name_score'] = df_temp['artist_name_score']\n",
    "        input_data.drop('artist_name', inplace=True, axis = 1)\n",
    "        input_data['artist_name'] = df_temp['artist_name_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, artist_hot)\n",
    "    val_data = encoder_each(val_data, artist_hot)\n",
    "    test_data = encoder_each(test_data, artist_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_artist_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_use.head().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp_data = pd.concat([train_use[['composer']], validation_use[['composer']], test[['composer']]], axis=0, join=\"inner\")\n",
    "\n",
    "# temp_data['composer'].apply(lambda x : len(x.replace(u'、','|').split('|'))).value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = train_use.head(100000).copy(deep=True)\n",
    "# print(df['lyricist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_genre_hot_rate(train_data, val_data, test_data):\n",
    "    # 0.685742\n",
    "#     temp_data = pd.concat([train_data[['genre_ids']], val_data[['genre_ids']], test_data[['genre_ids']]], axis=0, join=\"outer\")\n",
    "#     temp_data.reset_index(drop=True)\n",
    "\n",
    "#     genre_hot = np.log(temp_data['genre_ids'].value_counts()+1)\n",
    "#     #composer_hot = temp_data.value_counts()\n",
    "#     genre_hot['nan'] = 0.\n",
    "#     genre_hot['nan'] = genre_hot.mean()\n",
    "#     #print(composer_hot)\n",
    "#     genre_hot = genre_hot.reset_index()\n",
    "#     genre_hot.index.name='index'\n",
    "    \n",
    "#     genre_hot.columns = ['genre_ids', 'genre_ids_score']\n",
    "#     genre_hot['num_genre'] = genre_hot['genre_ids'].apply(lambda x : len(x.split('|')))\n",
    "#     train_data = train_data.merge(right = genre_hot, how = 'left', on='genre_ids')\n",
    "#     test_data = test_data.merge(right = genre_hot, how = 'left', on='genre_ids')\n",
    "#     val_data = val_data.merge(right = genre_hot, how = 'left', on='genre_ids')\n",
    "#     return train_data, test_data, val_data\n",
    "#     def encoder_each(input_data, hot_hist):\n",
    "#         input_data = input_data.copy()\n",
    "#         input_data['composer'] = input_data['genre_ids'].apply(lambda x : x.replace(u'、','|'))\n",
    "#         df_temp = input_data['composer'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "#         df_temp.columns = ['composer_{}'.format(x) for x in df_temp.columns]\n",
    "#         hot_hist = hot_hist.reset_index()\n",
    "#         hot_hist.index.name='index'\n",
    "        \n",
    "#         hot_hist.columns = ['composer_0', 'composer_0_score']\n",
    "#         df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_0')\n",
    "#         hot_hist.columns = ['composer_1', 'composer_1_score']\n",
    "#         df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_1')\n",
    "#         hot_hist.columns = ['composer_2', 'composer_2_score']\n",
    "#         df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_2')\n",
    "#         df_temp['composer_score'] = df_temp[['composer_0_score','composer_1_score','composer_2_score']].max(axis=1)\n",
    "#         #df_temp['composer_score'] = df_temp['composer_0_score']\n",
    "        \n",
    "#         input_data['composer_score'] = df_temp['composer_score']\n",
    "#         input_data.drop('genre_id', inplace=True, axis = 1)\n",
    "#         #input_data = input_data.drop('composer', inplace=False, axis = 1)\n",
    "#         input_data['genre_id'] = df_temp['composer_0']\n",
    "#         return input_data\n",
    "#     train_data = encoder_each(train_data, genre_hot)\n",
    "#     val_data = encoder_each(val_data, genre_hot)\n",
    "#     val_data = encoder_each(test_data, genre_hot)\n",
    "    # 0.684454 with bd filled\n",
    "    temp_data = pd.concat([train_data[['genre_ids']], val_data[['genre_ids']], test_data[['genre_ids']]], axis=0, join=\"outer\")\n",
    "\n",
    "    df_temp = temp_data['genre_ids'].str.split('\\s{0,}\\|\\s{0,}', 3, expand=True)\n",
    "    \n",
    "    df_temp.columns = ['genre_ids_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    temp_data = pd.concat([df_temp['genre_ids_0'], df_temp['genre_ids_1'], df_temp['genre_ids_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "\n",
    "    genre_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    genre_hot['nan'] = 0.\n",
    "    genre_hot['nan'] = genre_hot.mean()\n",
    "    #print(composer_hot)\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        df_temp = input_data['genre_ids'].str.split('\\s{0,}\\|\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['genre_ids_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name = 'index'\n",
    "        \n",
    "        hot_hist.columns = ['genre_ids_0', 'genre_ids_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='genre_ids_0')\n",
    "        hot_hist.columns = ['genre_ids_1', 'genre_ids_1_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='genre_ids_1')\n",
    "        hot_hist.columns = ['genre_ids_2', 'genre_ids_2_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='genre_ids_2')\n",
    "        df_temp['genre_ids_score'] = df_temp[['genre_ids_0_score','genre_ids_1_score','genre_ids_2_score']].max(axis=1)\n",
    "        idx_max = df_temp[['genre_ids_0_score','genre_ids_1_score','genre_ids_2_score']].idxmax(axis=1)\n",
    "        idx_max = idx_max.apply(lambda x : x.strip('_score'))\n",
    "        #print(idx_max)\n",
    "        #print(df_temp[['genre_ids_0','genre_ids_1','genre_ids_2']])\n",
    "        #print(df_temp[['genre_ids_0','genre_ids_1','genre_ids_2']].lookup(idx_max.index, idx_max.values))\n",
    "        \n",
    "        #return input_data\n",
    "        #df_temp['genre_ids_score'] = df_temp['genre_ids_0_score']\n",
    "        \n",
    "        input_data['genre_ids_score'] = df_temp['genre_ids_score']\n",
    "        #input_data.drop('genre_ids', inplace=True, axis = 1)\n",
    "        #input_data = input_data.drop('genre_ids', inplace=False, axis = 1)\n",
    "        input_data['genre_ids_popular'] = input_data['genre_ids'].apply(lambda x : len(x.split('|')))\n",
    "        input_data['genre_ids'] = df_temp[['genre_ids_0','genre_ids_1','genre_ids_2']].lookup(idx_max.index, idx_max.values)\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, genre_hot)\n",
    "    val_data = encoder_each(val_data, genre_hot)\n",
    "    test_data = encoder_each(test_data, genre_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_genre_hot_rate(train_use, validation_use, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def catogory_encode_transform(train_data, val_data, test_data, col):\n",
    "    gc.collect()\n",
    "    temp_data = pd.concat([train_data, val_data, test_data], axis=0, join=\"outer\")\n",
    "    gc.collect()\n",
    "    all_values = list(temp_data[col].unique())\n",
    "    gc.collect()\n",
    "    map_dict = dict(zip(all_values, [i for i in range(len(all_values))]))\n",
    "    gc.collect()\n",
    "#     train_data[col] = train_data[col].map(map_dict)\n",
    "#     val_data[col] = val_data[col].map(map_dict)\n",
    "#     test_data[col] = test_data[col].map(map_dict)\n",
    "    return train_data[col].map(map_dict), val_data[col].map(map_dict), test_data[col].map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = 'msno'\n",
    "train_use[[col]], validation_use[[col]], test[[col]] = catogory_encode_transform(train_use[[col]], validation_use[[col]], test[[col]], col)\n",
    "gc.collect()\n",
    "col = 'song_id'\n",
    "train_use[[col]], validation_use[[col]], test[[col]] = catogory_encode_transform(train_use[[col]], validation_use[[col]], test[[col]], col)\n",
    "gc.collect()\n",
    "col = 'name'\n",
    "train_use[[col]], validation_use[[col]], test[[col]] = catogory_encode_transform(train_use[[col]], validation_use[[col]], test[[col]], col)\n",
    "gc.collect()\n",
    "col = 'composer'\n",
    "train_use[[col]], validation_use[[col]], test[[col]] = catogory_encode_transform(train_use[[col]], validation_use[[col]], test[[col]], col)\n",
    "gc.collect()\n",
    "col = 'lyricist'\n",
    "train_use[[col]], validation_use[[col]], test[[col]] = catogory_encode_transform(train_use[[col]], validation_use[[col]], test[[col]], col)\n",
    "gc.collect()\n",
    "col = 'artist_name'\n",
    "train_use[[col]], validation_use[[col]], test[[col]] = catogory_encode_transform(train_use[[col]], validation_use[[col]], test[[col]], col)\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "for col in ['source_system_tab', 'source_screen_name', 'source_type', 'country']:\n",
    "    train_use[[col]], validation_use[[col]], test[[col]] = catogory_encode_transform(train_use[[col]], validation_use[[col]], test[[col]], col)\n",
    "    gc.collect()\n",
    "\n",
    "for col in ['city', 'registered_via', 'genre_ids', 'language']:\n",
    "    train_use[[col]], validation_use[[col]], test[[col]] = catogory_encode_transform(train_use[[col]], validation_use[[col]], test[[col]], col)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#time_wnd = [2018, 0]\n",
    "time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "def cal_song_listen_times(train_data, val_data, test_data):\n",
    "    all_data = pd.concat([train_data[['song_id', 'song_year', 'msno']], val_data[['song_id', 'song_year', 'msno']], test_data[['song_id', 'song_year', 'msno']]], axis=0, join=\"inner\")\n",
    "    #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "    #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "    for index, _ in enumerate(time_wnd[:-1]):\n",
    "        begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "#         begin_time = time_wnd[index]\n",
    "#         end_time = time_wnd[index+1]\n",
    "        select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "        #select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "        grouped = select_data[['song_id', 'msno']].groupby(['song_id'])\n",
    "\n",
    "        count_song = grouped.agg(['count'])\n",
    "        num_people_per_song = grouped.agg({\"msno\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "        popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "        popularity.columns = ['popular_{}'.format(index), 'num_people_{}'.format(index)]\n",
    "        popularity = popularity.reset_index(drop=False)\n",
    "        train_data = train_data.merge(popularity, on='song_id', how ='left')\n",
    "        test_data = test_data.merge(popularity, on='song_id', how ='left')\n",
    "        val_data = val_data.merge(popularity, on='song_id', how ='left')\n",
    "    return train_data, val_data, test_data\n",
    "def cal_song_listen_times_seperate(train_data, val_data, test_data):\n",
    "    def cal_each_of_them(input_data):\n",
    "        all_data = input_data[['song_id', 'song_year', 'msno']]\n",
    "        #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "        #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "        for index, _ in enumerate(time_wnd[:-1]):\n",
    "            begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "    #         begin_time = time_wnd[index]\n",
    "    #         end_time = time_wnd[index+1]\n",
    "            select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "            grouped = select_data[['song_id', 'msno']].groupby(['song_id'])\n",
    "\n",
    "            count_song = grouped.agg(['count'])\n",
    "            num_people_per_song = grouped.agg({\"msno\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "            popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "            popularity.columns = ['popular_{}'.format(index), 'num_people_{}'.format(index)]\n",
    "            popularity = popularity.reset_index(drop=False)\n",
    "            all_data = input_data.merge(popularity, on='song_id', how ='left')\n",
    "        return all_data\n",
    "    return cal_each_of_them(train_data), cal_each_of_them(val_data), cal_each_of_them(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_song_listen_times(train_use, validation_use, test)\n",
    "gc.collect()\n",
    "# train = cal_song_listen_times(train)\n",
    "# test = cal_song_listen_times(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#people_time_wnd = [2018, 0]\n",
    "people_time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "def get_people_active(train_data, val_data, test_data):\n",
    "    all_data = pd.concat([train_data[['song_id', 'song_year', 'msno']], val_data[['song_id', 'song_year', 'msno']], test_data[['song_id', 'song_year', 'msno']]], axis=0, join=\"inner\")\n",
    "    #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "    #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "    for index, _ in enumerate(people_time_wnd[:-1]):\n",
    "        begin_time, end_time = people_time_wnd[index] < people_time_wnd[index+1] and (people_time_wnd[index], people_time_wnd[index+1]) or (people_time_wnd[index+1], people_time_wnd[index])\n",
    "#         begin_time = time_wnd[index]\n",
    "#         end_time = time_wnd[index+1]\n",
    "        select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "        #select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "        grouped = select_data[['song_id', 'msno']].groupby(['msno'])\n",
    "\n",
    "        count_song = grouped.agg(['count'])\n",
    "        num_people_per_song = grouped.agg({\"song_id\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "        popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "        popularity.columns = ['active_{}'.format(index), 'num_song_{}'.format(index)]\n",
    "        popularity = popularity.reset_index(drop=False)\n",
    "        train_data = train_data.merge(popularity, on='msno', how ='left')\n",
    "        test_data = test_data.merge(popularity, on='msno', how ='left')\n",
    "        val_data = val_data.merge(popularity, on='msno', how ='left')\n",
    "    return train_data, val_data, test_data\n",
    "def get_people_active_seperate(train_data, val_data, test_data):\n",
    "    def cal_each_of_them(input_data):\n",
    "        all_data = input_data[['song_id', 'song_year', 'msno']]\n",
    "        #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "        #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "        for index, _ in enumerate(people_time_wnd[:-1]):\n",
    "            begin_time, end_time = people_time_wnd[index] < people_time_wnd[index+1] and (people_time_wnd[index], people_time_wnd[index+1]) or (people_time_wnd[index+1], people_time_wnd[index])\n",
    "    #         begin_time = time_wnd[index]\n",
    "    #         end_time = time_wnd[index+1]\n",
    "            select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "            grouped = select_data[['song_id', 'msno']].groupby(['msno'])\n",
    "\n",
    "            count_song = grouped.agg(['count'])\n",
    "            num_people_per_song = grouped.agg({\"song_id\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "            popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "            popularity.columns = ['active_{}'.format(index), 'num_song_{}'.format(index)]\n",
    "            popularity = popularity.reset_index(drop=False)\n",
    "            all_data = input_data.merge(popularity, on='msno', how ='left')\n",
    "        return all_data\n",
    "    return cal_each_of_them(train_data), cal_each_of_them(val_data), cal_each_of_them(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use, validation_use, test = get_people_active(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_by_different_city_lang_country(train_data, val_data, test_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']], val_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']], test_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "    count_dict = dict()\n",
    "    \n",
    "    for col in ['composer', 'lyricist', 'artist_name']:\n",
    "        temp_df = None\n",
    "        for target in ['city', 'country', 'language']:\n",
    "            grouped = temp_msno_songid.groupby([col])\n",
    "            df = grouped.agg({target: lambda x: x.nunique()})\n",
    "            # !!!!!!remove log here!!!!!\n",
    "            df = np.log(df+1)\n",
    "#             print(df)\n",
    "#             break\n",
    "            if temp_df is not None:\n",
    "                temp_df = pd.concat([temp_df, df[target]], axis=1, join=\"inner\")\n",
    "            else:\n",
    "                temp_df = df\n",
    "        temp_df = temp_df.reset_index()\n",
    "        temp_df.index.name='index'\n",
    "        \n",
    "        temp_df.columns = [col, *[col + '_by_' + col_name for col_name in ['city', 'country', 'language']]]\n",
    "        #print(temp_df)    \n",
    "        train_data = train_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        test_data = test_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        val_data = val_data.merge(right = temp_df, how = 'left', on=col)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_use_org, test_org, validation_use_org = measure_by_different_city_lang_country(train_use, test, validation_use)\n",
    "train_use, validation_use, test = measure_by_different_city_lang_country(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_use, test, validation_use = train_use_org.copy(deep=True), test_org.copy(deep=True), validation_use_org.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def song_msno_by_different_city_lang_country(train_data, val_data, test_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['song_id', 'msno', 'city', 'country', 'language']], val_data[['song_id', 'msno', 'city', 'country', 'language']], test_data[['song_id', 'msno', 'city', 'country', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "    count_dict = dict()\n",
    "    \n",
    "    for col in ['song_id', 'msno']:\n",
    "        temp_df = None\n",
    "        if col == 'song_id':\n",
    "            target_list = ['city']\n",
    "        else:\n",
    "            target_list = ['country', 'language']\n",
    "        for target in target_list:\n",
    "            grouped = temp_msno_songid.groupby([col])\n",
    "            df = grouped.agg({target: lambda x: x.nunique()})\n",
    "            # !!!!!!remove log here!!!!!\n",
    "            df = np.log(df+1)\n",
    "            #print(df)\n",
    "#             break\n",
    "            if temp_df is not None:\n",
    "                temp_df = pd.concat([temp_df, df[target]], axis=1, join=\"inner\")\n",
    "            else:\n",
    "                temp_df = df\n",
    "        temp_df = temp_df.reset_index()\n",
    "        temp_df.index.name='index'\n",
    "        \n",
    "        temp_df.columns = [col, *[col + '_by_' + col_name for col_name in target_list]]\n",
    "        #print(temp_df)    \n",
    "        train_data = train_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        test_data = test_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        val_data = val_data.merge(right = temp_df, how = 'left', on=col)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = song_msno_by_different_city_lang_country(train_use, validation_use, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_use['genre_ids'].apply(lambda x : len(x.split('|'))).value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_use.to_csv(DATASET_PATH + 'temp_train.csv', index = False)\n",
    "# validation_use.to_csv(DATASET_PATH + 'temp_validation.csv', index = False)\n",
    "# test.to_csv(DATASET_PATH + 'temp_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_use = pd.read_csv(DATASET_PATH + 'temp_train.csv')\n",
    "# validation_use = pd.read_csv(DATASET_PATH + 'temp_validation.csv')\n",
    "# test = pd.read_csv(DATASET_PATH + 'temp_test.csv')\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_with_songid_msno(train_data, val_data, test_data):\n",
    "    for index, col in enumerate(['song_id', 'msno']):\n",
    "        temp_data = pd.concat([train_data[[col]], val_data[[col]], test_data[[col]]], axis=0, join=\"outer\")[col].value_counts()\n",
    "        encode_dict = dict(zip(list(temp_data.index), [str(i) for i in range(len(temp_data))]))\n",
    "        #reserve_dict = dict(zip([str(i) for i in range(len(temp_data))], list(temp_data.index)))\n",
    "        train_data[col] = train_data[col].map(encode_dict)\n",
    "        val_data[col] = val_data[col].map(encode_dict)\n",
    "        test_data[col] = test_data[col].map(encode_dict)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_use, validation_use, test = encode_with_songid_msno(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for col in [col for col in test.columns if col != 'id' ]:\n",
    "#     if test[col].dtype == object:\n",
    "#         train_use[col] = train_use[col].astype('category')\n",
    "#         validation_use[col] = validation_use[col].astype('category')\n",
    "#         test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for col in test.columns:\n",
    "#     if col not in ['song_length']:\n",
    "#         if test[col].dtype == np.float64:\n",
    "#             train_use[col] = train_use[col].astype(np.float32)\n",
    "#             validation_use[col] = validation_use[col].astype(np.float32)\n",
    "#             test[col] = test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_use.to_csv(DATASET_PATH + 'temp_train_all_comp_1.csv', index = False, compression='gzip')\n",
    "# print('train saved.')\n",
    "# validation_use.to_csv(DATASET_PATH + 'temp_validation_all_comp_1.csv', index = False, compression='gzip')\n",
    "# print('val saved.')\n",
    "# test.to_csv(DATASET_PATH + 'temp_test_all_comp_1.csv', index = False, compression='gzip')\n",
    "# print('test saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# float32_list = list()\n",
    "# for col in test.columns:\n",
    "#     if col not in ['song_length', 'id']:\n",
    "#         if test[col].dtype == np.float32:\n",
    "#             float32_list.append(col)\n",
    "# print(float32_list)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del train_use, validation_use, test\n",
    "# gc.collect()\n",
    "# float32_list = ['song_year', 'composer_score', 'lyricist_score', 'artist_name_score', 'popular_0', 'num_people_0', 'popular_1', 'num_people_1', 'popular_2', 'num_people_2', 'popular_3', 'num_people_3', 'popular_4', 'num_people_4', 'active_0', 'num_song_0', 'active_1', 'num_song_1', 'active_2', 'num_song_2', 'active_3', 'num_song_3', 'active_4', 'num_song_4', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'artist_name_by_city', 'artist_name_by_country', 'artist_name_by_language', 'song_id_by_city', 'msno_by_country', 'msno_by_language', 'genre_ids_score']\n",
    "# data_type_map =dict(zip(float32_list, [np.float32]*len(float32_list))) \n",
    "# train_use = pd.read_csv(DATASET_PATH + 'temp_train_all_comp_1.csv', compression='gzip', dtype = data_type_map)\n",
    "# validation_use = pd.read_csv(DATASET_PATH + 'temp_validation_all_comp_1.csv', compression='gzip', dtype = data_type_map)\n",
    "# test = pd.read_csv(DATASET_PATH + 'temp_test_all_comp_1.csv', compression='gzip', dtype = data_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_data_of_birth(train_data, val_data, test_data):\n",
    "    #temp_data = pd.concat([train_data[['bd']], val_data[['bd']], test_data[['bd']]], axis=0, join=\"outer\")\n",
    "    #mean_bd = temp_data['bd'].mean(axis=0)\n",
    "#     mean_bd = temp_data['bd'].replace(0, np.nan).mean(axis=0)\n",
    "#     train_data = train_data.fillna(value={'bd': mean_bd})\n",
    "#     val_data = val_data.fillna(value={'bd': mean_bd})\n",
    "#     test_data = test_data.fillna(value={'bd': mean_bd})\n",
    "    train_data['bd'] = train_data['bd'].replace(0, np.nan)\n",
    "    val_data['bd'] = val_data['bd'].replace(0, np.nan)\n",
    "    test_data['bd'] = test_data['bd'].replace(0, np.nan)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = fill_data_of_birth(train_use, validation_use, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wheteher is the most genre of a artist or song or msno\n",
    "def binary_encode_genre_ids(train_data, val_data, test_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['msno', 'artist_name', 'genre_ids']], val_data[['msno', 'artist_name', 'genre_ids']], test_data[['msno', 'artist_name', 'genre_ids']]], axis=0, join=\"outer\")\n",
    "    #df.groupby('id')['value'].nlargest(2)\n",
    "\n",
    "    mode = lambda x: x.mode() if len(x) > 2 else np.array(x)\n",
    "    for left, right in [('msno', 'artist_name'), ('msno', 'genre_ids'), ('artist_name', 'genre_ids'), ('artist_name', 'msno')]:\n",
    "        #grouped = temp_msno_songid.groupby(left)[right]#.agg(mode)\n",
    "        grouped = temp_msno_songid.groupby(left)[right].agg(lambda x: list(x.value_counts().index[0:3]))\n",
    "\n",
    "        #print(grouped.index)\n",
    "        grouped_split = pd.DataFrame(grouped.values.tolist(), columns=[str(right)+'_0',str(right)+'_1',str(right)+'_2'])\n",
    "        grouped_split.index = grouped.index\n",
    "\n",
    "        grouped_split = grouped_split.reset_index()\n",
    "        grouped_split.index.name='index'\n",
    "        #print(grouped_split)\n",
    "        temp_df = temp_msno_songid[[left, right]].merge(right = grouped_split, how = 'left', on=left)\n",
    "       \n",
    "        temp_train_data = train_data[[left, right]].merge(right = grouped_split, how = 'left', on=left)\n",
    "        temp_val_data = val_data[[left, right]].merge(right = grouped_split, how = 'left', on=left)\n",
    "        temp_test_data = test_data[[left, right]].merge(right = grouped_split, how = 'left', on=left)\n",
    "        for df in [temp_train_data, temp_val_data, temp_test_data]:\n",
    "            #weights = [1., 0.8, 0.6]\n",
    "            weights = [1., 0.2, 0.]\n",
    "            for i, col in enumerate([str(right)+'_0',str(right)+'_1',str(right)+'_2']):\n",
    "                df['in_top_'+str(i)+'_'+col] = (df[col] == df[right]).astype(np.float32)*weights[i]\n",
    "            df[left+'_is_in_topK_of_'+right] = df[['in_top_'+str(i)+'_'+col for i, col in enumerate([str(right)+'_0',str(right)+'_1',str(right)+'_2'])]].sum(axis=1).astype(np.float32)\n",
    "\n",
    "        train_data = pd.concat([train_data, temp_train_data[[left+'_is_in_topK_of_'+right]]], axis=1, join=\"outer\")\n",
    "        val_data = pd.concat([val_data, temp_val_data[[left+'_is_in_topK_of_'+right]]], axis=1, join=\"outer\")\n",
    "        test_data = pd.concat([test_data, temp_test_data[[left+'_is_in_topK_of_'+right]]], axis=1, join=\"outer\")\n",
    "        \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd_to_merge, column_to_concat = binary_encode_genre_ids(train_use[['msno', 'artist_name', 'genre_ids']], validation_use[['msno', 'artist_name', 'genre_ids']], test[['msno', 'artist_name', 'genre_ids']])\n",
    "# for i, _ in enumerate(column_to_concat):\n",
    "#     train_use = pd.concat([train_use, pd_to_merge[i][0:len(train_use)]], axis=1, join=\"outer\")\n",
    "#     validation_use = pd.concat([validation_use, pd_to_merge[i][len(train_use):len(train_use)+len(validation_use)]], axis=1, join=\"outer\")\n",
    "#     test = pd.concat([test, pd_to_merge[i][len(train_use)+len(validation_use):]], axis=1, join=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = binary_encode_genre_ids(train_use, validation_use, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def routine_type_encode(train_data, val_data, test_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['source_screen_name', 'source_type', 'popular_0', 'popular_1', 'active_0', 'active_1']], val_data[['source_screen_name', 'source_type', 'popular_0', 'popular_1', 'active_0', 'active_1']], test_data[['source_screen_name', 'source_type', 'popular_0', 'popular_1', 'active_0', 'active_1']]], axis=0, join=\"outer\")\n",
    "\n",
    "    for col in ['source_screen_name', 'source_type']:\n",
    "        grouped = temp_msno_songid.groupby(col)\n",
    "        count_type = np.log(grouped.size()+1)\n",
    "        count_type = count_type.reset_index()\n",
    "        count_type.columns=[col, col + '_count']\n",
    "        #print(count_type)\n",
    "        group_mean0 = grouped['popular_0'].agg(['mean'])\n",
    "        group_mean0 = group_mean0.reset_index()\n",
    "        group_mean0.columns=[col, col + 'popular_0_mean']\n",
    "        \n",
    "        group_mean1 = grouped['popular_1'].agg(['mean'])\n",
    "        group_mean1 = group_mean1.reset_index()\n",
    "        group_mean1.columns=[col, col + 'popular_1_mean']\n",
    "        \n",
    "        group_mean2 = grouped['active_0'].agg(['mean'])\n",
    "        group_mean2 = group_mean2.reset_index()\n",
    "        group_mean2.columns=[col, col + 'active_0_mean']\n",
    "        \n",
    "        group_mean3 = grouped['active_1'].agg(['mean'])\n",
    "        group_mean3 = group_mean3.reset_index()\n",
    "        group_mean3.columns=[col, col + 'active_1_mean']\n",
    "        type_score = count_type.merge(right = group_mean0, how = 'left', on=col)\n",
    "        type_score = type_score.merge(right = group_mean1, how = 'left', on=col)\n",
    "        type_score = type_score.merge(right = group_mean2, how = 'left', on=col)\n",
    "        type_score = type_score.merge(right = group_mean3, how = 'left', on=col)\n",
    "\n",
    "        type_score.index.name='index'\n",
    "        #print(type_score)\n",
    "        type_score.columns = [col, col + '_count', col + '_mean_popular_0', col + '_mean_popular_1', col + '_mean_active_0', col + '_mean_active_1']\n",
    "\n",
    "        train_data = train_data.merge(right = type_score, how = 'left', on=col)\n",
    "        val_data = val_data.merge(right = type_score, how = 'left', on=col)\n",
    "        test_data = test_data.merge(right = type_score, how = 'left', on=col)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = routine_type_encode(train_use, validation_use, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_city_lang_country_by_others(train_data, val_data, test_data):\n",
    "#     temp_msno_songid = pd.concat([train_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'country', 'language']], val_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'country', 'language']], test_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'country', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "#     count_dict = dict()\n",
    "    \n",
    "#     for col in ['city', 'country', 'language']:\n",
    "#         temp_df = None\n",
    "#         for target in ['song_id', 'msno', 'genre_ids', 'artist_name']:\n",
    "#             grouped = temp_msno_songid.groupby([col])\n",
    "#             df = grouped.agg({target: lambda x: x.nunique()})\n",
    "#             df = np.log(df+1)\n",
    "# #             print(df)\n",
    "# #             break\n",
    "#             if temp_df is not None:\n",
    "#                 temp_df = pd.concat([temp_df, df[target]], axis=1, join=\"inner\")\n",
    "#             else:\n",
    "#                 temp_df = df\n",
    "#         temp_df = temp_df.reset_index()\n",
    "#         temp_df.index.name='index'\n",
    "        \n",
    "#         temp_df.columns = [col, *[col + '_by_' + col_name for col_name in ['song_id', 'msno', 'genre_ids', 'artist_name']]]\n",
    "#         #print(temp_df)    \n",
    "#         train_data = train_data.merge(right = temp_df, how = 'left', on=col)\n",
    "#         test_data = test_data.merge(right = temp_df, how = 'left', on=col)\n",
    "#         val_data = val_data.merge(right = temp_df, how = 'left', on=col)\n",
    "#     return train_data, val_data, test_data\n",
    "    temp_msno_songid = pd.concat([train_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'language']], val_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'language']], test_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "    count_dict = dict()\n",
    "    \n",
    "    for col in ['city', 'language']:\n",
    "        temp_df = None\n",
    "        for target in ['song_id', 'msno', 'genre_ids', 'artist_name']:\n",
    "            grouped = temp_msno_songid.groupby([col])\n",
    "            df = grouped.agg({target: lambda x: x.nunique()})\n",
    "            df = np.log(df+1)\n",
    "#             print(df)\n",
    "#             break\n",
    "            if temp_df is not None:\n",
    "                temp_df = pd.concat([temp_df, df[target]], axis=1, join=\"inner\")\n",
    "            else:\n",
    "                temp_df = df\n",
    "        temp_df = temp_df.reset_index()\n",
    "        temp_df.index.name='index'\n",
    "        \n",
    "        temp_df.columns = [col, *[col + '_by_' + col_name for col_name in ['song_id', 'msno', 'genre_ids', 'artist_name']]]\n",
    "        #print(temp_df)    \n",
    "        train_data = train_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        test_data = test_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        val_data = val_data.merge(right = temp_df, how = 'left', on=col)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = measure_city_lang_country_by_others(train_use, validation_use, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(validation_use.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def encode_binary_feature(train_data, test_data, val_data):\n",
    "#     mode = lambda x: x.mode() if len(x) > 2 else np.array(x)\n",
    "# >>> df.groupby('tag')['category'].agg(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "    if col not in ['song_length', 'id']:\n",
    "        if test[col].dtype == np.float64:\n",
    "            train_use[col] = train_use[col].astype(np.float32)\n",
    "            validation_use[col] = validation_use[col].astype(np.float32)\n",
    "            test[col] = test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in [col for col in test.columns if col != 'id' ]:\n",
    "    if train_use[col].dtype == object:\n",
    "        train_use[col] = train_use[col].astype('category')\n",
    "        validation_use[col] = validation_use[col].astype('category')\n",
    "        test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_use.drop(['composer_score', 'lyricist_score', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'msno_by_country', 'genre_ids_score', 'city_by_song_id', 'city_by_genre_ids', 'language_by_song_id', 'language_by_genre_ids'], axis=1, inplace=True)\n",
    "# validation_use.drop(['composer_score', 'lyricist_score', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'msno_by_country', 'genre_ids_score', 'city_by_song_id', 'city_by_genre_ids', 'language_by_song_id', 'language_by_genre_ids'], axis=1, inplace=True)\n",
    "# test.drop(['composer_score', 'lyricist_score', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'msno_by_country', 'genre_ids_score', 'city_by_song_id', 'city_by_genre_ids', 'language_by_song_id', 'language_by_genre_ids'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is barrier [end] for faster testing, load and use next\n",
    "#del train_use, validation_use, test\n",
    "# gc.collect()\n",
    "# float32_list = ['genre_ids', 'language', 'song_year', 'composer_score', 'lyricist_score', 'artist_name_score', 'popular_0', 'num_people_0', 'popular_1', 'num_people_1', 'popular_2', 'num_people_2', 'popular_3', 'num_people_3', 'popular_4', 'num_people_4', 'active_0', 'num_song_0', 'active_1', 'num_song_1', 'active_2', 'num_song_2', 'active_3', 'num_song_3', 'active_4', 'num_song_4', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'artist_name_by_city', 'artist_name_by_country', 'artist_name_by_language', 'song_id_by_city', 'msno_by_country', 'msno_by_language', 'genre_ids_score', 'msno_is_in_topK_of_artist_name', 'msno_is_in_topK_of_genre_ids', 'artist_name_is_in_topK_of_genre_ids', 'artist_name_is_in_topK_of_msno', 'source_screen_name_count', 'source_screen_name_mean_popular_0', 'source_screen_name_mean_popular_1', 'source_screen_name_mean_active_0', 'source_screen_name_mean_active_1', 'source_type_count', 'source_type_mean_popular_0', 'source_type_mean_popular_1', 'source_type_mean_active_0', 'source_type_mean_active_1', 'city_by_song_id', 'city_by_msno', 'city_by_genre_ids', 'city_by_artist_name', 'language_by_song_id', 'language_by_msno', 'language_by_genre_ids', 'language_by_artist_name']\n",
    "# data_type_map =dict(zip(float32_list, [np.float32]*len(float32_list))) \n",
    "# train_use = pd.read_csv(DATASET_PATH + 'temp_train_all_comp.csv', compression='gzip', dtype = data_type_map)\n",
    "# validation_use = pd.read_csv(DATASET_PATH + 'temp_validation_all_comp.csv', compression='gzip', dtype = data_type_map)\n",
    "# test = pd.read_csv(DATASET_PATH + 'temp_test_all_comp.csv', compression='gzip', dtype = data_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_system_tab_encode(train_data, val_data, test_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['source_system_tab', 'popular_0', 'popular_1', 'active_0', 'active_1']], val_data[['source_system_tab', 'popular_0', 'popular_1', 'active_0', 'active_1']], test_data[['source_system_tab', 'popular_0', 'popular_1', 'active_0', 'active_1']]], axis=0, join=\"outer\")\n",
    "\n",
    "    for col in ['source_system_tab']:\n",
    "        grouped = temp_msno_songid.groupby(col)\n",
    "        count_type = np.log(grouped.size()+1)\n",
    "        count_type = count_type.reset_index()\n",
    "        count_type.columns=[col, col + '_count']\n",
    "        #print(count_type)\n",
    "        group_mean0 = grouped['popular_0'].agg(['mean'])\n",
    "        group_mean0 = group_mean0.reset_index()\n",
    "        group_mean0.columns=[col, col + 'popular_0_mean']\n",
    "        \n",
    "        group_mean1 = grouped['popular_1'].agg(['mean'])\n",
    "        group_mean1 = group_mean1.reset_index()\n",
    "        group_mean1.columns=[col, col + 'popular_1_mean']\n",
    "        \n",
    "        group_mean2 = grouped['active_0'].agg(['mean'])\n",
    "        group_mean2 = group_mean2.reset_index()\n",
    "        group_mean2.columns=[col, col + 'active_0_mean']\n",
    "        \n",
    "        group_mean3 = grouped['active_1'].agg(['mean'])\n",
    "        group_mean3 = group_mean3.reset_index()\n",
    "        group_mean3.columns=[col, col + 'active_1_mean']\n",
    "        \n",
    "        type_score = count_type.merge(right = group_mean0, how = 'left', on=col)\n",
    "        type_score = type_score.merge(right = group_mean1, how = 'left', on=col)\n",
    "        type_score = type_score.merge(right = group_mean2, how = 'left', on=col)\n",
    "        type_score = type_score.merge(right = group_mean3, how = 'left', on=col)\n",
    "\n",
    "        type_score.index.name='index'\n",
    "        #print(type_score)\n",
    "        type_score.columns = [col, col + '_count', col + '_mean_popular_0', col + '_mean_popular_1', col + '_mean_active_0', col + '_mean_active_1']\n",
    "\n",
    "        train_data = train_data.merge(right = type_score, how = 'left', on=col)\n",
    "        val_data = val_data.merge(right = type_score, how = 'left', on=col)\n",
    "        test_data = test_data.merge(right = type_score, how = 'left', on=col)\n",
    "    return train_data, val_data, test_data\n",
    "gc.collect()\n",
    "train_use, validation_use, test = source_system_tab_encode(train_use, validation_use, test)\n",
    "for col in test.columns:\n",
    "    if col not in ['source_system_tab_count', 'source_system_tab_mean_popular_0', 'source_system_tab_mean_popular_1', 'source_system_tab_mean_active_0', 'source_system_tab_mean_active_1']:\n",
    "        if test[col].dtype == np.float64:\n",
    "            train_use[col] = train_use[col].astype(np.float32)\n",
    "            validation_use[col] = validation_use[col].astype(np.float32)\n",
    "            test[col] = test[col].astype(np.float32)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_use.drop(['composer_score', 'lyricist_score', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'msno_by_country', 'genre_ids_score', 'city_by_song_id', 'city_by_genre_ids', 'language_by_song_id', 'language_by_genre_ids'], axis=1, inplace=True)\n",
    "# validation_use.drop(['composer_score', 'lyricist_score', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'msno_by_country', 'genre_ids_score', 'city_by_song_id', 'city_by_genre_ids', 'language_by_song_id', 'language_by_genre_ids'], axis=1, inplace=True)\n",
    "# test.drop(['composer_score', 'lyricist_score', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'msno_by_country', 'genre_ids_score', 'city_by_song_id', 'city_by_genre_ids', 'language_by_song_id', 'language_by_genre_ids'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_target_from_test(train_data, val_data, test_data):\n",
    "    weight = 0.6\n",
    "\n",
    "    test_group = test.groupby(['msno', 'song_id'])['msno'].agg(['count'])\n",
    "    test_group = test_group.reset_index()\n",
    "    test_group.columns = ['msno', 'song_id', 'occur_cout_in_test']\n",
    "\n",
    "    train_data = pd.merge(train_data, test_group,  how='left', left_on=['msno','song_id'], right_on = ['msno','song_id']).fillna({'occur_cout_in_test':0})\n",
    "    train_data['ref_target'] = train_data['target']  + train_data['occur_cout_in_test'] * weight\n",
    "    train_data['ref_target'] = train_data['target'].clip(lower=0, upper=1, axis=0)\n",
    "\n",
    "    val_data = pd.merge(val_data, test_group,  how='left', left_on=['msno','song_id'], right_on = ['msno','song_id']).fillna({'occur_cout_in_test':0})\n",
    "    val_data['ref_target'] = val_data['target']  + val_data['occur_cout_in_test'] * weight\n",
    "    val_data['ref_target'] = val_data['target'].clip(lower=0, upper=1, axis=0)\n",
    "\n",
    "    train_data.drop('occur_cout_in_test', axis = 1, inplace=True)\n",
    "    val_data.drop('occur_cout_in_test', axis = 1, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "#train_use, validation_use, test = fix_target_from_test(train_use, validation_use, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_merge_and_drop(train_data, val_data, test_data):\n",
    "#     merge_columns = [('source_screen_name_mean_popular_0', 'source_screen_name_mean_popular_1', 'source_screen_name_mean_active_0', 'source_screen_name_mean_active_1'),\\\n",
    "#                     ('source_type_mean_popular_0', 'source_type_mean_popular_1', 'source_type_mean_active_0', 'source_type_mean_active_1'),\\\n",
    "#                     ('source_system_tab_mean_popular_0', 'source_system_tab_mean_popular_1', 'source_system_tab_mean_active_0', 'source_system_tab_mean_active_1')]\n",
    "\n",
    "#     merge_columns_name = ['source_screen_name_avg_score', 'source_type_avg_score', 'source_system_tab_avg_score']\n",
    "    \n",
    "    merge_columns = [('source_screen_name_mean_popular_0', 'source_screen_name_mean_popular_1', 'source_screen_name_mean_active_0', 'source_screen_name_mean_active_1'),\\\n",
    "                    ('source_type_mean_popular_0', 'source_type_mean_popular_1', 'source_type_mean_active_0', 'source_type_mean_active_1'),\\\n",
    "                    ('source_system_tab_mean_popular_0', 'source_system_tab_mean_popular_1', 'source_system_tab_mean_active_0', 'source_system_tab_mean_active_1'),\\\n",
    "                    ('composer_by_city', 'composer_by_country', 'composer_by_language'),\\\n",
    "                    ('lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language'),\\\n",
    "                    ('artist_name_by_city', 'artist_name_by_country', 'artist_name_by_language'),\\\n",
    "                    ('city_by_song_id', 'city_by_msno', 'city_by_genre_ids', 'city_by_artist_name'),\\\n",
    "                    ('language_by_song_id', 'language_by_msno', 'language_by_genre_ids', 'language_by_artist_name')]\n",
    "\n",
    "    merge_columns_name = ['source_screen_name_avg_score',\\\n",
    "                        'source_type_avg_score',\\\n",
    "                        'source_system_tab_avg_score',\\\n",
    "                        'composer_by_city_country_language',\\\n",
    "                        'lyricist_by_city_country_language',\\\n",
    "                        'artist_name_by_city_country_language',\\\n",
    "                        'city_hot',\\\n",
    "                        'language_hot']\n",
    "    \n",
    "    for i, cols in enumerate(merge_columns):\n",
    "        train_data[merge_columns_name[i]] = train_data[[col for col in cols]].mean(axis = 1).astype(np.float32)\n",
    "        train_data.drop([col for col in cols], axis = 1, inplace=True)\n",
    "\n",
    "        val_data[merge_columns_name[i]] = val_data[[col for col in cols]].mean(axis = 1).astype(np.float32)\n",
    "        val_data.drop([col for col in cols], axis = 1, inplace=True)\n",
    "\n",
    "        test_data[merge_columns_name[i]] = test_data[[col for col in cols]].mean(axis = 1).astype(np.float32)\n",
    "        test_data.drop([col for col in cols], axis = 1, inplace=True)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "#train_use, validation_use, test = sum_merge_and_drop(train_use, validation_use, test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_use.drop([ 'num_song_1', 'num_song_2', 'num_song_3', 'num_song_4', 'num_people_1', 'num_people_2', 'num_people_3', 'num_people_4'], axis=1, inplace=True)\n",
    "#validation_use.drop([ 'num_song_1', 'num_song_2', 'num_song_3', 'num_song_4', 'num_people_1', 'num_people_2', 'num_people_3', 'num_people_4'], axis=1, inplace=True)\n",
    "#test.drop([ 'num_song_1', 'num_song_2', 'num_song_3', 'num_song_4', 'num_people_1', 'num_people_2', 'num_people_3', 'num_people_4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_artistname_related(train_data, val_data, test_data):\n",
    "    gc.collect()\n",
    "    temp_data = pd.concat([train_data, val_data, test_data], axis=0, join=\"outer\")\n",
    "\n",
    "    temp_data['avg_active_per_msno'] = temp_data[['active_1', 'active_2', 'active_3', 'active_4']].mean(axis = 1).astype(np.float32)\n",
    "    \n",
    "    grouped = temp_data[['msno', 'artist_name']].groupby(['artist_name'])\n",
    "    \n",
    "    num_people_per_artist = grouped['msno'].agg(lambda x: x.nunique())\n",
    "    num_people_per_artist = num_people_per_artist.reset_index()\n",
    "    num_people_per_artist.columns = ['artist_name', 'num_people_per_artist']\n",
    "    #print(num_people_per_artist)\n",
    "    mean_people_active_per_artist = temp_data[['artist_name', 'avg_active_per_msno']].groupby(['artist_name'])['avg_active_per_msno'].agg('mean')\n",
    "    mean_people_active_per_artist = mean_people_active_per_artist.reset_index()\n",
    "    mean_people_active_per_artist.columns = ['artist_name', 'mean_people_active_per_artist']\n",
    "    #print(sum_people_active_per_artist)\n",
    "    train_data = train_data.merge(right = num_people_per_artist, how = 'left', on='artist_name')\n",
    "    val_data = val_data.merge(right = num_people_per_artist, how = 'left', on='artist_name')\n",
    "    test_data = test_data.merge(right = num_people_per_artist, how = 'left', on='artist_name')\n",
    "    \n",
    "    train_data = train_data.merge(right = mean_people_active_per_artist, how = 'left', on='artist_name')\n",
    "    val_data = val_data.merge(right = mean_people_active_per_artist, how = 'left', on='artist_name')\n",
    "    test_data = test_data.merge(right = mean_people_active_per_artist, how = 'left', on='artist_name')\n",
    "    \n",
    "    return train_data[['num_people_per_artist', 'mean_people_active_per_artist']], val_data[['num_people_per_artist', 'mean_people_active_per_artist']], test_data[['num_people_per_artist', 'mean_people_active_per_artist']]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "train_use[['num_people_per_artist', 'mean_people_active_per_artist']], \\\n",
    "validation_use[['num_people_per_artist', 'mean_people_active_per_artist']], \\\n",
    "test[['num_people_per_artist', 'mean_people_active_per_artist']] = new_artistname_related(train_use[['artist_name', 'msno', 'active_1', 'active_2', 'active_3', 'active_4']], \\\n",
    "                                                                                         validation_use[['artist_name', 'msno', 'active_1', 'active_2', 'active_3', 'active_4']], \\\n",
    "                                                                                         test[['artist_name', 'msno', 'active_1', 'active_2', 'active_3', 'active_4']])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "def cal_artist_popular(train_data, val_data, test_data):\n",
    "    all_data = pd.concat([train_data[['artist_name', 'song_year']], val_data[['artist_name', 'song_year']], test_data[['artist_name', 'song_year']]], axis=0, join=\"inner\")\n",
    "\n",
    "    for index, _ in enumerate(time_wnd[:-1]):\n",
    "        begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "        \n",
    "        select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "\n",
    "        grouped = select_data[['artist_name']].groupby(['artist_name'])\n",
    "\n",
    "        count_artist_name = grouped['artist_name'].agg(['count'])\n",
    "\n",
    "        popularity = pd.concat([np.log(count_artist_name+1)], axis=1, join=\"inner\")\n",
    "        popularity.columns = ['artist_popular_{}'.format(index)]\n",
    "        popularity = popularity.reset_index(drop=False)\n",
    "        train_data = train_data.merge(popularity, on='artist_name', how ='left')\n",
    "        test_data = test_data.merge(popularity, on='artist_name', how ='left')\n",
    "        val_data = val_data.merge(popularity, on='artist_name', how ='left')\n",
    "    return_list = list()\n",
    "    for index, _ in enumerate(time_wnd[:-1]):\n",
    "        return_list.append('artist_popular_{}'.format(index))\n",
    " \n",
    "    return train_data[return_list], val_data[return_list], test_data[return_list]\n",
    "\n",
    "return_list = ['artist_popular_0', 'artist_popular_1', 'artist_popular_2', 'artist_popular_3', 'artist_popular_4']\n",
    "input_list = ['artist_name', 'song_year']\n",
    "gc.collect()\n",
    "train_use[return_list], validation_use[return_list], test[return_list] = cal_artist_popular(train_use[input_list], validation_use[input_list], test[input_list])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_msno_related(train_data, val_data, test_data):\n",
    "    gc.collect()\n",
    "    temp_data = pd.concat([train_data, val_data, test_data], axis=0, join=\"outer\")\n",
    "\n",
    "    temp_data['avg_popular_per_artist'] = temp_data[['artist_popular_1', 'artist_popular_2', 'artist_popular_3', 'artist_popular_4']].mean(axis = 1).astype(np.float32)\n",
    "    \n",
    "    grouped = temp_data[['msno', 'artist_name']].groupby(['msno'])\n",
    "    \n",
    "    num_artist_per_people = grouped['artist_name'].agg(lambda x: x.nunique())\n",
    "    num_artist_per_people = num_artist_per_people.reset_index()\n",
    "    num_artist_per_people.columns = ['msno', 'num_artist_per_people']\n",
    "    #print(num_people_per_artist)\n",
    "    mean_artist_popular_per_people = temp_data[['msno', 'avg_popular_per_artist']].groupby(['msno'])['avg_popular_per_artist'].agg('mean')\n",
    "    mean_artist_popular_per_people = mean_artist_popular_per_people.reset_index()\n",
    "    mean_artist_popular_per_people.columns = ['msno', 'mean_artist_popular_per_people']\n",
    "    #print(sum_people_active_per_artist)\n",
    "    train_data = train_data.merge(right = num_artist_per_people, how = 'left', on='msno')\n",
    "    val_data = val_data.merge(right = num_artist_per_people, how = 'left', on='msno')\n",
    "    test_data = test_data.merge(right = num_artist_per_people, how = 'left', on='msno')\n",
    "    \n",
    "    train_data = train_data.merge(right = mean_artist_popular_per_people, how = 'left', on='msno')\n",
    "    val_data = val_data.merge(right = mean_artist_popular_per_people, how = 'left', on='msno')\n",
    "    test_data = test_data.merge(right = mean_artist_popular_per_people, how = 'left', on='msno')\n",
    "    \n",
    "    return train_data[['num_artist_per_people', 'mean_artist_popular_per_people']], val_data[['num_artist_per_people', 'mean_artist_popular_per_people']], test_data[['num_artist_per_people', 'mean_artist_popular_per_people']]\n",
    "gc.collect()\n",
    "train_use[['num_artist_per_people', 'mean_artist_popular_per_people']], \\\n",
    "validation_use[['num_artist_per_people', 'mean_artist_popular_per_people']], \\\n",
    "test[['num_artist_per_people', 'mean_artist_popular_per_people']] = new_msno_related(train_use[['artist_name', 'msno', 'artist_popular_1', 'artist_popular_2', 'artist_popular_3', 'artist_popular_4']], \\\n",
    "                                                                                         validation_use[['artist_name', 'msno', 'artist_popular_1', 'artist_popular_2', 'artist_popular_3', 'artist_popular_4']], \\\n",
    "                                                                                         test[['artist_name', 'msno','artist_popular_1', 'artist_popular_2', 'artist_popular_3', 'artist_popular_4']])\n",
    "\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_use.to_csv(DATASET_PATH + 'temp_train_all_comp_delete.csv', index = False, compression='gzip')\n",
    "# print('train saved.')\n",
    "# validation_use.to_csv(DATASET_PATH + 'temp_validation_all_comp_delete.csv', index = False, compression='gzip')\n",
    "# print('val saved.')\n",
    "# test.to_csv(DATASET_PATH + 'temp_test_all_comp_delete.csv', index = False, compression='gzip')\n",
    "# print('test saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float32_list = list()\n",
    "for col in test.columns:\n",
    "    if col not in ['song_length', 'id']:\n",
    "        if test[col].dtype == np.float32:\n",
    "            float32_list.append(col)\n",
    "print(float32_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this it barrier [start] for faster testing, save first\n",
    "# store = pd.HDFStore(HDF_FILENAME_TEMPSAVE)\n",
    "# store['train'] = train_use\n",
    "# print('train saved.')\n",
    "# store['val'] = validation_use\n",
    "# print('val saved.')\n",
    "# store['test'] = test\n",
    "# print('test saved.')\n",
    "# store.close()\n",
    "# train_use.to_csv(DATASET_PATH + 'temp_train_all_comp_2.csv', index = False, compression='gzip')\n",
    "# print('train saved.')\n",
    "# validation_use.to_csv(DATASET_PATH + 'temp_validation_all_comp_2.csv', index = False, compression='gzip')\n",
    "# print('val saved.')\n",
    "# test.to_csv(DATASET_PATH + 'temp_test_all_comp_2.csv', index = False, compression='gzip')\n",
    "# print('test saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del train_use, validation_use, test\n",
    "# gc.collect()\n",
    "# float32_list = ['genre_ids', 'language', 'song_year', 'composer_score', 'lyricist_score', 'artist_name_score', 'popular_0', 'num_people_0', 'popular_1', 'popular_2', 'popular_3', 'popular_4', 'active_0', 'num_song_0', 'active_1', 'active_2', 'active_3', 'active_4', 'song_id_by_city', 'msno_by_country', 'msno_by_language', 'genre_ids_score', 'msno_is_in_topK_of_artist_name', 'msno_is_in_topK_of_genre_ids', 'artist_name_is_in_topK_of_genre_ids', 'artist_name_is_in_topK_of_msno', 'source_screen_name_count', 'source_type_count', 'source_system_tab_count', 'source_screen_name_avg_score', 'source_type_avg_score', 'source_system_tab_avg_score', 'composer_by_city_country_language', 'lyricist_by_city_country_language', 'artist_name_by_city_country_language', 'city_hot', 'language_hot']\n",
    "# data_type_map =dict(zip(float32_list, [np.float32]*len(float32_list))) \n",
    "# train_use = pd.read_csv(DATASET_PATH + 'temp_train_all_comp_delete.csv', compression='gzip', dtype = data_type_map)\n",
    "# validation_use = pd.read_csv(DATASET_PATH + 'temp_validation_all_comp_delete.csv', compression='gzip', dtype = data_type_map)\n",
    "# test = pd.read_csv(DATASET_PATH + 'temp_test_all_comp_delete.csv', compression='gzip', dtype = data_type_map)\n",
    "# test_id =  test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in [col for col in test.columns if col != 'id' ]:\n",
    "    if train_use[col].dtype == object:\n",
    "        train_use[col] = train_use[col].astype('category')\n",
    "        validation_use[col] = validation_use[col].astype('category')\n",
    "        test[col] = test[col].astype('category')\n",
    "for col in test.columns:\n",
    "    if col not in ['song_length', 'id']:\n",
    "        if test[col].dtype == np.float64:\n",
    "            train_use[col] = train_use[col].astype(np.float32)\n",
    "            validation_use[col] = validation_use[col].astype(np.float32)\n",
    "            test[col] = test[col].astype(np.float32)\n",
    "for col in test.columns:\n",
    "    if col in ['registered_via', 'bd', 'city', 'registration_year', 'registration_month', 'registration_day', 'expiration_year', 'expiration_year', 'expiration_day']:\n",
    "        if test[col].dtype == np.int64:\n",
    "            train_use[col] = train_use[col].astype(np.int32)\n",
    "            validation_use[col] = validation_use[col].astype(np.int32)\n",
    "            test[col] = test[col].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use['bd'] = pd.to_numeric(train_use['bd'], downcast='signed')\n",
    "validation_use['bd'] = pd.to_numeric(validation_use['bd'], downcast='signed')\n",
    "test['bd'] = pd.to_numeric(test['bd'], downcast='signed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in test.columns: print(col, ':', test[col].dtype, '; uinque values:', len(test[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_id), len(test))\n",
    "#del train_use_org, test_org, validation_use_org\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list()\n",
    "for col in test.columns:\n",
    "    if col not in ['id']:\n",
    "        feature_list.append(col)\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catogory_list = list()\n",
    "for col in test.columns:\n",
    "    if col not in ['song_length', 'id']:\n",
    "        if test[col].dtype.name == 'category':\n",
    "            catogory_list.append(col)\n",
    "print(catogory_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用temp_train_all_comp_2 然后sum_merge_and_drop 和 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "# features = ['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', 'city', 'bd', 'gender', 'registered_via', 'song_length', 'genre_ids', 'language', 'name', 'country', 'song_year', 'registration_year', 'registration_month', 'registration_day', 'expiration_year', 'expiration_month', 'expiration_day', 'days', 'composer_score', 'composer', 'lyricist_score', 'lyricist', 'artist_name_score', 'artist_name', 'popular_0', 'num_people_0', 'popular_1', 'num_people_1', 'popular_2', 'num_people_2', 'popular_3', 'num_people_3', 'popular_4', 'num_people_4', 'active_0', 'num_song_0', 'active_1', 'num_song_1', 'active_2', 'num_song_2', 'active_3', 'num_song_3', 'active_4', 'num_song_4', 'composer_by_city', 'composer_by_country', 'composer_by_language', 'lyricist_by_city', 'lyricist_by_country', 'lyricist_by_language', 'artist_name_by_city', 'artist_name_by_country', 'artist_name_by_language', 'song_id_by_city', 'msno_by_country', 'msno_by_language', 'genre_ids_score', 'genre_ids_popular', 'msno_is_in_topK_of_artist_name', 'msno_is_in_topK_of_genre_ids', 'artist_name_is_in_topK_of_genre_ids', 'artist_name_is_in_topK_of_msno', 'source_screen_name_count', 'source_screen_name_mean_popular_0', 'source_screen_name_mean_popular_1', 'source_screen_name_mean_active_0', 'source_screen_name_mean_active_1', 'source_type_count', 'source_type_mean_popular_0', 'source_type_mean_popular_1', 'source_type_mean_active_0', 'source_type_mean_active_1', 'city_by_song_id', 'city_by_msno', 'city_by_genre_ids', 'city_by_artist_name', 'language_by_song_id', 'language_by_msno', 'language_by_genre_ids', 'language_by_artist_name']\n",
    "\n",
    "# train_use_array = train_use[features].values\n",
    "# labels = train_use['target'].values.astype('int').flatten()\n",
    "# #del train_use  # delete dataframe to release memory \n",
    "# gc.collect()\n",
    "# train_data = lgb.Dataset(train_use_array, labels, feature_name = features, categorical_feature = ['msno', 'song_id', 'source_system_tab', 'source_screen_name', 'source_type', 'gender', 'name', 'country', 'composer', 'lyricist', 'artist_name'])\n",
    "train_use_label = train_use['target']\n",
    "train_use = train_use.drop(['target'],axis=1)\n",
    "gc.collect()\n",
    "train_data = lgb.Dataset(train_use,label=train_use_label)\n",
    "gc.collect()\n",
    "val_data = lgb.Dataset(validation_use.drop(['target'],axis=1),label=validation_use['target'])\n",
    "gc.collect()\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1 ,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 128,#108\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 128,\n",
    "    'max_depth': 12,\n",
    "    'num_rounds': 800,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "bst = lgb.train(params, train_data, valid_sets=[val_data])\n",
    "\n",
    "#del train_data, val_data\n",
    "gc.collect()\n",
    "\n",
    "predictions+=bst.predict(test.drop(['id'],axis=1))\n",
    "print('cur fold finished.')\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plot feature importances...')\n",
    "ax = lgb.plot_importance(bst, max_num_features=20)\n",
    "plt.show()\n",
    "# last 400 0.696958 800 0.703245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "float32_list = list()\n",
    "for col in test.columns:\n",
    "    if col not in ['song_length', 'id']:\n",
    "        if test[col].dtype == np.float32:\n",
    "            float32_list.append(col)\n",
    "print(float32_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pd.read_csv(DATASET_PATH+'submission_2017-11-06 22:25:54.csv')#'submission_2017-11-07 17:29:32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_tune_with_val(params, tune_param, param_list, data_list, val_data, less_prefered = False):\n",
    "    #data_list = {'train':{'x':train_d,'y':train_y}, 'validation':{'x':valid_d,'y':valid_y}}\n",
    "    best_metric = (less_prefered and sys.float_info.max or -sys.float_info.max)\n",
    "    best_param = param_list[0]\n",
    "\n",
    "    for par_value in param_list:\n",
    "        params[tune_param] = par_value\n",
    "        # , num_boost_round=params['num_boost_round'], early_stopping_rounds = params['early_stopping_rounds']\n",
    "        model = lgb.train(params, data_list['train']['x'], valid_sets=[data_list['validation']['x']], \\\n",
    "                feature_name='auto', #categorical_feature=['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender',\\\n",
    "                                     #                       'bd', 'name', 'artist_name', 'composer', 'lyricist', 'msno', 'song_id', 'genre_ids',\\\n",
    "                                     #                       'country', 'language', 'registered_via'],、\n",
    "                        )\n",
    "       \n",
    "        val_predprob = model.predict(val_data)\n",
    "        auroc_score = metrics.roc_auc_score(data_list['validation']['y'], val_predprob)\n",
    "\n",
    "        if (not less_prefered and auroc_score > best_metric) or (less_prefered and auroc_score < best_metric):\n",
    "            best_metric = auroc_score\n",
    "            best_param = par_value\n",
    "    log.info('best param for {}: {}, metric: {}'.format(tune_param, best_param, best_metric))\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#{'top_k': 20, 'feature_fraction': 0.8, 'bagging_freq': 1, 'min_data_in_bin': 3, 'min_sum_hessian_in_leaf': 0.001, 'bagging_fraction': 0.9, 'max_depth': 12, 'num_leaves': 100, 'learning_rate': 0.01, 'objective': 'binary', 'lambda_l2': 0.01, 'feature_fraction_seed': 1024, 'min_data_in_leaf': 15, 'max_bin': 100, 'verbose': 0, 'bagging_seed': 6666, 'max_cat_to_onehot': 4, 'metric': 'auc', 'lambda_l1': 1e-05, 'num_threads': 16, 'boosting': 'gbdt', 'min_split_gain': 0.3}\n",
    "\n",
    "#{'bagging_seed': 6666, 'lambda_l1': 1e-05, 'lambda_l2': 0.01, 'metric': 'auc', 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 0.001, 'feature_fraction': 0.8, 'feature_fraction_seed': 1024, 'num_leaves': 90, 'boosting': 'gbdt', 'verbose': 0, 'min_data_in_leaf': 15, 'top_k': 20, 'objective': 'binary', 'min_data_in_bin': 3, 'num_threads': 16, 'max_cat_to_onehot': 4, 'max_depth': 10, 'bagging_fraction': 0.9, 'learning_rate': 0.01, 'max_bin': 80, 'min_split_gain': 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_for_best_params(train, validation, test):\n",
    "    \n",
    "    X_train = lgb.Dataset(np.array(train.drop(['target'], axis=1)), label=train['target'].values)\n",
    "    X_valid = lgb.Dataset(np.array(validation.drop(['target'], axis=1)), label=validation['target'].values)\n",
    "    \n",
    "    y_train = train['target'].values\n",
    "    y_valid = validation['target'].values\n",
    "\n",
    "    X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "    data_list = {'train':{'x':X_train,'y':y_train}, 'validation':{'x':X_valid,'y':y_valid}}\n",
    "######## for value rather than catogory ################\n",
    "#   params_to_eval = OrderedDict(\n",
    "#         ( \n",
    "#         ('num_boost_round', range(120,150,10)),\n",
    "#         ('num_leaves', range(80,100,10)), # number of leaves in one tree\n",
    "#         ('max_depth', range(8,12,1)),\n",
    "#         ('min_data_in_leaf', 15),\n",
    "#         ('min_sum_hessian_in_leaf', [0.001]),# too high will lead to under-fitting\n",
    "#         ('min_split_gain',[0.3]),# the minimum loss reduction required to make a split\n",
    "#         ('bagging_fraction',[0.9]),# [i/10.0 for i in range(6,10)]\n",
    "#         ('feature_fraction',[0.8]),# typical: 0.5-1\n",
    "#         ('max_bin', range(70,90,10)),\n",
    "#         ('lambda_l2',[0.01]),\n",
    "#         ('lambda_l1',[1e-5]),\n",
    "#         ('learning_rate',[0.01]), # typical: 0.01-0.2\n",
    "#         )\n",
    "#       )\n",
    "     \n",
    "#     initial_params = {\n",
    "#         'objective': 'binary',\n",
    "#         'boosting': 'gbdt',\n",
    "#         'num_boost_round': 140,\n",
    "#         'learning_rate': 0.01 ,\n",
    "#         'verbose': 0,\n",
    "#         'num_leaves': 90,\n",
    "#         'num_threads':16,\n",
    "#         'max_depth': 9,\n",
    "#         'min_data_in_leaf': 15, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "#         'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "#         'feature_fraction': 0.8, #colsample_bytree\n",
    "#         'feature_fraction_seed': 1024,\n",
    "#         'bagging_fraction': 0.9, #subsample\n",
    "#         'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "#         'bagging_seed': 6666,\n",
    "#         'early_stopping_rounds':10,   \n",
    "#         'lambda_l1': 1e-5, #L1 regularization\n",
    "#         'lambda_l2': 0.01, #L2 regularization\n",
    "#         'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "#         'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "#         'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "#         'max_bin': 70, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "#         'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "#         'metric' : 'auc',\n",
    "#     } \n",
    "    params_to_eval = OrderedDict(\n",
    "        ( \n",
    "        ('num_boost_round', range(100,400,50)),\n",
    "        ('num_leaves', range(80,160,10)), # number of leaves in one tree\n",
    "        ('max_depth', range(8,18,1)),\n",
    "        ('min_data_in_leaf', range(10,18,2)),\n",
    "        ('min_sum_hessian_in_leaf', [0.001]),# too high will lead to under-fitting\n",
    "        ('min_split_gain',[0.3]),# the minimum loss reduction required to make a split\n",
    "        ('bagging_fraction',[0.9]),# [i/10.0 for i in range(6,10)]\n",
    "        ('feature_fraction',[0.8]),# typical: 0.5-1\n",
    "        ('max_bin', range(80,200,10)),\n",
    "        ('lambda_l2',[0.01]),\n",
    "        ('lambda_l1',[1e-5]),\n",
    "        ('learning_rate',[0.01]), # typical: 0.01-0.2\n",
    "        )\n",
    "      )\n",
    "     \n",
    "    initial_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_boost_round': 200,\n",
    "        'learning_rate': 0.1 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 120,\n",
    "        'num_threads':16,\n",
    "        'max_depth': 14,\n",
    "        'min_data_in_leaf': 16, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.8, #colsample_bytree\n",
    "        'feature_fraction_seed': 1024,\n",
    "        'bagging_fraction': 0.9, #subsample\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "        'bagging_seed': 6666,\n",
    "        'early_stopping_rounds':10,   \n",
    "        'lambda_l1': 1e-5, #L1 regularization\n",
    "        'lambda_l2': 0.01, #L2 regularization\n",
    "        'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "        'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "        'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "        'max_bin': 140, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "        'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "        'metric' : 'auc',\n",
    "    } \n",
    "    # only param nin this list are tuned, total list are ['n_estimators', 'reg_alpha', 'reg_lambda', 'subsample', 'colsample_bytree', 'min_child_weight', 'max_depth', 'learning_rate', 'gamma']\n",
    "    #tuned_param_name = ['num_boost_round', 'num_leaves', 'max_depth', 'max_bin']\n",
    "    tuned_param_name = ['num_boost_round', 'num_leaves', 'max_depth', 'min_data_in_leaf', 'min_sum_hessian_in_leaf',\\\n",
    "                        'min_split_gain', 'bagging_fraction', 'feature_fraction', 'max_bin', 'lambda_l2', 'lambda_l1', 'learning_rate']\n",
    "    for par_name, par_list in params_to_eval.items():\n",
    "        if par_name in tuned_param_name:\n",
    "            log.info('tunning {}...'.format(par_name))\n",
    "            if len(par_list) > 1:\n",
    "                initial_params[par_name] = param_tune_with_val(initial_params, par_name, par_list, data_list, np.array(validation.drop(['target'], axis=1)))\n",
    "            else:\n",
    "                initial_params[par_name] = par_list[0]\n",
    "    \n",
    "    return initial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_param = search_for_best_params(train_use, validation_use, test)\n",
    "log.info(best_param)\n",
    "time_elapsed = time.time() - start_time\n",
    "log.info('time used: {:.3f}sec'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_boost_round': 140,\n",
    "        'learning_rate': 0.01 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 90,\n",
    "        'num_threads':16,\n",
    "        'max_depth': 9,\n",
    "        'min_data_in_leaf': 15, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.8, #colsample_bytree\n",
    "        'feature_fraction_seed': 1024,\n",
    "        'bagging_fraction': 0.9, #subsample\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "        'bagging_seed': 6666,\n",
    "        'early_stopping_rounds':10,   \n",
    "        'lambda_l1': 1e-5, #L1 regularization\n",
    "        'lambda_l2': 0.01, #L2 regularization\n",
    "        'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "        'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "        'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "        'max_bin': 70, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "        'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "        'metric' : 'auc',\n",
    "    } \n",
    "X_train = lgb.Dataset(np.array(train_use.drop(['target'], axis=1)), label=train_use['target'].values)\n",
    "X_valid = lgb.Dataset(np.array(validation_use.drop(['target'], axis=1)), label=validation_use['target'].values)\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "model = lgb.train(params, X_train, valid_sets=[X_valid])\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': pred})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_use.drop(['target'], axis=1))\n",
    "y_train = train_use['target'].values\n",
    "\n",
    "X_valid = np.array(validation_use.drop(['target'], axis=1))\n",
    "y_valid = validation_use['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "# d_train = xgb.DMatrix(X_train)\n",
    "# d_valid = xgb.DMatrix(X_valid) \n",
    "# d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "data_list = {'train':{'x':X_train,'y':y_train}, 'validation':{'x':X_valid,'y':y_valid}}\n",
    "# Train model, evaluate and make predictions\n",
    "params={\n",
    "    'n_estimators':500,\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.75,\n",
    "    'gamma':0.1,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.3,\n",
    "    'min_child_weight':3,\n",
    "    'max_depth':16,\n",
    "    'seed':1024,\n",
    "    }\n",
    "\n",
    "param_tune_with_val(params, 'max_depth', [5,1,6], data_list, 'auc', 20)\n",
    "\n",
    "# model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=20, \\\n",
    "#     maximize=True, verbose_eval=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_use.drop(['target'], axis=1))\n",
    "y_train = train_use['target'].values\n",
    "\n",
    "X_valid = np.array(validation_use.drop(['target'], axis=1))\n",
    "y_valid = validation_use['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid) \n",
    "d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "# Train model, evaluate and make predictions\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.75\n",
    "params['max_depth'] = 16\n",
    "params['silent'] = 1\n",
    "params['eval_metric'] = 'auc'\n",
    "\n",
    "model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=20, \\\n",
    "    maximize=True, verbose_eval=5)\n",
    "\n",
    "#Predict training set:\n",
    "train_predictions = model.predict(X_train)\n",
    "train_predprob = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "val_predictions = model.predict(X_valid)\n",
    "val_predprob = model.predict_proba(X_valid)[:,1]\n",
    "\n",
    "#Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"Train Accuracy : %.4g\" % metrics.accuracy_score(y_train, train_predictions))\n",
    "print(\"Train AUC Score (Train): %f\" % metrics.roc_auc_score(y_train, train_predprob))\n",
    "print(\"ValAccuracy : %.4g\" % metrics.accuracy_score(y_valid, val_predictions))\n",
    "print(\"Validation AUC Score (Train): %f\" % metrics.roc_auc_score(y_valid, val_predprob))\n",
    "\n",
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "\n",
    "p_test = model.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "modelfit(xgb1, train_use.drop(['target'],axis=1), train_use['target'], validation_use.drop(['target'],axis=1), validation_use['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'], eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, train, label, validation, val_label, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train.values, label=label.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train, label, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train)\n",
    "    train_predprob = alg.predict_proba(train)[:,1]\n",
    "    \n",
    "    val_predictions = alg.predict(validation)\n",
    "    val_predprob = alg.predict_proba(validation)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Train Accuracy : %.4g\" % metrics.accuracy_score(label.values, train_predictions))\n",
    "    print(\"Train AUC Score (Train): %f\" % metrics.roc_auc_score(label, train_predprob))\n",
    "    print(\"ValAccuracy : %.4g\" % metrics.accuracy_score(val_label.values, val_predictions))\n",
    "    print(\"Validation AUC Score (Train): %f\" % metrics.roc_auc_score(val_label, val_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "modelfit(xgb1, train_use.drop(['target'],axis=1), train_use['target'], validation_use.drop(['target'],axis=1), validation_use['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(train_use.drop(['target'],axis=1), label=train_use['target'])\n",
    "val_data = lgb.Dataset(validation_use.drop(['target'],axis=1), label=validation_use['target'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1 ,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 108,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 128,\n",
    "    'max_depth': 10,\n",
    "    'num_rounds': 200,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "predictions=bst.predict(test.drop(['id'],axis=1))\n",
    "print('finished.')\n",
    "\n",
    "    \n",
    "predictions = predictions/3\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "for train_indices,val_indices in kf.split(train) : \n",
    "    train_data = lgb.Dataset(train.drop(['target'],axis=1).loc[train_indices,:],label=train.loc[train_indices,'target'])\n",
    "    val_data = lgb.Dataset(train.drop(['target'],axis=1).loc[val_indices,:],label=train.loc[val_indices,'target'])\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.1 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 108,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 128,\n",
    "        'max_depth': 10,\n",
    "        'num_rounds': 200,\n",
    "        'metric' : 'auc',\n",
    "        } \n",
    "    \n",
    "    bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "    predictions+=bst.predict(test.drop(['id'],axis=1))\n",
    "    print('cur fold finished.')\n",
    "    del bst\n",
    "    \n",
    "predictions = predictions/3\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess songs data\n",
    "songs_genres = np.array(songs['genre_ids']\\\n",
    "    .apply(lambda x: [int(v) for v in str(x).split('|')]))\n",
    "genres_list = songs_genres.ravel().unique()\n",
    "print('Number of genres: ' + str(len(genres_list)))\n",
    "\n",
    "ohe_genres = np.zeros((len(songs_genres), len(genres_list)))\n",
    "for s_i, s_genres in enumerate(songs_genres):\n",
    "    for genre in s_genres:\n",
    "        g_i = genres_list.find(genre)\n",
    "        ohe_genres[s_i, g_i] = 1\n",
    "        \n",
    "for g_i, g in enumerate(genres_list):\n",
    "    songs['genre_' + str(g)] = ohe_genres[:, g_i]\n",
    "print(songs.head())\n",
    "songs = songs.drop(['genre_ids'], axis=1)\n",
    "\n",
    "song_cols = songs.columns\n",
    "\n",
    "# Preprocess dataset\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "cols = list(train.columns)\n",
    "cols.remove('target')\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = train[col].apply(str)\n",
    "        test[col] = test[col].apply(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        train_vals = list(train[col].unique())\n",
    "        test_vals = list(test[col].unique())\n",
    "        le.fit(train_vals + test_vals)\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "\n",
    "        print(col + ': ' + str(len(train_vals)) + ', ' + str(len(test_vals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "########################################\n",
    "## load the data\n",
    "########################################\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "uid = train.msno\n",
    "sid = train.song_id\n",
    "target = train.target\n",
    "\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "id_test = test.id\n",
    "uid_test = test.msno\n",
    "sid_test = test.song_id\n",
    "\n",
    "########################################\n",
    "## encoding\n",
    "########################################\n",
    "\n",
    "usr_encoder = LabelEncoder()\n",
    "usr_encoder.fit(uid.append(uid_test))\n",
    "uid = usr_encoder.transform(uid)\n",
    "uid_test = usr_encoder.transform(uid_test)\n",
    "\n",
    "sid_encoder = LabelEncoder()\n",
    "sid_encoder.fit(sid.append(sid_test))\n",
    "sid = sid_encoder.transform(sid)\n",
    "sid_test = sid_encoder.transform(sid_test)\n",
    "\n",
    "u_cnt = int(max(uid.max(), uid_test.max()) + 1)\n",
    "s_cnt = int(max(sid.max(), sid_test.max()) + 1)\n",
    "\n",
    "########################################\n",
    "## train-validation split\n",
    "########################################\n",
    "\n",
    "perm = np.random.permutation(len(train))\n",
    "trn_cnt = int(len(train) * 0.85)\n",
    "uid_trn = uid[perm[:trn_cnt]]\n",
    "uid_val = uid[perm[trn_cnt:]]\n",
    "sid_trn = sid[perm[:trn_cnt]]\n",
    "sid_val = sid[perm[trn_cnt:]]\n",
    "target_trn = target[perm[:trn_cnt]]\n",
    "target_val = target[perm[trn_cnt:]]\n",
    "\n",
    "########################################\n",
    "## define the model\n",
    "########################################\n",
    "\n",
    "def get_model():\n",
    "    user_embeddings = Embedding(u_cnt,\n",
    "            64,\n",
    "            embeddings_initializer=RandomUniform(minval=-0.1, maxval=0.1),\n",
    "            embeddings_regularizer=l2(1e-4),\n",
    "            input_length=1,\n",
    "            trainable=True)\n",
    "    song_embeddings = Embedding(s_cnt,\n",
    "            64,\n",
    "            embeddings_initializer=RandomUniform(minval=-0.1, maxval=0.1),\n",
    "            embeddings_regularizer=l2(1e-4),\n",
    "            input_length=1,\n",
    "            trainable=True)\n",
    "\n",
    "    uid_input = Input(shape=(1,), dtype='int32')\n",
    "    embedded_usr = user_embeddings(uid_input)\n",
    "    embedded_usr = Reshape((64,))(embedded_usr)\n",
    "\n",
    "    sid_input = Input(shape=(1,), dtype='int32')\n",
    "    embedded_song = song_embeddings(sid_input)\n",
    "    embedded_song = Reshape((64,))(embedded_song)\n",
    "\n",
    "    preds = dot([embedded_usr, embedded_song], axes=1)\n",
    "    preds = concatenate([embedded_usr, embedded_song, preds])\n",
    "    \n",
    "    preds = Dense(128, activation='relu')(preds)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    \n",
    "    preds = Dense(1, activation='sigmoid')(preds)\n",
    "\n",
    "    model = Model(inputs=[uid_input, sid_input], outputs=preds)\n",
    "    \n",
    "    opt = RMSprop(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "   \n",
    "model = get_model()\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=5)\n",
    "model_path = 'bst_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, \\\n",
    "        save_weights_only=True)\n",
    "\n",
    "hist = model.fit([uid_trn, sid_trn], target_trn, validation_data=([uid_val, sid_val], \\\n",
    "        target_val), epochs=100, batch_size=32768, shuffle=True, \\\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "model.load_weights(model_path)\n",
    "\n",
    "preds_val = model.predict([uid_val, sid_val], batch_size=32768)\n",
    "val_auc = roc_auc_score(target_val, preds_val)\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "\n",
    "preds_test = model.predict([uid_test, sid_test], batch_size=32768, verbose=1)\n",
    "sub = pd.DataFrame({'id': id_test, 'target': preds_test.ravel()})\n",
    "sub.to_csv('./sub_%.5f.csv'%(val_auc), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear algebra:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Graphics:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "# Frameworks:\n",
    "import lightgbm as lgb # LightGBM\n",
    "# Utils:\n",
    "import gc # garbage collector\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IDIR = '../input/' # main path\n",
    "members = pd.read_csv(IDIR + 'members.csv')\n",
    "songs = pd.read_csv(IDIR + 'songs.csv')\n",
    "song_extra_info = pd.read_csv(IDIR + 'song_extra_info.csv')\n",
    "train = pd.read_csv(IDIR + 'train.csv')\n",
    "test = pd.read_csv(IDIR + 'test.csv')\n",
    "\n",
    "# Adding songs' info:\n",
    "train_aug1 = pd.merge(left=train, right=songs, on='song_id', how='left')\n",
    "test_aug1 = pd.merge(left=test, right=songs, on='song_id', how='left')\n",
    "# Adding extra info about songs:\n",
    "train_aug2 = pd.merge(left=train_aug1, right=song_extra_info, on='song_id', how='left')\n",
    "test_aug2 = pd.merge(left=test_aug1, right=song_extra_info, on='song_id', how='left')\n",
    "del train_aug1, test_aug1\n",
    "# Addind users' info:\n",
    "train_aug3 = pd.merge(left=train_aug2, right=members, on='msno', how='left')\n",
    "test_aug3 = pd.merge(left=test_aug2, right=members, on='msno', how='left')\n",
    "del train_aug2, test_aug2\n",
    "# Merging train and test data:\n",
    "train_aug3.drop(['song_id'], axis=1, inplace=True)\n",
    "train_aug3['set'] = 0\n",
    "test_aug3.drop(['song_id'], axis=1, inplace=True)\n",
    "test_aug3['set'] = 1\n",
    "test_aug3['target'] = -1\n",
    "all_aug = pd.concat([train_aug3, test_aug3], axis=0)\n",
    "del train_aug3, test_aug3\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "\n",
    "# source_system_tab/source_screen_name/source_type/genre_ids/artist_name/composer/lyricist/name/isrc/gender 用'NA'填补并one-hot编码\n",
    "# genre_ids encoding:\n",
    "all_aug['genre_ids'] = all_aug.genre_ids.fillna('NA')\n",
    "all_aug['genre_ids'] = all_aug.genre_ids.astype(np.str)\n",
    "genre_ids_le = LabelEncoder()\n",
    "genre_ids_le.fit(all_aug.genre_ids)\n",
    "all_aug['genre_ids'] = genre_ids_le.transform(all_aug.genre_ids).astype(np.int16)\n",
    "\n",
    "# language encoding:\n",
    "all_aug['language'] = all_aug.language.fillna(-2)\n",
    "all_aug['language'] = all_aug.language.astype(np.int8)\n",
    "\n",
    "# city encoding:\n",
    "all_aug['city'] = all_aug.city.astype(np.int8)\n",
    "# bd encoding:\n",
    "all_aug['bd'] = all_aug.bd.astype(np.int16)\n",
    "\n",
    "# registered_via encoding:\n",
    "all_aug['registered_via'] = all_aug.registered_via.astype(np.int8)\n",
    "# registration_init_time encoding:\n",
    "all_aug['registration_init_time'] = all_aug.registration_init_time.astype(np.int32)\n",
    "# expiration_date encoding:\n",
    "all_aug['expiration_date'] = all_aug.expiration_date.astype(np.int32)\n",
    "# Info:\n",
    "all_aug.info(max_cols=0)\n",
    "all_aug.head(2)\n",
    "\n",
    "\n",
    "all_aug['exp_reg_time'] = all_aug.expiration_date - all_aug.registration_init_time\n",
    "\n",
    "\n",
    "\n",
    "gc.collect();\n",
    "d_train = lgb.Dataset(all_aug[all_aug.set == 0].drop(['target', 'msno', 'id', 'set'], axis=1), \n",
    "                      label=all_aug[all_aug.set == 0].pop('target'))\n",
    "ids_train = all_aug[all_aug.set == 0].pop('msno')\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 1.0,\n",
    "    'max_depth': 15,\n",
    "    'num_leaves': 250, \n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc'},\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 5,\n",
    "    'max_bin': 100}\n",
    "cv_result_lgb = lgb.cv(lgb_params, \n",
    "                       d_train, \n",
    "                       num_boost_round=5000, \n",
    "                       nfold=3, \n",
    "                       stratified=True, \n",
    "                       early_stopping_rounds=50, \n",
    "                       verbose_eval=100, \n",
    "                       show_stdv=True)\n",
    "\n",
    "num_boost_rounds_lgb = len(cv_result_lgb['auc-mean'])\n",
    "print('num_boost_rounds_lgb=' + str(num_boost_rounds_lgb))\n",
    "\n",
    "\n",
    "\n",
    "%%time\n",
    "ROUNDS = num_boost_rounds_lgb\n",
    "print('light GBM train :-)')\n",
    "bst = lgb.train(lgb_params, d_train, ROUNDS)\n",
    "# lgb.plot_importance(bst, figsize=(9,20))\n",
    "# del d_train\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "feature_imp = pd.Series(dict(zip(d_train.feature_name, \n",
    "                                 bst.feature_importance()))).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_imp.values, y=feature_imp.index.values, orient='h', color='g')\n",
    "plt.subplot(1,2,2)\n",
    "train_scores = np.array(cv_result_lgb['auc-mean'])\n",
    "train_stds = np.array(cv_result_lgb['auc-stdv'])\n",
    "plt.plot(train_scores, color='green')\n",
    "plt.fill_between(range(len(cv_result_lgb['auc-mean'])), \n",
    "                 train_scores - train_stds, train_scores + train_stds, \n",
    "                 alpha=0.1, color='green')\n",
    "plt.title('LightGMB CV-results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
