{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "get_ipython().magic('env CUDA_VISIBLE_DEVICES =  ')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp      # will come in handy due to the size of the data\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import io\n",
    "from datetime import datetime\n",
    "import gc # garbage collector\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import logging\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a pandas dataframe to disk as gunzip compressed csv\n",
    "- df.to_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Read from disk\n",
    "- df = pd.read_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Magic useful\n",
    "- %%timeit for the whole cell\n",
    "- %timeit for the specific line\n",
    "- %%latex to render the cell as a block of latex\n",
    "- %prun and %%prun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/WSDM/'\n",
    "HDF_FILENAME = DATASET_PATH + 'datas.h5'\n",
    "SUBMISSION_FILENAME = DATASET_PATH + 'submission_{}.csv'\n",
    "VALIDATION_INDICE = DATASET_PATH + 'validation_indice.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_logging(logger_name, logger_file_name):\n",
    "    log = logging.getLogger(logger_name)\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create formatter and add it to the handlers\n",
    "    print_formatter = logging.Formatter('%(message)s')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s_%(levelname)s: %(message)s')\n",
    "\n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(logger_file_name, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(file_formatter)\n",
    "    log.addHandler(fh)\n",
    "    # both output to console and file\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(print_formatter)\n",
    "    log.addHandler(consoleHandler)\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "here is an info message.\n",
      "here is an info message.\n"
     ]
    }
   ],
   "source": [
    "log = set_logging('MUSIC', DATASET_PATH + 'music_gbm.log')\n",
    "log.info('here is an info message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN_FILE = DATASET_PATH + 'train.csv'\n",
    "# TEST_FILE = DATASET_PATH + 'test.csv'\n",
    "# MEMBER_FILE = DATASET_PATH + 'members.csv'\n",
    "# SONG_FILE = DATASET_PATH + 'fix_songs.csv'\n",
    "# SONG_EXTRA_FILE = DATASET_PATH + 'song_extra_info.csv'\n",
    "\n",
    "# train_data = pd.read_csv(TRAIN_FILE)\n",
    "# test_data = pd.read_csv(TEST_FILE)\n",
    "# member_data = pd.read_csv(MEMBER_FILE)\n",
    "# song_data = pd.read_csv(SONG_FILE)\n",
    "# song_extra_data = pd.read_csv(SONG_EXTRA_FILE)\n",
    "\n",
    "# songs_all = pd.merge(left = song_data, right = song_extra_data, how = 'left', on='song_id')\n",
    "# train_with_mem = pd.merge(left = train_data, right = member_data, how = 'left', on='msno')\n",
    "# train_all = pd.merge(left = train_with_mem, right = songs_all, how = 'left', on='song_id')\n",
    "# test_with_mem = pd.merge(left = test_data, right = member_data, how = 'left', on='msno')\n",
    "# test_all = pd.merge(left = test_with_mem, right = songs_all, how = 'left', on='song_id')\n",
    "# del train_with_mem, test_with_mem; gc.collect()\n",
    "\n",
    "# def convert_unicode_to_str(df):\n",
    "#     df.columns = df.columns.astype(str)\n",
    "#     types = df.apply(lambda x: pd.api.types.infer_dtype(df.values))\n",
    "#     #print(types)#mixed-integer\n",
    "#     for col in types[types == 'mixed-integer'].index:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     for col in types[types == 'mixed'].index:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     return df\n",
    "\n",
    "# store = pd.HDFStore(HDF_FILENAME)\n",
    "# store['train_data'] = convert_unicode_to_str(train_all)\n",
    "# store['test_data'] = convert_unicode_to_str(test_all)\n",
    "# store['song_data'] = convert_unicode_to_str(songs_all)\n",
    "# store['test_id'] = test_data.id\n",
    "# store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_test = pd.HDFStore(HDF_FILENAME)\n",
    "# train = store_test['train_data'][0:100]\n",
    "# test = store_test['test_data'][0:100]\n",
    "# test_id =  store_test['test_id'][0:100]\n",
    "# store_test.close()\n",
    "store_test = pd.HDFStore(HDF_FILENAME)\n",
    "train = store_test['train_data']\n",
    "test = store_test['test_data']\n",
    "test_id =  store_test['test_id']\n",
    "store_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_country(input_data):\n",
    "    def get_country(isrc):\n",
    "        if isinstance(isrc, str) and isrc != 'nan':\n",
    "            return isrc[0:2]\n",
    "        else:\n",
    "            return np.nan\n",
    "    countries = train['isrc'].apply(get_country)\n",
    "    country_list = list(countries.value_counts().index)\n",
    "    country_map = dict(zip(country_list, country_list))\n",
    "    country_map['QM'] = 'QZ'\n",
    "    country_map['US'] = 'QZ'\n",
    "    return countries.map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['country'] = split_country(train)\n",
    "test['country'] = split_country(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if isinstance(isrc, str) and isrc != 'nan':\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "train['song_year'] = train['isrc'].apply(isrc_to_year)\n",
    "test['song_year'] = test['isrc'].apply(isrc_to_year)\n",
    "train.drop(['isrc'], axis = 1, inplace = True)\n",
    "test.drop(['isrc'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_reg_date(input_data):\n",
    "    input_data['registration_year'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[0:4]))\n",
    "    input_data['registration_year'] = pd.to_numeric(input_data['registration_year'], downcast='unsigned')\n",
    "\n",
    "    input_data['registration_month'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[4:6]))\n",
    "    input_data['registration_month'] = pd.to_numeric(input_data['registration_month'], downcast='unsigned')\n",
    "\n",
    "    input_data['registration_day'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[6:8]))\n",
    "    input_data['registration_day'] = pd.to_numeric(input_data['registration_day'], downcast='unsigned')\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_expir_date(input_data):\n",
    "    input_data['expiration_year'] = input_data['expiration_date'].apply(lambda x : int(str(x)[0:4]))\n",
    "    input_data['expiration_year'] = pd.to_numeric(input_data['expiration_year'], downcast='unsigned')\n",
    "\n",
    "    input_data['expiration_month'] = input_data['expiration_date'].apply(lambda x : int(str(x)[4:6]))\n",
    "    input_data['expiration_month'] = pd.to_numeric(input_data['expiration_month'], downcast='unsigned')\n",
    "\n",
    "    input_data['expiration_day'] = input_data['expiration_date'].apply(lambda x : int(str(x)[6:8]))\n",
    "    input_data['expiration_day'] = pd.to_numeric(input_data['expiration_day'], downcast='unsigned')\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_to_day(input_data):\n",
    "    # 转换注册时间\n",
    "    input_data['registration_init_time'] = pd.to_datetime(input_data['registration_init_time'],format=\"%Y%m%d\")\n",
    "    input_data['expiration_date'] = pd.to_datetime(input_data['expiration_date'],format=\"%Y%m%d\")\n",
    "    days = input_data.expiration_date - input_data.registration_init_time\n",
    "    days = [d.days for d in days]\n",
    "    input_data['days']=days\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = split_reg_date(train)\n",
    "test = split_reg_date(test)\n",
    "train = split_expir_date(train)\n",
    "test = split_expir_date(test)\n",
    "\n",
    "train = date_to_day(train)\n",
    "test = date_to_day(test)\n",
    "\n",
    "train.drop('registration_init_time',axis=1,inplace=True)\n",
    "train.drop('expiration_date',axis=1,inplace=True)\n",
    "test.drop('registration_init_time',axis=1,inplace=True)\n",
    "test.drop('expiration_date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['song_length'] = pd.to_numeric(train['song_length'].replace('nan', '235415'), downcast='unsigned')\n",
    "test['song_length'] = pd.to_numeric(test['song_length'].replace('nan', '235415'), downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno : object\n",
      "song_id : object\n",
      "source_system_tab : object\n",
      "source_screen_name : object\n",
      "source_type : object\n",
      "target : object\n",
      "city : object\n",
      "bd : object\n",
      "gender : object\n",
      "registered_via : object\n",
      "song_length : uint32\n",
      "genre_ids : object\n",
      "artist_name : object\n",
      "composer : object\n",
      "lyricist : object\n",
      "language : object\n",
      "name : object\n",
      "country : object\n",
      "song_year : float64\n",
      "registration_year : uint16\n",
      "registration_month : uint8\n",
      "registration_day : uint8\n",
      "expiration_year : uint16\n",
      "expiration_month : uint8\n",
      "expiration_day : uint8\n",
      "days : int64\n"
     ]
    }
   ],
   "source": [
    "for col in train.columns: print(col, ':', train[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in [col for col in test.columns if col != 'id' ]:\n",
    "    if train[col].dtype == object:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode registered_via, the less number of occurrences are merged into the top item which has the max number of occurrences\n",
    "# registered_via_hist = pd.concat([train['registered_via'], test['registered_via']], axis = 0).value_counts()\n",
    "# registered_via_map = dict(zip(registered_via_hist.index, [int(s) for s in registered_via_hist.index.values]))\n",
    "# registered_via_map[registered_via_hist.index[-1]] = int(str(registered_via_hist.index.values[0]))\n",
    "# train['registered_via'] = train['registered_via'].map(registered_via_map)\n",
    "# test['registered_via'] = test['registered_via'].map(registered_via_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode language, fill nan with most occurrences item\n",
    "# language_hist = pd.concat([train['language'], test['language']], axis = 0).value_counts()\n",
    "# language_map = dict(zip(language_hist.index, [int(float(s)) for s in language_hist.index.values if s != 'nan']))\n",
    "# language_map['nan'] = int(float(str(language_hist.index.values[0])))\n",
    "# train['language'] = train['language'].map(language_map)\n",
    "# test['language'] = test['language'].map(language_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode country, fill nan with most occurrences item\n",
    "# country_hist = pd.concat([train['country'], test['country']], axis = 0).value_counts()\n",
    "# merge_per = 0.25\n",
    "# country_map = dict(zip(country_hist.index, list(range(len(country_hist)))))\n",
    "# for key in list(country_hist[-int(len(country_hist)*merge_per):].index):\n",
    "#     country_map[key] = int(len(country_hist)*(1-merge_per)) + 1\n",
    "# train['country'] = train['country'].map(country_map)\n",
    "# test['country'] = test['country'].map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# msno : category ; uinque values: 30755\n",
    "# song_id : category ; uinque values: 359966\n",
    "# - source_system_tab : category ; uinque values: 10\n",
    "# - source_screen_name : category ; uinque values: 21\n",
    "# - source_type : category ; uinque values: 13\n",
    "# - target : object ; uinque values: 2\n",
    "# - city : category ; uinque values: 21\n",
    "# - bd : category ; uinque values: 92\n",
    "# - gender : category ; uinque values: 3\n",
    "# - registered_via : category ; uinque values: 5\n",
    "# song_length : uint32 ; uinque values: 60271\n",
    "# genre_ids : category ; uinque values: 573\n",
    "# artist_name : category ; uinque values: 40587\n",
    "# composer : category ; uinque values: 76072\n",
    "# lyricist : category ; uinque values: 33895\n",
    "# - language : category ; uinque values: 11\n",
    "# name : category ; uinque values: 234144\n",
    "# - country : category ; uinque values: 107\n",
    "# - song_year : float64 ; uinque values: 100\n",
    "# - registration_year : uint16 ; uinque values: 14\n",
    "# - registration_month : uint8 ; uinque values: 12\n",
    "# - registration_date : uint8 ; uinque values: 31\n",
    "# - expiration_year : uint16 ; uinque values: 18\n",
    "# - expiration_month : uint8 ; uinque values: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_transform(input_train_data, input_test_data, columns_to_transform):\n",
    "    for col in columns_to_transform:\n",
    "        le = LabelEncoder()\n",
    "        train_values = list(input_train_data[col].unique())\n",
    "        test_values = list(input_test_data[col].unique())\n",
    "        le.fit(train_values + test_values)\n",
    "        input_train_data[col] = le.transform(input_train_data[col])\n",
    "        input_test_data[col] = le.transform(input_test_data[col])\n",
    "    return input_train_data, input_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train, test = one_hot_transform(train, test, ['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender', 'name'])#, 'artist_name', 'composer', 'lyricist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: wether song_id should be merged like this or not? 231475 reserved and 188364 merged\n",
    "def encode_with_merge(input_train, input_test, columns, merge_value):\n",
    "    for index, col in enumerate(columns):\n",
    "        values_hist = pd.concat([input_train[col], input_train[col]], axis = 0).value_counts()\n",
    "        reserve_rows = values_hist[values_hist!=merge_value[index]]\n",
    "        merge_rows = values_hist[values_hist==merge_value[index]]\n",
    "\n",
    "        reserve_dict = dict(zip(list(reserve_rows.index), list(range(len(reserve_rows)))))\n",
    "        merge_dict = dict(zip(list(merge_rows.index), [len(reserve_rows)+1]*len(merge_rows.index)))\n",
    "        \n",
    "        map_dict = {**reserve_dict, **merge_dict}\n",
    "        \n",
    "        language_map['nan'] = int(float(str(language_hist.index.values[0])))\n",
    "        input_train[col] = input_train[col].map(map_dict)\n",
    "        input_test[col] = input_test[col].map(map_dict)\n",
    "    return input_train, input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train, test = encode_with_merge(train, test, ['msno', 'song_id', 'genre_ids'], [1, 1, 1])\n",
    "# print(train.head())\n",
    "# print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_org, test_org = train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_org.copy(deep=True)\n",
    "test = test_org.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_test = pd.HDFStore(VALIDATION_INDICE)\n",
    "validation_list = store_test['keep_index']['index'].values\n",
    "store_test.close()\n",
    "train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "#validation_use = train.iloc[validation_list].copy(deep=True).reset_index(drop=True)\n",
    "validation_use = train.iloc[list(range(7277417, 7377417))].copy(deep=True).reset_index(drop=True)\n",
    "#train_use = train.drop(validation_list)\n",
    "train_use = train.drop(list(range(7277417, 7377417)))\n",
    "# train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "# validation_use = train[50:].copy(deep=True).reset_index(drop=True)\n",
    "# train_use = train.drop(list(range(50,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(train_data, validation_data, test_data):\n",
    "    train_data['song_length'] = np.log(pd.to_numeric(train_data['song_length'], downcast='float') + 1)\n",
    "    validation_data['song_length'] = np.log(pd.to_numeric(validation_data['song_length'], downcast='float') + 1)\n",
    "    test_data['song_length'] = np.log(pd.to_numeric(test_data['song_length'], downcast='float') + 1)\n",
    "    return train_data, validation_data, test_data\n",
    "train_use, validation_use, test = log_transform(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_composer_hot_rate(train_data, val_data, test_data):\n",
    "    \n",
    "    temp_data = pd.concat([train_data[['composer']], val_data[['composer']], test_data[['composer']]], axis=0, join=\"outer\")\n",
    "    temp_data['composer'] = temp_data['composer'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['composer'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['composer_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    temp_data = pd.concat([df_temp['composer_0'], df_temp['composer_1'], df_temp['composer_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "\n",
    "    composer_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    composer_hot['nan'] = 0.\n",
    "    composer_hot['nan'] = composer_hot.mean()\n",
    "    #print(composer_hot)\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['composer'] = input_data['composer'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['composer'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['composer_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['composer_0', 'composer_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_0')\n",
    "        hot_hist.columns = ['composer_1', 'composer_1_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_1')\n",
    "        hot_hist.columns = ['composer_2', 'composer_2_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_2')\n",
    "        df_temp['composer_score'] = df_temp[['composer_0_score','composer_1_score','composer_2_score']].max(axis=1)\n",
    "        #df_temp['composer_score'] = df_temp['composer_0_score']\n",
    "        \n",
    "        input_data['composer_score'] = df_temp['composer_score']\n",
    "        input_data.drop('composer', inplace=True, axis = 1)\n",
    "        #input_data = input_data.drop('composer', inplace=False, axis = 1)\n",
    "        input_data['composer'] = df_temp['composer_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, composer_hot)\n",
    "    val_data = encoder_each(val_data, composer_hot)\n",
    "    test_data = encoder_each(test_data, composer_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_composer_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_lyricist_hot_rate(train_data, val_data, test_data):\n",
    "    temp_data = pd.concat([train_data[['lyricist']], val_data[['lyricist']], test_data[['lyricist']]], axis=0, join=\"outer\")\n",
    "    temp_data['lyricist'] = temp_data['lyricist'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['lyricist'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['lyricist_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    #temp_data = df_temp['lyricist_0']\n",
    "    temp_data = pd.concat([df_temp['lyricist_0'], df_temp['lyricist_1'], df_temp['lyricist_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "    lyricist_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    lyricist_hot['nan'] = 0.\n",
    "    lyricist_hot['nan'] = lyricist_hot.mean()\n",
    "\n",
    "    #print(lyricist_hot)\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['lyricist'] = input_data['lyricist'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['lyricist'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['lyricist_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['lyricist_0', 'lyricist_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='lyricist_0')\n",
    "        \n",
    "        df_temp['lyricist_score'] = df_temp['lyricist_0_score']\n",
    "        \n",
    "        input_data['lyricist_score'] = df_temp['lyricist_score']\n",
    "        input_data.drop('lyricist', inplace=True, axis = 1)\n",
    "        input_data['lyricist'] = df_temp['lyricist_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, lyricist_hot)\n",
    "    val_data = encoder_each(val_data, lyricist_hot)\n",
    "    test_data = encoder_each(test_data, lyricist_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_lyricist_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_artist_hot_rate(train_data, val_data, test_data):\n",
    "    temp_data = pd.concat([train_data[['artist_name']], val_data[['artist_name']], test_data[['artist_name']]], axis=0, join=\"outer\")\n",
    "    temp_data['artist_name'] = temp_data['artist_name'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['artist_name'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['artist_name_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    #temp_data = df_temp['artist_name_0']\n",
    "    temp_data = pd.concat([df_temp['artist_name_0'], df_temp['artist_name_1'], df_temp['artist_name_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "    artist_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    artist_hot['nan'] = 0.\n",
    "    artist_hot['nan'] = artist_hot.mean()\n",
    "    #print(artist_hot)\n",
    "\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['artist_name'] = input_data['artist_name'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['artist_name'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['artist_name_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['artist_name_0', 'artist_name_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='artist_name_0')\n",
    "        \n",
    "        df_temp['artist_name_score'] = df_temp['artist_name_0_score']\n",
    "        \n",
    "        input_data['artist_name_score'] = df_temp['artist_name_score']\n",
    "        input_data.drop('artist_name', inplace=True, axis = 1)\n",
    "        input_data['artist_name'] = df_temp['artist_name_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, artist_hot)\n",
    "    val_data = encoder_each(val_data, artist_hot)\n",
    "    test_data = encoder_each(test_data, artist_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_artist_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['msno', 'song_id', 'source_system_tab', 'source_screen_name',\n",
      "       'source_type', 'target', 'city', 'bd', 'gender', 'registered_via',\n",
      "       'song_length', 'genre_ids', 'language', 'name', 'country', 'song_year',\n",
      "       'registration_year', 'registration_month', 'registration_day',\n",
      "       'expiration_year', 'expiration_month', 'expiration_day', 'days',\n",
      "       'composer_score', 'composer', 'lyricist_score', 'lyricist',\n",
      "       'artist_name_score', 'artist_name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_use.head().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = pd.concat([train_use[['composer']], validation_use[['composer']], test[['composer']]], axis=0, join=\"inner\")\n",
    "\n",
    "# temp_data['composer'].apply(lambda x : len(x.replace(u'、','|').split('|'))).value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train_use.head(100000).copy(deep=True)\n",
    "# print(df['lyricist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_wnd = [2018, 0]\n",
    "#time_wnd = [2018, 0]#, 2000, 2010, 2014, 2018]\n",
    "def cal_song_listen_times(train_data, test_data, val_data):\n",
    "    all_data = pd.concat([train_data[['song_id', 'song_year', 'msno']], val_data[['song_id', 'song_year', 'msno']], test_data[['song_id', 'song_year', 'msno']]], axis=0, join=\"inner\")\n",
    "    #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "    #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "    for index, _ in enumerate(time_wnd[:-1]):\n",
    "        begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "#         begin_time = time_wnd[index]\n",
    "#         end_time = time_wnd[index+1]\n",
    "        select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "        #select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "        grouped = select_data[['song_id', 'msno']].groupby(['song_id'])\n",
    "\n",
    "        count_song = grouped.agg(['count'])\n",
    "        num_people_per_song = grouped.agg({\"msno\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "        popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "        popularity.columns = ['popular_{}'.format(index), 'num_people_{}'.format(index)]\n",
    "        popularity = popularity.reset_index(drop=False)\n",
    "        train_data = train_data.merge(popularity, on='song_id', how ='left')\n",
    "        test_data = test_data.merge(popularity, on='song_id', how ='left')\n",
    "        val_data = val_data.merge(popularity, on='song_id', how ='left')\n",
    "    return train_data, test_data, val_data\n",
    "def cal_song_listen_times_seperate(train_data, test_data, val_data):\n",
    "    def cal_each_of_them(input_data):\n",
    "        all_data = input_data[['song_id', 'song_year', 'msno']]\n",
    "        #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "        #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "        for index, _ in enumerate(time_wnd[:-1]):\n",
    "            begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "    #         begin_time = time_wnd[index]\n",
    "    #         end_time = time_wnd[index+1]\n",
    "            select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "            grouped = select_data[['song_id', 'msno']].groupby(['song_id'])\n",
    "\n",
    "            count_song = grouped.agg(['count'])\n",
    "            num_people_per_song = grouped.agg({\"msno\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "            popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "            popularity.columns = ['popular_{}'.format(index), 'num_people_{}'.format(index)]\n",
    "            popularity = popularity.reset_index(drop=False)\n",
    "            all_data = input_data.merge(popularity, on='song_id', how ='left')\n",
    "        return all_data\n",
    "    return cal_each_of_them(train_data), cal_each_of_them(test_data), cal_each_of_them(val_data)\n",
    "\n",
    "# time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "# def cal_song_listen_times(train_data, test_data):\n",
    "#     test_data['song_id'] = pd.to_numeric(test_data['song_id'], downcast='unsigned')\n",
    "#     for index, _ in enumerate(time_wnd[:-1]):\n",
    "#         begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "# #         begin_time = time_wnd[index]\n",
    "# #         end_time = time_wnd[index+1]\n",
    "#         select_data = train_data[train_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "#         select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "#         grouped = select_data[['song_id', 'target']].groupby(['song_id'])\n",
    "\n",
    "#         count_song = grouped.agg(['count'])\n",
    "#         mean_repeat_song = grouped['target'].mean()\n",
    "\n",
    "#         popularity = pd.concat([np.log(count_song+1), mean_repeat_song, np.log(count_song.multiply(mean_repeat_song, axis=0)+1)], axis=1, join=\"inner\")\n",
    "#         popularity.columns = ['popular_{}'.format(index), 'mean_repeat_{}'.format(index), 'replay_prob_{}'.format(index)]\n",
    "#         popularity = popularity.reset_index(drop=False)\n",
    "#         test_data = test_data.merge(popularity, on='song_id', how ='left')\n",
    "#         train_data = train_data.merge(popularity, on='song_id', how ='left')\n",
    "#     return train_data, test_data\n",
    "\n",
    "\n",
    "# time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "# def cal_song_listen_times(train_data):\n",
    "#     train_data['song_id'] = pd.to_numeric(train_data['song_id'], downcast='unsigned')\n",
    "#     for index, _ in enumerate(time_wnd[:-1]):\n",
    "#         begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "# #         begin_time = time_wnd[index]\n",
    "# #         end_time = time_wnd[index+1]\n",
    "#         select_data = train_data[train_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "#         select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "#         grouped = select_data[['song_id', 'target']].groupby(['song_id'])\n",
    "\n",
    "#         count_song = grouped.agg(['count'])\n",
    "#         mean_repeat_song = grouped['target'].mean()\n",
    "\n",
    "#         popularity = pd.concat([np.log(count_song+1), mean_repeat_song, np.log(count_song.multiply(mean_repeat_song, axis=0)+1)], axis=1, join=\"inner\")\n",
    "#         popularity.columns = ['popular_{}'.format(index), 'mean_repeat_{}'.format(index), 'replay_prob_{}'.format(index)]\n",
    "#         popularity = popularity.reset_index(drop=False)\n",
    "#         train_data = train_data.merge(popularity, on='song_id', how ='left')\n",
    "#     return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, test, validation_use = cal_song_listen_times(train_use, test, validation_use)\n",
    "# train = cal_song_listen_times(train)\n",
    "# test = cal_song_listen_times(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people_time_wnd = [2018, 0]\n",
    "#people_time_wnd = [2018, 0]#, 2000, 2010, 2014, 2018]\n",
    "def get_people_active(train_data, test_data, val_data):\n",
    "    all_data = pd.concat([train_data[['song_id', 'song_year', 'msno']], val_data[['song_id', 'song_year', 'msno']], test_data[['song_id', 'song_year', 'msno']]], axis=0, join=\"inner\")\n",
    "    #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "    #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "    for index, _ in enumerate(people_time_wnd[:-1]):\n",
    "        begin_time, end_time = people_time_wnd[index] < people_time_wnd[index+1] and (people_time_wnd[index], people_time_wnd[index+1]) or (people_time_wnd[index+1], people_time_wnd[index])\n",
    "#         begin_time = time_wnd[index]\n",
    "#         end_time = time_wnd[index+1]\n",
    "        select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "        #select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "        grouped = select_data[['song_id', 'msno']].groupby(['msno'])\n",
    "\n",
    "        count_song = grouped.agg(['count'])\n",
    "        num_people_per_song = grouped.agg({\"song_id\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "        popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "        popularity.columns = ['active_{}'.format(index), 'num_song_{}'.format(index)]\n",
    "        popularity = popularity.reset_index(drop=False)\n",
    "        train_data = train_data.merge(popularity, on='msno', how ='left')\n",
    "        test_data = test_data.merge(popularity, on='msno', how ='left')\n",
    "        val_data = val_data.merge(popularity, on='msno', how ='left')\n",
    "    return train_data, test_data, val_data\n",
    "def get_people_active_seperate(train_data, test_data, val_data):\n",
    "    def cal_each_of_them(input_data):\n",
    "        all_data = input_data[['song_id', 'song_year', 'msno']]\n",
    "        #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "        #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "        for index, _ in enumerate(people_time_wnd[:-1]):\n",
    "            begin_time, end_time = people_time_wnd[index] < people_time_wnd[index+1] and (people_time_wnd[index], people_time_wnd[index+1]) or (people_time_wnd[index+1], people_time_wnd[index])\n",
    "    #         begin_time = time_wnd[index]\n",
    "    #         end_time = time_wnd[index+1]\n",
    "            select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "            grouped = select_data[['song_id', 'msno']].groupby(['msno'])\n",
    "\n",
    "            count_song = grouped.agg(['count'])\n",
    "            num_people_per_song = grouped.agg({\"song_id\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "            popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "            popularity.columns = ['active_{}'.format(index), 'num_song_{}'.format(index)]\n",
    "            popularity = popularity.reset_index(drop=False)\n",
    "            all_data = input_data.merge(popularity, on='msno', how ='left')\n",
    "        return all_data\n",
    "    return cal_each_of_them(train_data), cal_each_of_them(test_data), cal_each_of_them(val_data)\n",
    "\n",
    "# test = test.reset_index(drop=False)\n",
    "# #test['msno'] = test['msno'].astype(int)   \n",
    "# train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "\n",
    "# grouped = train[['msno', 'target']].groupby(['msno'])\n",
    "\n",
    "# count_msno = grouped.agg(['count'])\n",
    "# mean_repeat_msno = grouped['target'].mean()\n",
    "\n",
    "# popularity = pd.concat([np.log(count_msno+1), mean_repeat_msno, np.log(count_msno.multiply(mean_repeat_msno, axis=0)+1)], axis=1, join=\"inner\")\n",
    "# popularity.columns = ['ms_popular', 'ms_mean_repeat', 'ms_replay_prob']\n",
    "# popularity = popularity.reset_index(drop=False)\n",
    "# test = test.merge(popularity, on='msno', how ='left')\n",
    "# train = train.merge(popularity, on='msno', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, test, validation_use = get_people_active(train_use, test, validation_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_by_different_city_lang_country(train_data, test_data, val_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']], val_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']], test_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "    count_dict = dict()\n",
    "    \n",
    "    for col in ['composer', 'lyricist', 'artist_name']:\n",
    "        temp_df = None\n",
    "        for target in ['city', 'country', 'language']:\n",
    "            grouped = temp_msno_songid.groupby([col])\n",
    "            df = grouped.agg({target: lambda x: x.nunique()})\n",
    "            \n",
    "            if temp_df is not None:\n",
    "                temp_df = pd.concat([temp_df, df], axis=1, join=\"inner\")\n",
    "            else:\n",
    "                temp_df = df\n",
    "\n",
    "        temp_df.columns = [col + '_by_{}'.format(index) for index in target]\n",
    "        \n",
    "        \n",
    "        temp_msno_songid['composer'] = df_temp['composer_0']\n",
    "\n",
    "    \n",
    "\n",
    "    #count_song = grouped.agg(['count'])\n",
    "    num_people_per_song = grouped.agg({\"msno\": lambda x: x.nunique()})\n",
    "    \n",
    "    print(num_people_per_song)\n",
    "    num_people_per_song = grouped.agg({\"song_id\": lambda x:  x.nunique()})#np.log(x.nunique()+1)})\n",
    "    print(num_people_per_song)\n",
    "    return \n",
    "    temp_data = df_temp['composer_0']\n",
    "    print(temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender',\\\n",
    "            'name', 'artist_name', 'composer', 'lyricist', 'msno', 'song_id', 'genre_ids',\\\n",
    "           'country', 'language', 'registered_via']:\n",
    "    train_use[col] = train_use[col].astype('category')\n",
    "    validation_use[col] = validation_use[col].astype('category')\n",
    "    test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno : category ; uinque values: 30571\n",
      "song_id : category ; uinque values: 357496\n",
      "source_system_tab : category ; uinque values: 10\n",
      "source_screen_name : category ; uinque values: 21\n",
      "source_type : category ; uinque values: 13\n",
      "target : int8 ; uinque values: 2\n",
      "city : category ; uinque values: 21\n",
      "bd : category ; uinque values: 92\n",
      "gender : category ; uinque values: 3\n",
      "registered_via : category ; uinque values: 5\n",
      "song_length : float32 ; uinque values: 60028\n",
      "genre_ids : category ; uinque values: 573\n",
      "language : category ; uinque values: 11\n",
      "name : category ; uinque values: 234144\n",
      "country : category ; uinque values: 107\n",
      "song_year : float64 ; uinque values: 100\n",
      "registration_year : uint16 ; uinque values: 14\n",
      "registration_month : uint8 ; uinque values: 12\n",
      "registration_day : uint8 ; uinque values: 31\n",
      "expiration_year : uint16 ; uinque values: 18\n",
      "expiration_month : uint8 ; uinque values: 12\n",
      "expiration_day : uint8 ; uinque values: 31\n",
      "days : int64 ; uinque values: 4319\n",
      "composer_score : float64 ; uinque values: 2218\n",
      "composer : category ; uinque values: 49088\n",
      "lyricist_score : float64 ; uinque values: 1652\n",
      "lyricist : category ; uinque values: 24206\n",
      "artist_name_score : float64 ; uinque values: 1842\n",
      "artist_name : category ; uinque values: 39236\n",
      "popular_0 : float64 ; uinque values: 2041\n",
      "num_people_0 : float64 ; uinque values: 2003\n",
      "active_0 : float64 ; uinque values: 1814\n",
      "num_song_0 : float64 ; uinque values: 1772\n"
     ]
    }
   ],
   "source": [
    "for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : object ; uinque values: 2556790\n",
      "msno : category ; uinque values: 25131\n",
      "song_id : category ; uinque values: 224753\n",
      "source_system_tab : category ; uinque values: 10\n",
      "source_screen_name : category ; uinque values: 23\n",
      "source_type : category ; uinque values: 13\n",
      "city : category ; uinque values: 21\n",
      "bd : category ; uinque values: 89\n",
      "gender : category ; uinque values: 3\n",
      "registered_via : category ; uinque values: 6\n",
      "song_length : float32 ; uinque values: 45659\n",
      "genre_ids : category ; uinque values: 502\n",
      "language : category ; uinque values: 11\n",
      "name : category ; uinque values: 154716\n",
      "country : category ; uinque values: 94\n",
      "song_year : float64 ; uinque values: 100\n",
      "registration_year : uint16 ; uinque values: 14\n",
      "registration_month : uint8 ; uinque values: 12\n",
      "registration_day : uint8 ; uinque values: 31\n",
      "expiration_year : uint16 ; uinque values: 16\n",
      "expiration_month : uint8 ; uinque values: 12\n",
      "expiration_day : uint8 ; uinque values: 31\n",
      "days : int64 ; uinque values: 4240\n",
      "composer_score : float64 ; uinque values: 2207\n",
      "composer : category ; uinque values: 35089\n",
      "lyricist_score : float64 ; uinque values: 1654\n",
      "lyricist : category ; uinque values: 18137\n",
      "artist_name_score : float64 ; uinque values: 1848\n",
      "artist_name : category ; uinque values: 26902\n",
      "popular_0 : float64 ; uinque values: 2083\n",
      "num_people_0 : float64 ; uinque values: 2045\n",
      "active_0 : float64 ; uinque values: 1811\n",
      "num_song_0 : float64 ; uinque values: 1767\n"
     ]
    }
   ],
   "source": [
    "for col in test.columns: print(col, ':', test[col].dtype, '; uinque values:', len(test[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno : category ; uinque values: 30571\n",
      "song_id : category ; uinque values: 357496\n",
      "source_system_tab : category ; uinque values: 10\n",
      "source_screen_name : category ; uinque values: 21\n",
      "source_type : category ; uinque values: 13\n",
      "target : int8 ; uinque values: 2\n",
      "city : category ; uinque values: 21\n",
      "bd : category ; uinque values: 92\n",
      "gender : category ; uinque values: 3\n",
      "registered_via : category ; uinque values: 5\n",
      "song_length : float32 ; uinque values: 60028\n",
      "genre_ids : category ; uinque values: 573\n",
      "language : category ; uinque values: 11\n",
      "name : category ; uinque values: 234144\n",
      "country : category ; uinque values: 107\n",
      "song_year : float64 ; uinque values: 100\n",
      "registration_year : uint16 ; uinque values: 14\n",
      "registration_month : uint8 ; uinque values: 12\n",
      "registration_day : uint8 ; uinque values: 31\n",
      "expiration_year : uint16 ; uinque values: 18\n",
      "expiration_month : uint8 ; uinque values: 12\n",
      "expiration_day : uint8 ; uinque values: 31\n",
      "days : int64 ; uinque values: 4319\n",
      "composer_score : float64 ; uinque values: 2218\n",
      "composer : category ; uinque values: 49088\n",
      "lyricist_score : float64 ; uinque values: 1652\n",
      "lyricist : category ; uinque values: 24206\n",
      "artist_name_score : float64 ; uinque values: 1842\n",
      "artist_name : category ; uinque values: 39236\n",
      "popular_0 : float64 ; uinque values: 2041\n",
      "num_people_0 : float64 ; uinque values: 2003\n",
      "active_0 : float64 ; uinque values: 1814\n",
      "num_song_0 : float64 ; uinque values: 1772\n"
     ]
    }
   ],
   "source": [
    "for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556790 2556790\n"
     ]
    }
   ],
   "source": [
    "print(len(test_id), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapok/pyenv35/lib/python3.5/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.633637\n",
      "[2]\tvalid_0's auc: 0.641298\n",
      "[3]\tvalid_0's auc: 0.642856\n",
      "[4]\tvalid_0's auc: 0.643825\n",
      "[5]\tvalid_0's auc: 0.645342\n",
      "[6]\tvalid_0's auc: 0.646425\n",
      "[7]\tvalid_0's auc: 0.646916\n",
      "[8]\tvalid_0's auc: 0.647135\n",
      "[9]\tvalid_0's auc: 0.64819\n",
      "[10]\tvalid_0's auc: 0.649298\n",
      "[11]\tvalid_0's auc: 0.650209\n",
      "[12]\tvalid_0's auc: 0.651037\n",
      "[13]\tvalid_0's auc: 0.652429\n",
      "[14]\tvalid_0's auc: 0.653274\n",
      "[15]\tvalid_0's auc: 0.653858\n",
      "[16]\tvalid_0's auc: 0.654175\n",
      "[17]\tvalid_0's auc: 0.654961\n",
      "[18]\tvalid_0's auc: 0.655386\n",
      "[19]\tvalid_0's auc: 0.656219\n",
      "[20]\tvalid_0's auc: 0.656772\n",
      "[21]\tvalid_0's auc: 0.657357\n",
      "[22]\tvalid_0's auc: 0.657511\n",
      "[23]\tvalid_0's auc: 0.658149\n",
      "[24]\tvalid_0's auc: 0.658683\n",
      "[25]\tvalid_0's auc: 0.659072\n",
      "[26]\tvalid_0's auc: 0.659686\n",
      "[27]\tvalid_0's auc: 0.660407\n",
      "[28]\tvalid_0's auc: 0.661173\n",
      "[29]\tvalid_0's auc: 0.661536\n",
      "[30]\tvalid_0's auc: 0.662121\n",
      "[31]\tvalid_0's auc: 0.662484\n",
      "[32]\tvalid_0's auc: 0.662864\n",
      "[33]\tvalid_0's auc: 0.663241\n",
      "[34]\tvalid_0's auc: 0.663462\n",
      "[35]\tvalid_0's auc: 0.663644\n",
      "[36]\tvalid_0's auc: 0.664081\n",
      "[37]\tvalid_0's auc: 0.664182\n",
      "[38]\tvalid_0's auc: 0.664307\n",
      "[39]\tvalid_0's auc: 0.664716\n",
      "[40]\tvalid_0's auc: 0.664829\n",
      "[41]\tvalid_0's auc: 0.665002\n",
      "[42]\tvalid_0's auc: 0.66535\n",
      "[43]\tvalid_0's auc: 0.665793\n",
      "[44]\tvalid_0's auc: 0.665956\n",
      "[45]\tvalid_0's auc: 0.666198\n",
      "[46]\tvalid_0's auc: 0.666856\n",
      "[47]\tvalid_0's auc: 0.667153\n",
      "[48]\tvalid_0's auc: 0.667379\n",
      "[49]\tvalid_0's auc: 0.667627\n",
      "[50]\tvalid_0's auc: 0.667762\n",
      "[51]\tvalid_0's auc: 0.667946\n",
      "[52]\tvalid_0's auc: 0.667999\n",
      "[53]\tvalid_0's auc: 0.668209\n",
      "[54]\tvalid_0's auc: 0.668292\n",
      "[55]\tvalid_0's auc: 0.668619\n",
      "[56]\tvalid_0's auc: 0.668933\n",
      "[57]\tvalid_0's auc: 0.669117\n",
      "[58]\tvalid_0's auc: 0.669337\n",
      "[59]\tvalid_0's auc: 0.669508\n",
      "[60]\tvalid_0's auc: 0.669644\n",
      "[61]\tvalid_0's auc: 0.669767\n",
      "[62]\tvalid_0's auc: 0.669993\n",
      "[63]\tvalid_0's auc: 0.67008\n",
      "[64]\tvalid_0's auc: 0.670373\n",
      "[65]\tvalid_0's auc: 0.670467\n",
      "[66]\tvalid_0's auc: 0.670578\n",
      "[67]\tvalid_0's auc: 0.670604\n",
      "[68]\tvalid_0's auc: 0.670753\n",
      "[69]\tvalid_0's auc: 0.670917\n",
      "[70]\tvalid_0's auc: 0.671075\n",
      "[71]\tvalid_0's auc: 0.671145\n",
      "[72]\tvalid_0's auc: 0.671257\n",
      "[73]\tvalid_0's auc: 0.671482\n",
      "[74]\tvalid_0's auc: 0.671715\n",
      "[75]\tvalid_0's auc: 0.671872\n",
      "[76]\tvalid_0's auc: 0.67196\n",
      "[77]\tvalid_0's auc: 0.672082\n",
      "[78]\tvalid_0's auc: 0.672204\n",
      "[79]\tvalid_0's auc: 0.672369\n",
      "[80]\tvalid_0's auc: 0.672548\n",
      "[81]\tvalid_0's auc: 0.672635\n",
      "[82]\tvalid_0's auc: 0.672695\n",
      "[83]\tvalid_0's auc: 0.672677\n",
      "[84]\tvalid_0's auc: 0.672965\n",
      "[85]\tvalid_0's auc: 0.672934\n",
      "[86]\tvalid_0's auc: 0.673258\n",
      "[87]\tvalid_0's auc: 0.673547\n",
      "[88]\tvalid_0's auc: 0.673665\n",
      "[89]\tvalid_0's auc: 0.673767\n",
      "[90]\tvalid_0's auc: 0.673905\n",
      "[91]\tvalid_0's auc: 0.674107\n",
      "[92]\tvalid_0's auc: 0.674269\n",
      "[93]\tvalid_0's auc: 0.674418\n",
      "[94]\tvalid_0's auc: 0.674445\n",
      "[95]\tvalid_0's auc: 0.67457\n",
      "[96]\tvalid_0's auc: 0.674701\n",
      "[97]\tvalid_0's auc: 0.67472\n",
      "[98]\tvalid_0's auc: 0.674876\n",
      "[99]\tvalid_0's auc: 0.67499\n",
      "[100]\tvalid_0's auc: 0.675137\n",
      "[101]\tvalid_0's auc: 0.675304\n",
      "[102]\tvalid_0's auc: 0.675432\n",
      "[103]\tvalid_0's auc: 0.67549\n",
      "[104]\tvalid_0's auc: 0.675654\n",
      "[105]\tvalid_0's auc: 0.675685\n",
      "[106]\tvalid_0's auc: 0.675777\n",
      "[107]\tvalid_0's auc: 0.675786\n",
      "[108]\tvalid_0's auc: 0.67591\n",
      "[109]\tvalid_0's auc: 0.675995\n",
      "[110]\tvalid_0's auc: 0.676063\n",
      "[111]\tvalid_0's auc: 0.676134\n",
      "[112]\tvalid_0's auc: 0.676206\n",
      "[113]\tvalid_0's auc: 0.67629\n",
      "[114]\tvalid_0's auc: 0.676298\n",
      "[115]\tvalid_0's auc: 0.676403\n",
      "[116]\tvalid_0's auc: 0.676572\n",
      "[117]\tvalid_0's auc: 0.676689\n",
      "[118]\tvalid_0's auc: 0.676817\n",
      "[119]\tvalid_0's auc: 0.676908\n",
      "[120]\tvalid_0's auc: 0.67703\n",
      "[121]\tvalid_0's auc: 0.67708\n",
      "[122]\tvalid_0's auc: 0.677099\n",
      "[123]\tvalid_0's auc: 0.677112\n",
      "[124]\tvalid_0's auc: 0.677187\n",
      "[125]\tvalid_0's auc: 0.677237\n",
      "[126]\tvalid_0's auc: 0.677376\n",
      "[127]\tvalid_0's auc: 0.677382\n",
      "[128]\tvalid_0's auc: 0.67741\n",
      "[129]\tvalid_0's auc: 0.677569\n",
      "[130]\tvalid_0's auc: 0.677602\n",
      "[131]\tvalid_0's auc: 0.677586\n",
      "[132]\tvalid_0's auc: 0.677786\n",
      "[133]\tvalid_0's auc: 0.677783\n",
      "[134]\tvalid_0's auc: 0.678009\n",
      "[135]\tvalid_0's auc: 0.678105\n",
      "[136]\tvalid_0's auc: 0.678269\n",
      "[137]\tvalid_0's auc: 0.678394\n",
      "[138]\tvalid_0's auc: 0.67844\n",
      "[139]\tvalid_0's auc: 0.678577\n",
      "[140]\tvalid_0's auc: 0.678679\n",
      "[141]\tvalid_0's auc: 0.678851\n",
      "[142]\tvalid_0's auc: 0.678927\n",
      "[143]\tvalid_0's auc: 0.678959\n",
      "[144]\tvalid_0's auc: 0.678999\n",
      "[145]\tvalid_0's auc: 0.679085\n",
      "[146]\tvalid_0's auc: 0.679157\n",
      "[147]\tvalid_0's auc: 0.67931\n",
      "[148]\tvalid_0's auc: 0.679398\n",
      "[149]\tvalid_0's auc: 0.679476\n",
      "[150]\tvalid_0's auc: 0.679473\n",
      "[151]\tvalid_0's auc: 0.679509\n",
      "[152]\tvalid_0's auc: 0.679527\n",
      "[153]\tvalid_0's auc: 0.67963\n",
      "[154]\tvalid_0's auc: 0.6798\n",
      "[155]\tvalid_0's auc: 0.679873\n",
      "[156]\tvalid_0's auc: 0.679986\n",
      "[157]\tvalid_0's auc: 0.680122\n",
      "[158]\tvalid_0's auc: 0.68015\n",
      "[159]\tvalid_0's auc: 0.680192\n",
      "[160]\tvalid_0's auc: 0.680219\n",
      "[161]\tvalid_0's auc: 0.680266\n",
      "[162]\tvalid_0's auc: 0.680385\n",
      "[163]\tvalid_0's auc: 0.680411\n",
      "[164]\tvalid_0's auc: 0.680525\n",
      "[165]\tvalid_0's auc: 0.680552\n",
      "[166]\tvalid_0's auc: 0.680619\n",
      "[167]\tvalid_0's auc: 0.68073\n",
      "[168]\tvalid_0's auc: 0.6808\n",
      "[169]\tvalid_0's auc: 0.68079\n",
      "[170]\tvalid_0's auc: 0.680848\n",
      "[171]\tvalid_0's auc: 0.680969\n",
      "[172]\tvalid_0's auc: 0.680992\n",
      "[173]\tvalid_0's auc: 0.680974\n",
      "[174]\tvalid_0's auc: 0.681119\n",
      "[175]\tvalid_0's auc: 0.681203\n",
      "[176]\tvalid_0's auc: 0.681292\n",
      "[177]\tvalid_0's auc: 0.681355\n",
      "[178]\tvalid_0's auc: 0.681328\n",
      "[179]\tvalid_0's auc: 0.681416\n",
      "[180]\tvalid_0's auc: 0.681456\n",
      "[181]\tvalid_0's auc: 0.681511\n",
      "[182]\tvalid_0's auc: 0.681522\n",
      "[183]\tvalid_0's auc: 0.681542\n",
      "[184]\tvalid_0's auc: 0.681646\n",
      "[185]\tvalid_0's auc: 0.681769\n",
      "[186]\tvalid_0's auc: 0.681847\n",
      "[187]\tvalid_0's auc: 0.681995\n",
      "[188]\tvalid_0's auc: 0.682148\n",
      "[189]\tvalid_0's auc: 0.682239\n",
      "[190]\tvalid_0's auc: 0.682188\n",
      "[191]\tvalid_0's auc: 0.682236\n",
      "[192]\tvalid_0's auc: 0.682243\n",
      "[193]\tvalid_0's auc: 0.682352\n",
      "[194]\tvalid_0's auc: 0.682416\n",
      "[195]\tvalid_0's auc: 0.682407\n",
      "[196]\tvalid_0's auc: 0.682445\n",
      "[197]\tvalid_0's auc: 0.682486\n",
      "[198]\tvalid_0's auc: 0.682572\n",
      "[199]\tvalid_0's auc: 0.682561\n",
      "[200]\tvalid_0's auc: 0.682615\n",
      "cur fold finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "train_data = lgb.Dataset(train_use.drop(['target'],axis=1),label=train_use['target'])\n",
    "val_data = lgb.Dataset(validation_use.drop(['target'],axis=1),label=validation_use['target'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1 ,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 108,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 128,\n",
    "    'max_depth': 10,\n",
    "    'num_rounds': 200,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "predictions+=bst.predict(test.drop(['id'],axis=1))\n",
    "print('cur fold finished.')\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_tune_with_val(params, tune_param, param_list, data_list, val_data, less_prefered = False):\n",
    "    #data_list = {'train':{'x':train_d,'y':train_y}, 'validation':{'x':valid_d,'y':valid_y}}\n",
    "    best_metric = (less_prefered and sys.float_info.max or -sys.float_info.max)\n",
    "    best_param = param_list[0]\n",
    "\n",
    "    for par_value in param_list:\n",
    "        params[tune_param] = par_value\n",
    "        # , num_boost_round=params['num_boost_round'], early_stopping_rounds = params['early_stopping_rounds']\n",
    "        model = lgb.train(params, data_list['train']['x'], valid_sets=[data_list['validation']['x']], \\\n",
    "                feature_name='auto', #categorical_feature=['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender',\\\n",
    "                                     #                       'bd', 'name', 'artist_name', 'composer', 'lyricist', 'msno', 'song_id', 'genre_ids',\\\n",
    "                                     #                       'country', 'language', 'registered_via'],、\n",
    "                        )\n",
    "       \n",
    "        val_predprob = model.predict(val_data)\n",
    "        auroc_score = metrics.roc_auc_score(data_list['validation']['y'], val_predprob)\n",
    "\n",
    "        if (not less_prefered and auroc_score > best_metric) or (less_prefered and auroc_score < best_metric):\n",
    "            best_metric = auroc_score\n",
    "            best_param = par_value\n",
    "    log.info('best param for {}: {}, metric: {}'.format(tune_param, best_param, best_metric))\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#{'top_k': 20, 'feature_fraction': 0.8, 'bagging_freq': 1, 'min_data_in_bin': 3, 'min_sum_hessian_in_leaf': 0.001, 'bagging_fraction': 0.9, 'max_depth': 12, 'num_leaves': 100, 'learning_rate': 0.01, 'objective': 'binary', 'lambda_l2': 0.01, 'feature_fraction_seed': 1024, 'min_data_in_leaf': 15, 'max_bin': 100, 'verbose': 0, 'bagging_seed': 6666, 'max_cat_to_onehot': 4, 'metric': 'auc', 'lambda_l1': 1e-05, 'num_threads': 16, 'boosting': 'gbdt', 'min_split_gain': 0.3}\n",
    "\n",
    "#{'bagging_seed': 6666, 'lambda_l1': 1e-05, 'lambda_l2': 0.01, 'metric': 'auc', 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 0.001, 'feature_fraction': 0.8, 'feature_fraction_seed': 1024, 'num_leaves': 90, 'boosting': 'gbdt', 'verbose': 0, 'min_data_in_leaf': 15, 'top_k': 20, 'objective': 'binary', 'min_data_in_bin': 3, 'num_threads': 16, 'max_cat_to_onehot': 4, 'max_depth': 10, 'bagging_fraction': 0.9, 'learning_rate': 0.01, 'max_bin': 80, 'min_split_gain': 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_for_best_params(train, validation, test):\n",
    "    \n",
    "    X_train = lgb.Dataset(np.array(train.drop(['target'], axis=1)), label=train['target'].values)\n",
    "    X_valid = lgb.Dataset(np.array(validation.drop(['target'], axis=1)), label=validation['target'].values)\n",
    "    \n",
    "    y_train = train['target'].values\n",
    "    y_valid = validation['target'].values\n",
    "\n",
    "    X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "    data_list = {'train':{'x':X_train,'y':y_train}, 'validation':{'x':X_valid,'y':y_valid}}\n",
    "######## for value rather than catogory ################\n",
    "#   params_to_eval = OrderedDict(\n",
    "#         ( \n",
    "#         ('num_boost_round', range(120,150,10)),\n",
    "#         ('num_leaves', range(80,100,10)), # number of leaves in one tree\n",
    "#         ('max_depth', range(8,12,1)),\n",
    "#         ('min_data_in_leaf', 15),\n",
    "#         ('min_sum_hessian_in_leaf', [0.001]),# too high will lead to under-fitting\n",
    "#         ('min_split_gain',[0.3]),# the minimum loss reduction required to make a split\n",
    "#         ('bagging_fraction',[0.9]),# [i/10.0 for i in range(6,10)]\n",
    "#         ('feature_fraction',[0.8]),# typical: 0.5-1\n",
    "#         ('max_bin', range(70,90,10)),\n",
    "#         ('lambda_l2',[0.01]),\n",
    "#         ('lambda_l1',[1e-5]),\n",
    "#         ('learning_rate',[0.01]), # typical: 0.01-0.2\n",
    "#         )\n",
    "#       )\n",
    "     \n",
    "#     initial_params = {\n",
    "#         'objective': 'binary',\n",
    "#         'boosting': 'gbdt',\n",
    "#         'num_boost_round': 140,\n",
    "#         'learning_rate': 0.01 ,\n",
    "#         'verbose': 0,\n",
    "#         'num_leaves': 90,\n",
    "#         'num_threads':16,\n",
    "#         'max_depth': 9,\n",
    "#         'min_data_in_leaf': 15, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "#         'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "#         'feature_fraction': 0.8, #colsample_bytree\n",
    "#         'feature_fraction_seed': 1024,\n",
    "#         'bagging_fraction': 0.9, #subsample\n",
    "#         'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "#         'bagging_seed': 6666,\n",
    "#         'early_stopping_rounds':10,   \n",
    "#         'lambda_l1': 1e-5, #L1 regularization\n",
    "#         'lambda_l2': 0.01, #L2 regularization\n",
    "#         'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "#         'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "#         'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "#         'max_bin': 70, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "#         'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "#         'metric' : 'auc',\n",
    "#     } \n",
    "    params_to_eval = OrderedDict(\n",
    "        ( \n",
    "        ('num_boost_round', range(100,400,50)),\n",
    "        ('num_leaves', range(80,160,10)), # number of leaves in one tree\n",
    "        ('max_depth', range(8,18,1)),\n",
    "        ('min_data_in_leaf', range(10,18,2)),\n",
    "        ('min_sum_hessian_in_leaf', [0.001]),# too high will lead to under-fitting\n",
    "        ('min_split_gain',[0.3]),# the minimum loss reduction required to make a split\n",
    "        ('bagging_fraction',[0.9]),# [i/10.0 for i in range(6,10)]\n",
    "        ('feature_fraction',[0.8]),# typical: 0.5-1\n",
    "        ('max_bin', range(80,200,10)),\n",
    "        ('lambda_l2',[0.01]),\n",
    "        ('lambda_l1',[1e-5]),\n",
    "        ('learning_rate',[0.01]), # typical: 0.01-0.2\n",
    "        )\n",
    "      )\n",
    "     \n",
    "    initial_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_boost_round': 200,\n",
    "        'learning_rate': 0.1 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 120,\n",
    "        'num_threads':16,\n",
    "        'max_depth': 14,\n",
    "        'min_data_in_leaf': 16, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.8, #colsample_bytree\n",
    "        'feature_fraction_seed': 1024,\n",
    "        'bagging_fraction': 0.9, #subsample\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "        'bagging_seed': 6666,\n",
    "        'early_stopping_rounds':10,   \n",
    "        'lambda_l1': 1e-5, #L1 regularization\n",
    "        'lambda_l2': 0.01, #L2 regularization\n",
    "        'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "        'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "        'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "        'max_bin': 140, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "        'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "        'metric' : 'auc',\n",
    "    } \n",
    "    # only param nin this list are tuned, total list are ['n_estimators', 'reg_alpha', 'reg_lambda', 'subsample', 'colsample_bytree', 'min_child_weight', 'max_depth', 'learning_rate', 'gamma']\n",
    "    #tuned_param_name = ['num_boost_round', 'num_leaves', 'max_depth', 'max_bin']\n",
    "    tuned_param_name = ['num_boost_round', 'num_leaves', 'max_depth', 'min_data_in_leaf', 'min_sum_hessian_in_leaf',\\\n",
    "                        'min_split_gain', 'bagging_fraction', 'feature_fraction', 'max_bin', 'lambda_l2', 'lambda_l1', 'learning_rate']\n",
    "    for par_name, par_list in params_to_eval.items():\n",
    "        if par_name in tuned_param_name:\n",
    "            log.info('tunning {}...'.format(par_name))\n",
    "            if len(par_list) > 1:\n",
    "                initial_params[par_name] = param_tune_with_val(initial_params, par_name, par_list, data_list, np.array(validation.drop(['target'], axis=1)))\n",
    "            else:\n",
    "                initial_params[par_name] = par_list[0]\n",
    "    \n",
    "    return initial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_param = search_for_best_params(train_use, validation_use, test)\n",
    "log.info(best_param)\n",
    "time_elapsed = time.time() - start_time\n",
    "log.info('time used: {:.3f}sec'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_boost_round': 140,\n",
    "        'learning_rate': 0.01 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 90,\n",
    "        'num_threads':16,\n",
    "        'max_depth': 9,\n",
    "        'min_data_in_leaf': 15, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.8, #colsample_bytree\n",
    "        'feature_fraction_seed': 1024,\n",
    "        'bagging_fraction': 0.9, #subsample\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "        'bagging_seed': 6666,\n",
    "        'early_stopping_rounds':10,   \n",
    "        'lambda_l1': 1e-5, #L1 regularization\n",
    "        'lambda_l2': 0.01, #L2 regularization\n",
    "        'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "        'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "        'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "        'max_bin': 70, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "        'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "        'metric' : 'auc',\n",
    "    } \n",
    "X_train = lgb.Dataset(np.array(train_use.drop(['target'], axis=1)), label=train_use['target'].values)\n",
    "X_valid = lgb.Dataset(np.array(validation_use.drop(['target'], axis=1)), label=validation_use['target'].values)\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "model = lgb.train(params, X_train, valid_sets=[X_valid])\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': pred})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_use.drop(['target'], axis=1))\n",
    "y_train = train_use['target'].values\n",
    "\n",
    "X_valid = np.array(validation_use.drop(['target'], axis=1))\n",
    "y_valid = validation_use['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "# d_train = xgb.DMatrix(X_train)\n",
    "# d_valid = xgb.DMatrix(X_valid) \n",
    "# d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "data_list = {'train':{'x':X_train,'y':y_train}, 'validation':{'x':X_valid,'y':y_valid}}\n",
    "# Train model, evaluate and make predictions\n",
    "params={\n",
    "    'n_estimators':500,\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.75,\n",
    "    'gamma':0.1,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.3,\n",
    "    'min_child_weight':3,\n",
    "    'max_depth':16,\n",
    "    'seed':1024,\n",
    "    }\n",
    "\n",
    "param_tune_with_val(params, 'max_depth', [5,1,6], data_list, 'auc', 20)\n",
    "\n",
    "# model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=20, \\\n",
    "#     maximize=True, verbose_eval=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_use.drop(['target'], axis=1))\n",
    "y_train = train_use['target'].values\n",
    "\n",
    "X_valid = np.array(validation_use.drop(['target'], axis=1))\n",
    "y_valid = validation_use['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid) \n",
    "d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "# Train model, evaluate and make predictions\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.75\n",
    "params['max_depth'] = 16\n",
    "params['silent'] = 1\n",
    "params['eval_metric'] = 'auc'\n",
    "\n",
    "model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=20, \\\n",
    "    maximize=True, verbose_eval=5)\n",
    "\n",
    "#Predict training set:\n",
    "train_predictions = model.predict(X_train)\n",
    "train_predprob = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "val_predictions = model.predict(X_valid)\n",
    "val_predprob = model.predict_proba(X_valid)[:,1]\n",
    "\n",
    "#Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"Train Accuracy : %.4g\" % metrics.accuracy_score(y_train, train_predictions))\n",
    "print(\"Train AUC Score (Train): %f\" % metrics.roc_auc_score(y_train, train_predprob))\n",
    "print(\"ValAccuracy : %.4g\" % metrics.accuracy_score(y_valid, val_predictions))\n",
    "print(\"Validation AUC Score (Train): %f\" % metrics.roc_auc_score(y_valid, val_predprob))\n",
    "\n",
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "\n",
    "p_test = model.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "modelfit(xgb1, train_use.drop(['target'],axis=1), train_use['target'], validation_use.drop(['target'],axis=1), validation_use['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'], eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, train, label, validation, val_label, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train.values, label=label.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train, label, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train)\n",
    "    train_predprob = alg.predict_proba(train)[:,1]\n",
    "    \n",
    "    val_predictions = alg.predict(validation)\n",
    "    val_predprob = alg.predict_proba(validation)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Train Accuracy : %.4g\" % metrics.accuracy_score(label.values, train_predictions))\n",
    "    print(\"Train AUC Score (Train): %f\" % metrics.roc_auc_score(label, train_predprob))\n",
    "    print(\"ValAccuracy : %.4g\" % metrics.accuracy_score(val_label.values, val_predictions))\n",
    "    print(\"Validation AUC Score (Train): %f\" % metrics.roc_auc_score(val_label, val_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "modelfit(xgb1, train_use.drop(['target'],axis=1), train_use['target'], validation_use.drop(['target'],axis=1), validation_use['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(train_use.drop(['target'],axis=1), label=train_use['target'])\n",
    "val_data = lgb.Dataset(validation_use.drop(['target'],axis=1), label=validation_use['target'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1 ,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 108,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 128,\n",
    "    'max_depth': 10,\n",
    "    'num_rounds': 200,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "predictions=bst.predict(test.drop(['id'],axis=1))\n",
    "print('finished.')\n",
    "\n",
    "    \n",
    "predictions = predictions/3\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "for train_indices,val_indices in kf.split(train) : \n",
    "    train_data = lgb.Dataset(train.drop(['target'],axis=1).loc[train_indices,:],label=train.loc[train_indices,'target'])\n",
    "    val_data = lgb.Dataset(train.drop(['target'],axis=1).loc[val_indices,:],label=train.loc[val_indices,'target'])\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.1 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 108,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 128,\n",
    "        'max_depth': 10,\n",
    "        'num_rounds': 200,\n",
    "        'metric' : 'auc',\n",
    "        } \n",
    "    \n",
    "    bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "    predictions+=bst.predict(test.drop(['id'],axis=1))\n",
    "    print('cur fold finished.')\n",
    "    del bst\n",
    "    \n",
    "predictions = predictions/3\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess songs data\n",
    "songs_genres = np.array(songs['genre_ids']\\\n",
    "    .apply(lambda x: [int(v) for v in str(x).split('|')]))\n",
    "genres_list = songs_genres.ravel().unique()\n",
    "print('Number of genres: ' + str(len(genres_list)))\n",
    "\n",
    "ohe_genres = np.zeros((len(songs_genres), len(genres_list)))\n",
    "for s_i, s_genres in enumerate(songs_genres):\n",
    "    for genre in s_genres:\n",
    "        g_i = genres_list.find(genre)\n",
    "        ohe_genres[s_i, g_i] = 1\n",
    "        \n",
    "for g_i, g in enumerate(genres_list):\n",
    "    songs['genre_' + str(g)] = ohe_genres[:, g_i]\n",
    "print(songs.head())\n",
    "songs = songs.drop(['genre_ids'], axis=1)\n",
    "\n",
    "song_cols = songs.columns\n",
    "\n",
    "# Preprocess dataset\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "cols = list(train.columns)\n",
    "cols.remove('target')\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = train[col].apply(str)\n",
    "        test[col] = test[col].apply(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        train_vals = list(train[col].unique())\n",
    "        test_vals = list(test[col].unique())\n",
    "        le.fit(train_vals + test_vals)\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "\n",
    "        print(col + ': ' + str(len(train_vals)) + ', ' + str(len(test_vals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "########################################\n",
    "## load the data\n",
    "########################################\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "uid = train.msno\n",
    "sid = train.song_id\n",
    "target = train.target\n",
    "\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "id_test = test.id\n",
    "uid_test = test.msno\n",
    "sid_test = test.song_id\n",
    "\n",
    "########################################\n",
    "## encoding\n",
    "########################################\n",
    "\n",
    "usr_encoder = LabelEncoder()\n",
    "usr_encoder.fit(uid.append(uid_test))\n",
    "uid = usr_encoder.transform(uid)\n",
    "uid_test = usr_encoder.transform(uid_test)\n",
    "\n",
    "sid_encoder = LabelEncoder()\n",
    "sid_encoder.fit(sid.append(sid_test))\n",
    "sid = sid_encoder.transform(sid)\n",
    "sid_test = sid_encoder.transform(sid_test)\n",
    "\n",
    "u_cnt = int(max(uid.max(), uid_test.max()) + 1)\n",
    "s_cnt = int(max(sid.max(), sid_test.max()) + 1)\n",
    "\n",
    "########################################\n",
    "## train-validation split\n",
    "########################################\n",
    "\n",
    "perm = np.random.permutation(len(train))\n",
    "trn_cnt = int(len(train) * 0.85)\n",
    "uid_trn = uid[perm[:trn_cnt]]\n",
    "uid_val = uid[perm[trn_cnt:]]\n",
    "sid_trn = sid[perm[:trn_cnt]]\n",
    "sid_val = sid[perm[trn_cnt:]]\n",
    "target_trn = target[perm[:trn_cnt]]\n",
    "target_val = target[perm[trn_cnt:]]\n",
    "\n",
    "########################################\n",
    "## define the model\n",
    "########################################\n",
    "\n",
    "def get_model():\n",
    "    user_embeddings = Embedding(u_cnt,\n",
    "            64,\n",
    "            embeddings_initializer=RandomUniform(minval=-0.1, maxval=0.1),\n",
    "            embeddings_regularizer=l2(1e-4),\n",
    "            input_length=1,\n",
    "            trainable=True)\n",
    "    song_embeddings = Embedding(s_cnt,\n",
    "            64,\n",
    "            embeddings_initializer=RandomUniform(minval=-0.1, maxval=0.1),\n",
    "            embeddings_regularizer=l2(1e-4),\n",
    "            input_length=1,\n",
    "            trainable=True)\n",
    "\n",
    "    uid_input = Input(shape=(1,), dtype='int32')\n",
    "    embedded_usr = user_embeddings(uid_input)\n",
    "    embedded_usr = Reshape((64,))(embedded_usr)\n",
    "\n",
    "    sid_input = Input(shape=(1,), dtype='int32')\n",
    "    embedded_song = song_embeddings(sid_input)\n",
    "    embedded_song = Reshape((64,))(embedded_song)\n",
    "\n",
    "    preds = dot([embedded_usr, embedded_song], axes=1)\n",
    "    preds = concatenate([embedded_usr, embedded_song, preds])\n",
    "    \n",
    "    preds = Dense(128, activation='relu')(preds)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    \n",
    "    preds = Dense(1, activation='sigmoid')(preds)\n",
    "\n",
    "    model = Model(inputs=[uid_input, sid_input], outputs=preds)\n",
    "    \n",
    "    opt = RMSprop(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "   \n",
    "model = get_model()\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=5)\n",
    "model_path = 'bst_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, \\\n",
    "        save_weights_only=True)\n",
    "\n",
    "hist = model.fit([uid_trn, sid_trn], target_trn, validation_data=([uid_val, sid_val], \\\n",
    "        target_val), epochs=100, batch_size=32768, shuffle=True, \\\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "model.load_weights(model_path)\n",
    "\n",
    "preds_val = model.predict([uid_val, sid_val], batch_size=32768)\n",
    "val_auc = roc_auc_score(target_val, preds_val)\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "\n",
    "preds_test = model.predict([uid_test, sid_test], batch_size=32768, verbose=1)\n",
    "sub = pd.DataFrame({'id': id_test, 'target': preds_test.ravel()})\n",
    "sub.to_csv('./sub_%.5f.csv'%(val_auc), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear algebra:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Graphics:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "# Frameworks:\n",
    "import lightgbm as lgb # LightGBM\n",
    "# Utils:\n",
    "import gc # garbage collector\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IDIR = '../input/' # main path\n",
    "members = pd.read_csv(IDIR + 'members.csv')\n",
    "songs = pd.read_csv(IDIR + 'songs.csv')\n",
    "song_extra_info = pd.read_csv(IDIR + 'song_extra_info.csv')\n",
    "train = pd.read_csv(IDIR + 'train.csv')\n",
    "test = pd.read_csv(IDIR + 'test.csv')\n",
    "\n",
    "# Adding songs' info:\n",
    "train_aug1 = pd.merge(left=train, right=songs, on='song_id', how='left')\n",
    "test_aug1 = pd.merge(left=test, right=songs, on='song_id', how='left')\n",
    "# Adding extra info about songs:\n",
    "train_aug2 = pd.merge(left=train_aug1, right=song_extra_info, on='song_id', how='left')\n",
    "test_aug2 = pd.merge(left=test_aug1, right=song_extra_info, on='song_id', how='left')\n",
    "del train_aug1, test_aug1\n",
    "# Addind users' info:\n",
    "train_aug3 = pd.merge(left=train_aug2, right=members, on='msno', how='left')\n",
    "test_aug3 = pd.merge(left=test_aug2, right=members, on='msno', how='left')\n",
    "del train_aug2, test_aug2\n",
    "# Merging train and test data:\n",
    "train_aug3.drop(['song_id'], axis=1, inplace=True)\n",
    "train_aug3['set'] = 0\n",
    "test_aug3.drop(['song_id'], axis=1, inplace=True)\n",
    "test_aug3['set'] = 1\n",
    "test_aug3['target'] = -1\n",
    "all_aug = pd.concat([train_aug3, test_aug3], axis=0)\n",
    "del train_aug3, test_aug3\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "\n",
    "# source_system_tab/source_screen_name/source_type/genre_ids/artist_name/composer/lyricist/name/isrc/gender 用'NA'填补并one-hot编码\n",
    "# genre_ids encoding:\n",
    "all_aug['genre_ids'] = all_aug.genre_ids.fillna('NA')\n",
    "all_aug['genre_ids'] = all_aug.genre_ids.astype(np.str)\n",
    "genre_ids_le = LabelEncoder()\n",
    "genre_ids_le.fit(all_aug.genre_ids)\n",
    "all_aug['genre_ids'] = genre_ids_le.transform(all_aug.genre_ids).astype(np.int16)\n",
    "\n",
    "# language encoding:\n",
    "all_aug['language'] = all_aug.language.fillna(-2)\n",
    "all_aug['language'] = all_aug.language.astype(np.int8)\n",
    "\n",
    "# city encoding:\n",
    "all_aug['city'] = all_aug.city.astype(np.int8)\n",
    "# bd encoding:\n",
    "all_aug['bd'] = all_aug.bd.astype(np.int16)\n",
    "\n",
    "# registered_via encoding:\n",
    "all_aug['registered_via'] = all_aug.registered_via.astype(np.int8)\n",
    "# registration_init_time encoding:\n",
    "all_aug['registration_init_time'] = all_aug.registration_init_time.astype(np.int32)\n",
    "# expiration_date encoding:\n",
    "all_aug['expiration_date'] = all_aug.expiration_date.astype(np.int32)\n",
    "# Info:\n",
    "all_aug.info(max_cols=0)\n",
    "all_aug.head(2)\n",
    "\n",
    "\n",
    "all_aug['exp_reg_time'] = all_aug.expiration_date - all_aug.registration_init_time\n",
    "\n",
    "\n",
    "\n",
    "gc.collect();\n",
    "d_train = lgb.Dataset(all_aug[all_aug.set == 0].drop(['target', 'msno', 'id', 'set'], axis=1), \n",
    "                      label=all_aug[all_aug.set == 0].pop('target'))\n",
    "ids_train = all_aug[all_aug.set == 0].pop('msno')\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 1.0,\n",
    "    'max_depth': 15,\n",
    "    'num_leaves': 250, \n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc'},\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 5,\n",
    "    'max_bin': 100}\n",
    "cv_result_lgb = lgb.cv(lgb_params, \n",
    "                       d_train, \n",
    "                       num_boost_round=5000, \n",
    "                       nfold=3, \n",
    "                       stratified=True, \n",
    "                       early_stopping_rounds=50, \n",
    "                       verbose_eval=100, \n",
    "                       show_stdv=True)\n",
    "\n",
    "num_boost_rounds_lgb = len(cv_result_lgb['auc-mean'])\n",
    "print('num_boost_rounds_lgb=' + str(num_boost_rounds_lgb))\n",
    "\n",
    "\n",
    "\n",
    "%%time\n",
    "ROUNDS = num_boost_rounds_lgb\n",
    "print('light GBM train :-)')\n",
    "bst = lgb.train(lgb_params, d_train, ROUNDS)\n",
    "# lgb.plot_importance(bst, figsize=(9,20))\n",
    "# del d_train\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "feature_imp = pd.Series(dict(zip(d_train.feature_name, \n",
    "                                 bst.feature_importance()))).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_imp.values, y=feature_imp.index.values, orient='h', color='g')\n",
    "plt.subplot(1,2,2)\n",
    "train_scores = np.array(cv_result_lgb['auc-mean'])\n",
    "train_stds = np.array(cv_result_lgb['auc-stdv'])\n",
    "plt.plot(train_scores, color='green')\n",
    "plt.fill_between(range(len(cv_result_lgb['auc-mean'])), \n",
    "                 train_scores - train_stds, train_scores + train_stds, \n",
    "                 alpha=0.1, color='green')\n",
    "plt.title('LightGMB CV-results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
