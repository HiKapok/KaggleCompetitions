{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapok/pyenv35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/kapok/pyenv35/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "get_ipython().magic('env CUDA_VISIBLE_DEVICES =  ')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp      # will come in handy due to the size of the data\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import io\n",
    "from datetime import datetime\n",
    "import gc # garbage collector\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import logging\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a pandas dataframe to disk as gunzip compressed csv\n",
    "- df.to_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Read from disk\n",
    "- df = pd.read_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Magic useful\n",
    "- %%timeit for the whole cell\n",
    "- %timeit for the specific line\n",
    "- %%latex to render the cell as a block of latex\n",
    "- %prun and %%prun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/WSDM/'\n",
    "HDF_FILENAME = DATASET_PATH + 'datas.h5'\n",
    "SUBMISSION_FILENAME = DATASET_PATH + 'submission_{}.csv'\n",
    "VALIDATION_INDICE = DATASET_PATH + 'validation_indice.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_logging(logger_name, logger_file_name):\n",
    "    log = logging.getLogger(logger_name)\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create formatter and add it to the handlers\n",
    "    print_formatter = logging.Formatter('%(message)s')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s_%(levelname)s: %(message)s')\n",
    "\n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(logger_file_name, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(file_formatter)\n",
    "    log.addHandler(fh)\n",
    "    # both output to console and file\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(print_formatter)\n",
    "    log.addHandler(consoleHandler)\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "here is an info message.\n"
     ]
    }
   ],
   "source": [
    "log = set_logging('MUSIC', DATASET_PATH + 'music_gbm.log')\n",
    "log.info('here is an info message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN_FILE = DATASET_PATH + 'train.csv'\n",
    "# TEST_FILE = DATASET_PATH + 'test.csv'\n",
    "# MEMBER_FILE = DATASET_PATH + 'members.csv'\n",
    "# SONG_FILE = DATASET_PATH + 'fix_songs.csv'\n",
    "# SONG_EXTRA_FILE = DATASET_PATH + 'song_extra_info.csv'\n",
    "\n",
    "# train_data = pd.read_csv(TRAIN_FILE)\n",
    "# test_data = pd.read_csv(TEST_FILE)\n",
    "# member_data = pd.read_csv(MEMBER_FILE)\n",
    "# song_data = pd.read_csv(SONG_FILE)\n",
    "# song_extra_data = pd.read_csv(SONG_EXTRA_FILE)\n",
    "\n",
    "# songs_all = pd.merge(left = song_data, right = song_extra_data, how = 'left', on='song_id')\n",
    "# train_with_mem = pd.merge(left = train_data, right = member_data, how = 'left', on='msno')\n",
    "# train_all = pd.merge(left = train_with_mem, right = songs_all, how = 'left', on='song_id')\n",
    "# test_with_mem = pd.merge(left = test_data, right = member_data, how = 'left', on='msno')\n",
    "# test_all = pd.merge(left = test_with_mem, right = songs_all, how = 'left', on='song_id')\n",
    "# del train_with_mem, test_with_mem; gc.collect()\n",
    "\n",
    "# def convert_unicode_to_str(df):\n",
    "#     df.columns = df.columns.astype(str)\n",
    "#     types = df.apply(lambda x: pd.api.types.infer_dtype(df.values))\n",
    "#     #print(types)#mixed-integer\n",
    "#     for col in types[types == 'mixed-integer'].index:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     for col in types[types == 'mixed'].index:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     return df\n",
    "\n",
    "# store = pd.HDFStore(HDF_FILENAME)\n",
    "# store['train_data'] = convert_unicode_to_str(train_all)\n",
    "# store['test_data'] = convert_unicode_to_str(test_all)\n",
    "# store['song_data'] = convert_unicode_to_str(songs_all)\n",
    "# store['test_id'] = test_data.id\n",
    "# store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store_test = pd.HDFStore(HDF_FILENAME)\n",
    "# train = store_test['train_data'][0:100]\n",
    "# test = store_test['test_data'][0:100]\n",
    "# test_id =  store_test['test_id'][0:100]\n",
    "# store_test.close()\n",
    "store_test = pd.HDFStore(HDF_FILENAME)\n",
    "train = store_test['train_data']\n",
    "test = store_test['test_data']\n",
    "test_id =  store_test['test_id']\n",
    "store_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_country(input_data):\n",
    "    def get_country(isrc):\n",
    "        if isinstance(isrc, str) and isrc != 'nan':\n",
    "            return isrc[0:2]\n",
    "        else:\n",
    "            return np.nan\n",
    "    countries = train['isrc'].apply(get_country)\n",
    "    country_list = list(countries.value_counts().index)\n",
    "    country_map = dict(zip(country_list, country_list))\n",
    "    country_map['QM'] = 'QZ'\n",
    "    country_map['US'] = 'QZ'\n",
    "    return countries.map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['country'] = split_country(train)\n",
    "test['country'] = split_country(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if isinstance(isrc, str) and isrc != 'nan':\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "train['song_year'] = train['isrc'].apply(isrc_to_year)\n",
    "test['song_year'] = test['isrc'].apply(isrc_to_year)\n",
    "train.drop(['isrc'], axis = 1, inplace = True)\n",
    "test.drop(['isrc'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_reg_date(input_data):\n",
    "    input_data['registration_year'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[0:4]))\n",
    "    input_data['registration_year'] = pd.to_numeric(input_data['registration_year'], downcast='unsigned')\n",
    "\n",
    "    input_data['registration_month'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[4:6]))\n",
    "    input_data['registration_month'] = pd.to_numeric(input_data['registration_month'], downcast='unsigned')\n",
    "\n",
    "    input_data['registration_day'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[6:8]))\n",
    "    input_data['registration_day'] = pd.to_numeric(input_data['registration_day'], downcast='unsigned')\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_expir_date(input_data):\n",
    "    input_data['expiration_year'] = input_data['expiration_date'].apply(lambda x : int(str(x)[0:4]))\n",
    "    input_data['expiration_year'] = pd.to_numeric(input_data['expiration_year'], downcast='unsigned')\n",
    "\n",
    "    input_data['expiration_month'] = input_data['expiration_date'].apply(lambda x : int(str(x)[4:6]))\n",
    "    input_data['expiration_month'] = pd.to_numeric(input_data['expiration_month'], downcast='unsigned')\n",
    "\n",
    "    input_data['expiration_day'] = input_data['expiration_date'].apply(lambda x : int(str(x)[6:8]))\n",
    "    input_data['expiration_day'] = pd.to_numeric(input_data['expiration_day'], downcast='unsigned')\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_to_day(input_data):\n",
    "    # 转换注册时间\n",
    "    input_data['registration_init_time'] = pd.to_datetime(input_data['registration_init_time'],format=\"%Y%m%d\")\n",
    "    input_data['expiration_date'] = pd.to_datetime(input_data['expiration_date'],format=\"%Y%m%d\")\n",
    "    days = input_data.expiration_date - input_data.registration_init_time\n",
    "    days = [d.days for d in days]\n",
    "    input_data['days']=days\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = split_reg_date(train)\n",
    "test = split_reg_date(test)\n",
    "train = split_expir_date(train)\n",
    "test = split_expir_date(test)\n",
    "\n",
    "train = date_to_day(train)\n",
    "test = date_to_day(test)\n",
    "\n",
    "train.drop('registration_init_time',axis=1,inplace=True)\n",
    "train.drop('expiration_date',axis=1,inplace=True)\n",
    "test.drop('registration_init_time',axis=1,inplace=True)\n",
    "test.drop('expiration_date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['song_length'] = pd.to_numeric(train['song_length'].replace('nan', '235415.0'), downcast='unsigned')\n",
    "test['song_length'] = pd.to_numeric(test['song_length'].replace('nan', '235415.0'), downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno : object\n",
      "song_id : object\n",
      "source_system_tab : object\n",
      "source_screen_name : object\n",
      "source_type : object\n",
      "target : object\n",
      "city : object\n",
      "bd : object\n",
      "gender : object\n",
      "registered_via : object\n",
      "song_length : uint32\n",
      "genre_ids : object\n",
      "artist_name : object\n",
      "composer : object\n",
      "lyricist : object\n",
      "language : object\n",
      "name : object\n",
      "country : object\n",
      "song_year : float64\n",
      "registration_year : uint16\n",
      "registration_month : uint8\n",
      "registration_day : uint8\n",
      "expiration_year : uint16\n",
      "expiration_month : uint8\n",
      "expiration_day : uint8\n",
      "days : int64\n"
     ]
    }
   ],
   "source": [
    "for col in train.columns: print(col, ':', train[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in [col for col in test.columns if col != 'id' ]:\n",
    "    if train[col].dtype == object:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode registered_via, the less number of occurrences are merged into the top item which has the max number of occurrences\n",
    "registered_via_hist = pd.concat([train['registered_via'], test['registered_via']], axis = 0).value_counts()\n",
    "registered_via_map = dict(zip(registered_via_hist.index, [int(s) for s in registered_via_hist.index.values]))\n",
    "registered_via_map[registered_via_hist.index[-1]] = int(str(registered_via_hist.index.values[0]))\n",
    "train['registered_via'] = train['registered_via'].map(registered_via_map)\n",
    "test['registered_via'] = test['registered_via'].map(registered_via_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode language, fill nan with most occurrences item\n",
    "language_hist = pd.concat([train['language'], test['language']], axis = 0).value_counts()\n",
    "language_map = dict(zip(language_hist.index, [int(float(s)) for s in language_hist.index.values if s != 'nan']))\n",
    "language_map['nan'] = int(float(str(language_hist.index.values[0])))\n",
    "train['language'] = train['language'].map(language_map)\n",
    "test['language'] = test['language'].map(language_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode country, fill nan with most occurrences item\n",
    "country_hist = pd.concat([train['country'], test['country']], axis = 0).value_counts()\n",
    "merge_per = 0.25\n",
    "country_map = dict(zip(country_hist.index, list(range(len(country_hist)))))\n",
    "for key in list(country_hist[-int(len(country_hist)*merge_per):].index):\n",
    "    country_map[key] = int(len(country_hist)*(1-merge_per)) + 1\n",
    "train['country'] = train['country'].map(country_map)\n",
    "test['country'] = test['country'].map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# msno : category ; uinque values: 30755\n",
    "# song_id : category ; uinque values: 359966\n",
    "# - source_system_tab : category ; uinque values: 10\n",
    "# - source_screen_name : category ; uinque values: 21\n",
    "# - source_type : category ; uinque values: 13\n",
    "# - target : object ; uinque values: 2\n",
    "# - city : category ; uinque values: 21\n",
    "# - bd : category ; uinque values: 92\n",
    "# - gender : category ; uinque values: 3\n",
    "# - registered_via : category ; uinque values: 5\n",
    "# song_length : uint32 ; uinque values: 60271\n",
    "# genre_ids : category ; uinque values: 573\n",
    "# artist_name : category ; uinque values: 40587\n",
    "# composer : category ; uinque values: 76072\n",
    "# lyricist : category ; uinque values: 33895\n",
    "# - language : category ; uinque values: 11\n",
    "# name : category ; uinque values: 234144\n",
    "# - country : category ; uinque values: 107\n",
    "# - song_year : float64 ; uinque values: 100\n",
    "# - registration_year : uint16 ; uinque values: 14\n",
    "# - registration_month : uint8 ; uinque values: 12\n",
    "# - registration_date : uint8 ; uinque values: 31\n",
    "# - expiration_year : uint16 ; uinque values: 18\n",
    "# - expiration_month : uint8 ; uinque values: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_transform(input_train_data, input_test_data, columns_to_transform):\n",
    "    for col in columns_to_transform:\n",
    "        le = LabelEncoder()\n",
    "        train_values = list(input_train_data[col].unique())\n",
    "        test_values = list(input_test_data[col].unique())\n",
    "        le.fit(train_values + test_values)\n",
    "        input_train_data[col] = le.transform(input_train_data[col])\n",
    "        input_test_data[col] = le.transform(input_test_data[col])\n",
    "    return input_train_data, input_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = one_hot_transform(train, test, ['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender', 'bd', 'name', 'artist_name', 'composer', 'lyricist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: wether song_id should be merged like this or not? 231475 reserved and 188364 merged\n",
    "def encode_with_merge(input_train, input_test, columns, merge_value):\n",
    "    for index, col in enumerate(columns):\n",
    "        values_hist = pd.concat([input_train[col], input_train[col]], axis = 0).value_counts()\n",
    "        reserve_rows = values_hist[values_hist!=merge_value[index]]\n",
    "        merge_rows = values_hist[values_hist==merge_value[index]]\n",
    "\n",
    "        reserve_dict = dict(zip(list(reserve_rows.index), list(range(len(reserve_rows)))))\n",
    "        merge_dict = dict(zip(list(merge_rows.index), [len(reserve_rows)+1]*len(merge_rows.index)))\n",
    "        \n",
    "        map_dict = {**reserve_dict, **merge_dict}\n",
    "        \n",
    "        language_map['nan'] = int(float(str(language_hist.index.values[0])))\n",
    "        input_train[col] = input_train[col].map(map_dict)\n",
    "        input_test[col] = input_test[col].map(map_dict)\n",
    "    return input_train, input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = encode_with_merge(train, test, ['msno', 'song_id', 'genre_ids'], [1, 1, 1])\n",
    "# print(train.head())\n",
    "# print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.to_numeric(train['language'], downcast='signed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_transform(train_data, test_data):\n",
    "    train_data['song_length'] = np.log(pd.to_numeric(train_data['song_length'], downcast='float') + 1)\n",
    "    test_data['song_length'] = np.log(pd.to_numeric(test_data['song_length'], downcast='float') + 1)\n",
    "    return train_data, test_data\n",
    "train, test = log_transform(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "def cal_song_listen_times(train_data, test_data):\n",
    "    test_data['song_id'] = pd.to_numeric(test_data['song_id'], downcast='unsigned')\n",
    "    for index, _ in enumerate(time_wnd[:-1]):\n",
    "        begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "#         begin_time = time_wnd[index]\n",
    "#         end_time = time_wnd[index+1]\n",
    "        select_data = train_data[train_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "        select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "        grouped = select_data[['song_id', 'target']].groupby(['song_id'])\n",
    "\n",
    "        count_song = grouped.agg(['count'])\n",
    "        mean_repeat_song = grouped['target'].mean()\n",
    "\n",
    "        popularity = pd.concat([np.log(count_song+1), mean_repeat_song, np.log(count_song.multiply(mean_repeat_song, axis=0)+1)], axis=1, join=\"inner\")\n",
    "        popularity.columns = ['popular_{}'.format(index), 'mean_repeat_{}'.format(index), 'replay_prob_{}'.format(index)]\n",
    "        popularity = popularity.reset_index(drop=False)\n",
    "        test_data = test_data.merge(popularity, on='song_id', how ='left')\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "test = cal_song_listen_times(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.reset_index(drop=False)\n",
    "#test['msno'] = test['msno'].astype(int)   \n",
    "train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "\n",
    "grouped = train[['msno', 'target']].groupby(['msno'])\n",
    "\n",
    "count_msno = grouped.agg(['count'])\n",
    "mean_repeat_msno = grouped['target'].mean()\n",
    "\n",
    "popularity = pd.concat([np.log(count_msno+1), mean_repeat_msno, np.log(count_msno.multiply(mean_repeat_msno, axis=0)+1)], axis=1, join=\"inner\")\n",
    "popularity.columns = ['ms_popular', 'ms_mean_repeat', 'ms_replay_prob']\n",
    "popularity = popularity.reset_index(drop=False)\n",
    "test = test.merge(popularity, on='msno', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in ['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender',\\\n",
    "            'bd', 'name', 'artist_name', 'composer', 'lyricist', 'msno', 'song_id', 'genre_ids',\\\n",
    "           'country', 'language', 'registered_via']:\n",
    "    train[col] = train[col].astype('category')\n",
    "    test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index : int64 ; uinque values: 2556790\n",
      "id : object ; uinque values: 2556790\n",
      "msno : category ; uinque values: 21483\n",
      "song_id : category ; uinque values: 164880\n",
      "source_system_tab : category ; uinque values: 10\n",
      "source_screen_name : category ; uinque values: 23\n",
      "source_type : category ; uinque values: 13\n",
      "city : category ; uinque values: 21\n",
      "bd : category ; uinque values: 89\n",
      "gender : category ; uinque values: 3\n",
      "registered_via : category ; uinque values: 5\n",
      "song_length : float32 ; uinque values: 45659\n",
      "genre_ids : category ; uinque values: 466\n",
      "artist_name : category ; uinque values: 27564\n",
      "composer : category ; uinque values: 52310\n",
      "lyricist : category ; uinque values: 24913\n",
      "language : category ; uinque values: 10\n",
      "name : category ; uinque values: 154716\n",
      "country : category ; uinque values: 82\n",
      "song_year : float64 ; uinque values: 100\n",
      "registration_year : uint16 ; uinque values: 14\n",
      "registration_month : uint8 ; uinque values: 12\n",
      "registration_day : uint8 ; uinque values: 31\n",
      "expiration_year : uint16 ; uinque values: 16\n",
      "expiration_month : uint8 ; uinque values: 12\n",
      "expiration_day : uint8 ; uinque values: 31\n",
      "days : int64 ; uinque values: 4240\n",
      "popular_0 : float64 ; uinque values: 1763\n",
      "mean_repeat_0 : float64 ; uinque values: 7363\n",
      "replay_prob_0 : float64 ; uinque values: 1325\n",
      "popular_1 : float64 ; uinque values: 389\n",
      "mean_repeat_1 : float64 ; uinque values: 1146\n",
      "replay_prob_1 : float64 ; uinque values: 268\n",
      "popular_2 : float64 ; uinque values: 855\n",
      "mean_repeat_2 : float64 ; uinque values: 2799\n",
      "replay_prob_2 : float64 ; uinque values: 624\n",
      "popular_3 : float64 ; uinque values: 870\n",
      "mean_repeat_3 : float64 ; uinque values: 2648\n",
      "replay_prob_3 : float64 ; uinque values: 661\n",
      "popular_4 : float64 ; uinque values: 1379\n",
      "mean_repeat_4 : float64 ; uinque values: 4414\n",
      "replay_prob_4 : float64 ; uinque values: 1035\n",
      "ms_popular : float64 ; uinque values: 1558\n",
      "ms_mean_repeat : float64 ; uinque values: 13197\n",
      "ms_replay_prob : float64 ; uinque values: 1074\n"
     ]
    }
   ],
   "source": [
    "for col in test.columns: print(col, ':', test[col].dtype, '; uinque values:', len(test[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno : category ; uinque values: 30755\n",
      "song_id : category ; uinque values: 359966\n",
      "source_system_tab : category ; uinque values: 10\n",
      "source_screen_name : category ; uinque values: 21\n",
      "source_type : category ; uinque values: 13\n",
      "target : int8 ; uinque values: 2\n",
      "city : category ; uinque values: 21\n",
      "bd : category ; uinque values: 92\n",
      "gender : category ; uinque values: 3\n",
      "registered_via : category ; uinque values: 5\n",
      "song_length : float32 ; uinque values: 60269\n",
      "genre_ids : category ; uinque values: 573\n",
      "artist_name : category ; uinque values: 40587\n",
      "composer : category ; uinque values: 76072\n",
      "lyricist : category ; uinque values: 33895\n",
      "language : category ; uinque values: 10\n",
      "name : category ; uinque values: 234144\n",
      "country : category ; uinque values: 82\n",
      "song_year : float64 ; uinque values: 100\n",
      "registration_year : uint16 ; uinque values: 14\n",
      "registration_month : uint8 ; uinque values: 12\n",
      "registration_day : uint8 ; uinque values: 31\n",
      "expiration_year : uint16 ; uinque values: 18\n",
      "expiration_month : uint8 ; uinque values: 12\n",
      "expiration_day : uint8 ; uinque values: 31\n",
      "days : int64 ; uinque values: 4321\n"
     ]
    }
   ],
   "source": [
    "for col in train.columns: print(col, ':', train[col].dtype, '; uinque values:', len(train[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_test = pd.HDFStore(VALIDATION_INDICE)\n",
    "validation_list = store_test['keep_index']['index'].values\n",
    "store_test.close()\n",
    "train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "validation_use = train.iloc[validation_list].copy(deep=True).reset_index(drop=True)\n",
    "#train_use = train.drop(validation_list)\n",
    "train_use = train.drop(list(range(7277417, 7377417)))\n",
    "# train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "# validation_use = train[50:].copy(deep=True).reset_index(drop=True)\n",
    "# train_use = train.drop(list(range(50,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556790 2556790\n"
     ]
    }
   ],
   "source": [
    "print(len(test_id), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_tune_with_val(params, tune_param, param_list, data_list, val_data, less_prefered = False):\n",
    "    #data_list = {'train':{'x':train_d,'y':train_y}, 'validation':{'x':valid_d,'y':valid_y}}\n",
    "    best_metric = (less_prefered and sys.float_info.max or -sys.float_info.max)\n",
    "    best_param = param_list[0]\n",
    "\n",
    "    for par_value in param_list:\n",
    "        params[tune_param] = par_value\n",
    "        # , num_boost_round=params['num_boost_round'], early_stopping_rounds = params['early_stopping_rounds']\n",
    "        model = lgb.train(params, data_list['train']['x'], valid_sets=[data_list['validation']['x']], \\\n",
    "                feature_name='auto', #categorical_feature=['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender',\\\n",
    "                                     #                       'bd', 'name', 'artist_name', 'composer', 'lyricist', 'msno', 'song_id', 'genre_ids',\\\n",
    "                                     #                       'country', 'language', 'registered_via'],、\n",
    "                        )\n",
    "       \n",
    "        val_predprob = model.predict(val_data)\n",
    "        auroc_score = metrics.roc_auc_score(data_list['validation']['y'], val_predprob)\n",
    "\n",
    "        if (not less_prefered and auroc_score > best_metric) or (less_prefered and auroc_score < best_metric):\n",
    "            best_metric = auroc_score\n",
    "            best_param = par_value\n",
    "    log.info('best param for {}: {}, metric: {}'.format(tune_param, best_param, best_metric))\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#{'top_k': 20, 'feature_fraction': 0.8, 'bagging_freq': 1, 'min_data_in_bin': 3, 'min_sum_hessian_in_leaf': 0.001, 'bagging_fraction': 0.9, 'max_depth': 12, 'num_leaves': 100, 'learning_rate': 0.01, 'objective': 'binary', 'lambda_l2': 0.01, 'feature_fraction_seed': 1024, 'min_data_in_leaf': 15, 'max_bin': 100, 'verbose': 0, 'bagging_seed': 6666, 'max_cat_to_onehot': 4, 'metric': 'auc', 'lambda_l1': 1e-05, 'num_threads': 16, 'boosting': 'gbdt', 'min_split_gain': 0.3}\n",
    "\n",
    "#{'bagging_seed': 6666, 'lambda_l1': 1e-05, 'lambda_l2': 0.01, 'metric': 'auc', 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 0.001, 'feature_fraction': 0.8, 'feature_fraction_seed': 1024, 'num_leaves': 90, 'boosting': 'gbdt', 'verbose': 0, 'min_data_in_leaf': 15, 'top_k': 20, 'objective': 'binary', 'min_data_in_bin': 3, 'num_threads': 16, 'max_cat_to_onehot': 4, 'max_depth': 10, 'bagging_fraction': 0.9, 'learning_rate': 0.01, 'max_bin': 80, 'min_split_gain': 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_for_best_params(train, validation, test):\n",
    "    \n",
    "    X_train = lgb.Dataset(np.array(train.drop(['target'], axis=1)), label=train['target'].values)\n",
    "    X_valid = lgb.Dataset(np.array(validation.drop(['target'], axis=1)), label=validation['target'].values)\n",
    "    \n",
    "    y_train = train['target'].values\n",
    "    y_valid = validation['target'].values\n",
    "\n",
    "    X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "    data_list = {'train':{'x':X_train,'y':y_train}, 'validation':{'x':X_valid,'y':y_valid}}\n",
    "######## for value rather than catogory ################\n",
    "#   params_to_eval = OrderedDict(\n",
    "#         ( \n",
    "#         ('num_boost_round', range(120,150,10)),\n",
    "#         ('num_leaves', range(80,100,10)), # number of leaves in one tree\n",
    "#         ('max_depth', range(8,12,1)),\n",
    "#         ('min_data_in_leaf', 15),\n",
    "#         ('min_sum_hessian_in_leaf', [0.001]),# too high will lead to under-fitting\n",
    "#         ('min_split_gain',[0.3]),# the minimum loss reduction required to make a split\n",
    "#         ('bagging_fraction',[0.9]),# [i/10.0 for i in range(6,10)]\n",
    "#         ('feature_fraction',[0.8]),# typical: 0.5-1\n",
    "#         ('max_bin', range(70,90,10)),\n",
    "#         ('lambda_l2',[0.01]),\n",
    "#         ('lambda_l1',[1e-5]),\n",
    "#         ('learning_rate',[0.01]), # typical: 0.01-0.2\n",
    "#         )\n",
    "#       )\n",
    "     \n",
    "#     initial_params = {\n",
    "#         'objective': 'binary',\n",
    "#         'boosting': 'gbdt',\n",
    "#         'num_boost_round': 140,\n",
    "#         'learning_rate': 0.01 ,\n",
    "#         'verbose': 0,\n",
    "#         'num_leaves': 90,\n",
    "#         'num_threads':16,\n",
    "#         'max_depth': 9,\n",
    "#         'min_data_in_leaf': 15, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "#         'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "#         'feature_fraction': 0.8, #colsample_bytree\n",
    "#         'feature_fraction_seed': 1024,\n",
    "#         'bagging_fraction': 0.9, #subsample\n",
    "#         'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "#         'bagging_seed': 6666,\n",
    "#         'early_stopping_rounds':10,   \n",
    "#         'lambda_l1': 1e-5, #L1 regularization\n",
    "#         'lambda_l2': 0.01, #L2 regularization\n",
    "#         'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "#         'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "#         'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "#         'max_bin': 70, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "#         'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "#         'metric' : 'auc',\n",
    "#     } \n",
    "    params_to_eval = OrderedDict(\n",
    "        ( \n",
    "        ('num_boost_round', range(100,400,50)),\n",
    "        ('num_leaves', range(80,160,10)), # number of leaves in one tree\n",
    "        ('max_depth', range(8,18,1)),\n",
    "        ('min_data_in_leaf', range(10,18,2)),\n",
    "        ('min_sum_hessian_in_leaf', [0.001]),# too high will lead to under-fitting\n",
    "        ('min_split_gain',[0.3]),# the minimum loss reduction required to make a split\n",
    "        ('bagging_fraction',[0.9]),# [i/10.0 for i in range(6,10)]\n",
    "        ('feature_fraction',[0.8]),# typical: 0.5-1\n",
    "        ('max_bin', range(80,200,10)),\n",
    "        ('lambda_l2',[0.01]),\n",
    "        ('lambda_l1',[1e-5]),\n",
    "        ('learning_rate',[0.01]), # typical: 0.01-0.2\n",
    "        )\n",
    "      )\n",
    "     \n",
    "    initial_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_boost_round': 200,\n",
    "        'learning_rate': 0.1 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 120,\n",
    "        'num_threads':16,\n",
    "        'max_depth': 14,\n",
    "        'min_data_in_leaf': 16, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.8, #colsample_bytree\n",
    "        'feature_fraction_seed': 1024,\n",
    "        'bagging_fraction': 0.9, #subsample\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "        'bagging_seed': 6666,\n",
    "        'early_stopping_rounds':10,   \n",
    "        'lambda_l1': 1e-5, #L1 regularization\n",
    "        'lambda_l2': 0.01, #L2 regularization\n",
    "        'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "        'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "        'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "        'max_bin': 140, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "        'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "        'metric' : 'auc',\n",
    "    } \n",
    "    # only param nin this list are tuned, total list are ['n_estimators', 'reg_alpha', 'reg_lambda', 'subsample', 'colsample_bytree', 'min_child_weight', 'max_depth', 'learning_rate', 'gamma']\n",
    "    #tuned_param_name = ['num_boost_round', 'num_leaves', 'max_depth', 'max_bin']\n",
    "    tuned_param_name = ['num_boost_round', 'num_leaves', 'max_depth', 'min_data_in_leaf', 'min_sum_hessian_in_leaf',\\\n",
    "                        'min_split_gain', 'bagging_fraction', 'feature_fraction', 'max_bin', 'lambda_l2', 'lambda_l1', 'learning_rate']\n",
    "    for par_name, par_list in params_to_eval.items():\n",
    "        if par_name in tuned_param_name:\n",
    "            log.info('tunning {}...'.format(par_name))\n",
    "            if len(par_list) > 1:\n",
    "                initial_params[par_name] = param_tune_with_val(initial_params, par_name, par_list, data_list, np.array(validation.drop(['target'], axis=1)))\n",
    "            else:\n",
    "                initial_params[par_name] = par_list[0]\n",
    "    \n",
    "    return initial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_param = search_for_best_params(train_use, validation_use, test)\n",
    "log.info(best_param)\n",
    "time_elapsed = time.time() - start_time\n",
    "log.info('time used: {:.3f}sec'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_boost_round': 140,\n",
    "        'learning_rate': 0.01 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 90,\n",
    "        'num_threads':16,\n",
    "        'max_depth': 9,\n",
    "        'min_data_in_leaf': 15, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.8, #colsample_bytree\n",
    "        'feature_fraction_seed': 1024,\n",
    "        'bagging_fraction': 0.9, #subsample\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "        'bagging_seed': 6666,\n",
    "        'early_stopping_rounds':10,   \n",
    "        'lambda_l1': 1e-5, #L1 regularization\n",
    "        'lambda_l2': 0.01, #L2 regularization\n",
    "        'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "        'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "        'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "        'max_bin': 70, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "        'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "        'metric' : 'auc',\n",
    "    } \n",
    "X_train = lgb.Dataset(np.array(train_use.drop(['target'], axis=1)), label=train_use['target'].values)\n",
    "X_valid = lgb.Dataset(np.array(validation_use.drop(['target'], axis=1)), label=validation_use['target'].values)\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "model = lgb.train(params, X_train, valid_sets=[X_valid])\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': pred})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapok/pyenv35/lib/python3.5/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.627199\n",
      "[2]\tvalid_0's auc: 0.631447\n",
      "[3]\tvalid_0's auc: 0.635049\n",
      "[4]\tvalid_0's auc: 0.638507\n",
      "[5]\tvalid_0's auc: 0.640811\n",
      "[6]\tvalid_0's auc: 0.641259\n",
      "[7]\tvalid_0's auc: 0.644187\n",
      "[8]\tvalid_0's auc: 0.645293\n",
      "[9]\tvalid_0's auc: 0.646921\n",
      "[10]\tvalid_0's auc: 0.6483\n",
      "[11]\tvalid_0's auc: 0.649746\n",
      "[12]\tvalid_0's auc: 0.65121\n",
      "[13]\tvalid_0's auc: 0.651658\n",
      "[14]\tvalid_0's auc: 0.652625\n",
      "[15]\tvalid_0's auc: 0.653125\n",
      "[16]\tvalid_0's auc: 0.65395\n",
      "[17]\tvalid_0's auc: 0.655048\n",
      "[18]\tvalid_0's auc: 0.655343\n",
      "[19]\tvalid_0's auc: 0.656289\n",
      "[20]\tvalid_0's auc: 0.656791\n",
      "[21]\tvalid_0's auc: 0.657604\n",
      "[22]\tvalid_0's auc: 0.657731\n",
      "[23]\tvalid_0's auc: 0.658008\n",
      "[24]\tvalid_0's auc: 0.658669\n",
      "[25]\tvalid_0's auc: 0.659192\n",
      "[26]\tvalid_0's auc: 0.659256\n",
      "[27]\tvalid_0's auc: 0.659737\n",
      "[28]\tvalid_0's auc: 0.66018\n",
      "[29]\tvalid_0's auc: 0.660534\n",
      "[30]\tvalid_0's auc: 0.660899\n",
      "[31]\tvalid_0's auc: 0.661548\n",
      "[32]\tvalid_0's auc: 0.661921\n",
      "[33]\tvalid_0's auc: 0.662259\n",
      "[34]\tvalid_0's auc: 0.662877\n",
      "[35]\tvalid_0's auc: 0.663265\n",
      "[36]\tvalid_0's auc: 0.66351\n",
      "[37]\tvalid_0's auc: 0.663842\n",
      "[38]\tvalid_0's auc: 0.664326\n",
      "[39]\tvalid_0's auc: 0.664508\n",
      "[40]\tvalid_0's auc: 0.664722\n",
      "[41]\tvalid_0's auc: 0.665289\n",
      "[42]\tvalid_0's auc: 0.665325\n",
      "[43]\tvalid_0's auc: 0.665412\n",
      "[44]\tvalid_0's auc: 0.665646\n",
      "[45]\tvalid_0's auc: 0.665917\n",
      "[46]\tvalid_0's auc: 0.666162\n",
      "[47]\tvalid_0's auc: 0.666232\n",
      "[48]\tvalid_0's auc: 0.666507\n",
      "[49]\tvalid_0's auc: 0.666696\n",
      "[50]\tvalid_0's auc: 0.666718\n",
      "[51]\tvalid_0's auc: 0.66683\n",
      "[52]\tvalid_0's auc: 0.666911\n",
      "[53]\tvalid_0's auc: 0.667181\n",
      "[54]\tvalid_0's auc: 0.667235\n",
      "[55]\tvalid_0's auc: 0.667451\n",
      "[56]\tvalid_0's auc: 0.667761\n",
      "[57]\tvalid_0's auc: 0.667873\n",
      "[58]\tvalid_0's auc: 0.668244\n",
      "[59]\tvalid_0's auc: 0.668433\n",
      "[60]\tvalid_0's auc: 0.668651\n",
      "[61]\tvalid_0's auc: 0.668817\n",
      "[62]\tvalid_0's auc: 0.66887\n",
      "[63]\tvalid_0's auc: 0.668974\n",
      "[64]\tvalid_0's auc: 0.669083\n",
      "[65]\tvalid_0's auc: 0.669216\n",
      "[66]\tvalid_0's auc: 0.669251\n",
      "[67]\tvalid_0's auc: 0.669352\n",
      "[68]\tvalid_0's auc: 0.669384\n",
      "[69]\tvalid_0's auc: 0.669514\n",
      "[70]\tvalid_0's auc: 0.669606\n",
      "[71]\tvalid_0's auc: 0.669703\n",
      "[72]\tvalid_0's auc: 0.669838\n",
      "[73]\tvalid_0's auc: 0.669943\n",
      "[74]\tvalid_0's auc: 0.669917\n",
      "[75]\tvalid_0's auc: 0.670014\n",
      "[76]\tvalid_0's auc: 0.670148\n",
      "[77]\tvalid_0's auc: 0.670282\n",
      "[78]\tvalid_0's auc: 0.670329\n",
      "[79]\tvalid_0's auc: 0.670506\n",
      "[80]\tvalid_0's auc: 0.670674\n",
      "[81]\tvalid_0's auc: 0.670757\n",
      "[82]\tvalid_0's auc: 0.670892\n",
      "[83]\tvalid_0's auc: 0.670999\n",
      "[84]\tvalid_0's auc: 0.671255\n",
      "[85]\tvalid_0's auc: 0.671325\n",
      "[86]\tvalid_0's auc: 0.671557\n",
      "[87]\tvalid_0's auc: 0.671737\n",
      "[88]\tvalid_0's auc: 0.672037\n",
      "[89]\tvalid_0's auc: 0.672173\n",
      "[90]\tvalid_0's auc: 0.672229\n",
      "[91]\tvalid_0's auc: 0.672288\n",
      "[92]\tvalid_0's auc: 0.672419\n",
      "[93]\tvalid_0's auc: 0.672467\n",
      "[94]\tvalid_0's auc: 0.672539\n",
      "[95]\tvalid_0's auc: 0.672721\n",
      "[96]\tvalid_0's auc: 0.672784\n",
      "[97]\tvalid_0's auc: 0.672959\n",
      "[98]\tvalid_0's auc: 0.673049\n",
      "[99]\tvalid_0's auc: 0.673153\n",
      "[100]\tvalid_0's auc: 0.673309\n",
      "[101]\tvalid_0's auc: 0.673466\n",
      "[102]\tvalid_0's auc: 0.67363\n",
      "[103]\tvalid_0's auc: 0.673775\n",
      "[104]\tvalid_0's auc: 0.673872\n",
      "[105]\tvalid_0's auc: 0.673978\n",
      "[106]\tvalid_0's auc: 0.674241\n",
      "[107]\tvalid_0's auc: 0.674392\n",
      "[108]\tvalid_0's auc: 0.674495\n",
      "[109]\tvalid_0's auc: 0.674825\n",
      "[110]\tvalid_0's auc: 0.674888\n",
      "[111]\tvalid_0's auc: 0.674961\n",
      "[112]\tvalid_0's auc: 0.675068\n",
      "[113]\tvalid_0's auc: 0.675146\n",
      "[114]\tvalid_0's auc: 0.675265\n",
      "[115]\tvalid_0's auc: 0.67534\n",
      "[116]\tvalid_0's auc: 0.675407\n",
      "[117]\tvalid_0's auc: 0.675576\n",
      "[118]\tvalid_0's auc: 0.675803\n",
      "[119]\tvalid_0's auc: 0.675859\n",
      "[120]\tvalid_0's auc: 0.675969\n",
      "[121]\tvalid_0's auc: 0.676136\n",
      "[122]\tvalid_0's auc: 0.676192\n",
      "[123]\tvalid_0's auc: 0.676212\n",
      "[124]\tvalid_0's auc: 0.67627\n",
      "[125]\tvalid_0's auc: 0.676398\n",
      "[126]\tvalid_0's auc: 0.676531\n",
      "[127]\tvalid_0's auc: 0.676507\n",
      "[128]\tvalid_0's auc: 0.6767\n",
      "[129]\tvalid_0's auc: 0.677056\n",
      "[130]\tvalid_0's auc: 0.677071\n",
      "[131]\tvalid_0's auc: 0.677189\n",
      "[132]\tvalid_0's auc: 0.677279\n",
      "[133]\tvalid_0's auc: 0.677564\n",
      "[134]\tvalid_0's auc: 0.677686\n",
      "[135]\tvalid_0's auc: 0.677728\n",
      "[136]\tvalid_0's auc: 0.677819\n",
      "[137]\tvalid_0's auc: 0.677912\n",
      "[138]\tvalid_0's auc: 0.677975\n",
      "[139]\tvalid_0's auc: 0.678083\n",
      "[140]\tvalid_0's auc: 0.678126\n",
      "[141]\tvalid_0's auc: 0.678141\n",
      "[142]\tvalid_0's auc: 0.67827\n",
      "[143]\tvalid_0's auc: 0.678346\n",
      "[144]\tvalid_0's auc: 0.678351\n",
      "[145]\tvalid_0's auc: 0.678459\n",
      "[146]\tvalid_0's auc: 0.678555\n",
      "[147]\tvalid_0's auc: 0.678648\n",
      "[148]\tvalid_0's auc: 0.678744\n",
      "[149]\tvalid_0's auc: 0.678992\n",
      "[150]\tvalid_0's auc: 0.679041\n",
      "[151]\tvalid_0's auc: 0.679116\n",
      "[152]\tvalid_0's auc: 0.679197\n",
      "[153]\tvalid_0's auc: 0.679285\n",
      "[154]\tvalid_0's auc: 0.679391\n",
      "[155]\tvalid_0's auc: 0.679432\n",
      "[156]\tvalid_0's auc: 0.679493\n",
      "[157]\tvalid_0's auc: 0.679426\n",
      "[158]\tvalid_0's auc: 0.679546\n",
      "[159]\tvalid_0's auc: 0.679525\n",
      "[160]\tvalid_0's auc: 0.67953\n",
      "[161]\tvalid_0's auc: 0.679664\n",
      "[162]\tvalid_0's auc: 0.679764\n",
      "[163]\tvalid_0's auc: 0.679881\n",
      "[164]\tvalid_0's auc: 0.68004\n",
      "[165]\tvalid_0's auc: 0.680083\n",
      "[166]\tvalid_0's auc: 0.680159\n",
      "[167]\tvalid_0's auc: 0.680203\n",
      "[168]\tvalid_0's auc: 0.680313\n",
      "[169]\tvalid_0's auc: 0.680356\n",
      "[170]\tvalid_0's auc: 0.68039\n",
      "[171]\tvalid_0's auc: 0.680567\n",
      "[172]\tvalid_0's auc: 0.680831\n",
      "[173]\tvalid_0's auc: 0.680893\n",
      "[174]\tvalid_0's auc: 0.680925\n",
      "[175]\tvalid_0's auc: 0.68092\n",
      "[176]\tvalid_0's auc: 0.680973\n",
      "[177]\tvalid_0's auc: 0.681025\n",
      "[178]\tvalid_0's auc: 0.681123\n",
      "[179]\tvalid_0's auc: 0.681231\n",
      "[180]\tvalid_0's auc: 0.68127\n",
      "[181]\tvalid_0's auc: 0.681368\n",
      "[182]\tvalid_0's auc: 0.681397\n",
      "[183]\tvalid_0's auc: 0.681495\n",
      "[184]\tvalid_0's auc: 0.681533\n",
      "[185]\tvalid_0's auc: 0.681617\n",
      "[186]\tvalid_0's auc: 0.681652\n",
      "[187]\tvalid_0's auc: 0.681649\n",
      "[188]\tvalid_0's auc: 0.681627\n",
      "[189]\tvalid_0's auc: 0.681709\n",
      "[190]\tvalid_0's auc: 0.681811\n",
      "[191]\tvalid_0's auc: 0.681894\n",
      "[192]\tvalid_0's auc: 0.681927\n",
      "[193]\tvalid_0's auc: 0.682015\n",
      "[194]\tvalid_0's auc: 0.68209\n",
      "[195]\tvalid_0's auc: 0.682161\n",
      "[196]\tvalid_0's auc: 0.682341\n",
      "[197]\tvalid_0's auc: 0.682378\n",
      "[198]\tvalid_0's auc: 0.682367\n",
      "[199]\tvalid_0's auc: 0.68243\n",
      "[200]\tvalid_0's auc: 0.682471\n",
      "cur fold finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "train_data = lgb.Dataset(train_use.drop(['target'],axis=1),label=train_use['target'])\n",
    "val_data = lgb.Dataset(validation_use.drop(['target'],axis=1),label=validation_use['target'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1 ,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 108,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 128,\n",
    "    'max_depth': 10,\n",
    "    'num_rounds': 200,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "predictions+=bst.predict(test.drop(['id'],axis=1))\n",
    "print('cur fold finished.')\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_use.drop(['target'], axis=1))\n",
    "y_train = train_use['target'].values\n",
    "\n",
    "X_valid = np.array(validation_use.drop(['target'], axis=1))\n",
    "y_valid = validation_use['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "# d_train = xgb.DMatrix(X_train)\n",
    "# d_valid = xgb.DMatrix(X_valid) \n",
    "# d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "data_list = {'train':{'x':X_train,'y':y_train}, 'validation':{'x':X_valid,'y':y_valid}}\n",
    "# Train model, evaluate and make predictions\n",
    "params={\n",
    "    'n_estimators':500,\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.75,\n",
    "    'gamma':0.1,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.3,\n",
    "    'min_child_weight':3,\n",
    "    'max_depth':16,\n",
    "    'seed':1024,\n",
    "    }\n",
    "\n",
    "param_tune_with_val(params, 'max_depth', [5,1,6], data_list, 'auc', 20)\n",
    "\n",
    "# model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=20, \\\n",
    "#     maximize=True, verbose_eval=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_use.drop(['target'], axis=1))\n",
    "y_train = train_use['target'].values\n",
    "\n",
    "X_valid = np.array(validation_use.drop(['target'], axis=1))\n",
    "y_valid = validation_use['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid) \n",
    "d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "# Train model, evaluate and make predictions\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.75\n",
    "params['max_depth'] = 16\n",
    "params['silent'] = 1\n",
    "params['eval_metric'] = 'auc'\n",
    "\n",
    "model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=20, \\\n",
    "    maximize=True, verbose_eval=5)\n",
    "\n",
    "#Predict training set:\n",
    "train_predictions = model.predict(X_train)\n",
    "train_predprob = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "val_predictions = model.predict(X_valid)\n",
    "val_predprob = model.predict_proba(X_valid)[:,1]\n",
    "\n",
    "#Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"Train Accuracy : %.4g\" % metrics.accuracy_score(y_train, train_predictions))\n",
    "print(\"Train AUC Score (Train): %f\" % metrics.roc_auc_score(y_train, train_predprob))\n",
    "print(\"ValAccuracy : %.4g\" % metrics.accuracy_score(y_valid, val_predictions))\n",
    "print(\"Validation AUC Score (Train): %f\" % metrics.roc_auc_score(y_valid, val_predprob))\n",
    "\n",
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "\n",
    "p_test = model.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "modelfit(xgb1, train_use.drop(['target'],axis=1), train_use['target'], validation_use.drop(['target'],axis=1), validation_use['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'], eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, train, label, validation, val_label, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train.values, label=label.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train, label, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train)\n",
    "    train_predprob = alg.predict_proba(train)[:,1]\n",
    "    \n",
    "    val_predictions = alg.predict(validation)\n",
    "    val_predprob = alg.predict_proba(validation)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Train Accuracy : %.4g\" % metrics.accuracy_score(label.values, train_predictions))\n",
    "    print(\"Train AUC Score (Train): %f\" % metrics.roc_auc_score(label, train_predprob))\n",
    "    print(\"ValAccuracy : %.4g\" % metrics.accuracy_score(val_label.values, val_predictions))\n",
    "    print(\"Validation AUC Score (Train): %f\" % metrics.roc_auc_score(val_label, val_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "modelfit(xgb1, train_use.drop(['target'],axis=1), train_use['target'], validation_use.drop(['target'],axis=1), validation_use['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(train_use.drop(['target'],axis=1), label=train_use['target'])\n",
    "val_data = lgb.Dataset(validation_use.drop(['target'],axis=1), label=validation_use['target'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1 ,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 108,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 128,\n",
    "    'max_depth': 10,\n",
    "    'num_rounds': 200,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "predictions=bst.predict(test.drop(['id'],axis=1))\n",
    "print('finished.')\n",
    "\n",
    "    \n",
    "predictions = predictions/3\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "for train_indices,val_indices in kf.split(train) : \n",
    "    train_data = lgb.Dataset(train.drop(['target'],axis=1).loc[train_indices,:],label=train.loc[train_indices,'target'])\n",
    "    val_data = lgb.Dataset(train.drop(['target'],axis=1).loc[val_indices,:],label=train.loc[val_indices,'target'])\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.1 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 108,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 128,\n",
    "        'max_depth': 10,\n",
    "        'num_rounds': 200,\n",
    "        'metric' : 'auc',\n",
    "        } \n",
    "    \n",
    "    bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "    predictions+=bst.predict(test.drop(['id'],axis=1))\n",
    "    print('cur fold finished.')\n",
    "    del bst\n",
    "    \n",
    "predictions = predictions/3\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess songs data\n",
    "songs_genres = np.array(songs['genre_ids']\\\n",
    "    .apply(lambda x: [int(v) for v in str(x).split('|')]))\n",
    "genres_list = songs_genres.ravel().unique()\n",
    "print('Number of genres: ' + str(len(genres_list)))\n",
    "\n",
    "ohe_genres = np.zeros((len(songs_genres), len(genres_list)))\n",
    "for s_i, s_genres in enumerate(songs_genres):\n",
    "    for genre in s_genres:\n",
    "        g_i = genres_list.find(genre)\n",
    "        ohe_genres[s_i, g_i] = 1\n",
    "        \n",
    "for g_i, g in enumerate(genres_list):\n",
    "    songs['genre_' + str(g)] = ohe_genres[:, g_i]\n",
    "print(songs.head())\n",
    "songs = songs.drop(['genre_ids'], axis=1)\n",
    "\n",
    "song_cols = songs.columns\n",
    "\n",
    "# Preprocess dataset\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "cols = list(train.columns)\n",
    "cols.remove('target')\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = train[col].apply(str)\n",
    "        test[col] = test[col].apply(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        train_vals = list(train[col].unique())\n",
    "        test_vals = list(test[col].unique())\n",
    "        le.fit(train_vals + test_vals)\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "\n",
    "        print(col + ': ' + str(len(train_vals)) + ', ' + str(len(test_vals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "########################################\n",
    "## load the data\n",
    "########################################\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "uid = train.msno\n",
    "sid = train.song_id\n",
    "target = train.target\n",
    "\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "id_test = test.id\n",
    "uid_test = test.msno\n",
    "sid_test = test.song_id\n",
    "\n",
    "########################################\n",
    "## encoding\n",
    "########################################\n",
    "\n",
    "usr_encoder = LabelEncoder()\n",
    "usr_encoder.fit(uid.append(uid_test))\n",
    "uid = usr_encoder.transform(uid)\n",
    "uid_test = usr_encoder.transform(uid_test)\n",
    "\n",
    "sid_encoder = LabelEncoder()\n",
    "sid_encoder.fit(sid.append(sid_test))\n",
    "sid = sid_encoder.transform(sid)\n",
    "sid_test = sid_encoder.transform(sid_test)\n",
    "\n",
    "u_cnt = int(max(uid.max(), uid_test.max()) + 1)\n",
    "s_cnt = int(max(sid.max(), sid_test.max()) + 1)\n",
    "\n",
    "########################################\n",
    "## train-validation split\n",
    "########################################\n",
    "\n",
    "perm = np.random.permutation(len(train))\n",
    "trn_cnt = int(len(train) * 0.85)\n",
    "uid_trn = uid[perm[:trn_cnt]]\n",
    "uid_val = uid[perm[trn_cnt:]]\n",
    "sid_trn = sid[perm[:trn_cnt]]\n",
    "sid_val = sid[perm[trn_cnt:]]\n",
    "target_trn = target[perm[:trn_cnt]]\n",
    "target_val = target[perm[trn_cnt:]]\n",
    "\n",
    "########################################\n",
    "## define the model\n",
    "########################################\n",
    "\n",
    "def get_model():\n",
    "    user_embeddings = Embedding(u_cnt,\n",
    "            64,\n",
    "            embeddings_initializer=RandomUniform(minval=-0.1, maxval=0.1),\n",
    "            embeddings_regularizer=l2(1e-4),\n",
    "            input_length=1,\n",
    "            trainable=True)\n",
    "    song_embeddings = Embedding(s_cnt,\n",
    "            64,\n",
    "            embeddings_initializer=RandomUniform(minval=-0.1, maxval=0.1),\n",
    "            embeddings_regularizer=l2(1e-4),\n",
    "            input_length=1,\n",
    "            trainable=True)\n",
    "\n",
    "    uid_input = Input(shape=(1,), dtype='int32')\n",
    "    embedded_usr = user_embeddings(uid_input)\n",
    "    embedded_usr = Reshape((64,))(embedded_usr)\n",
    "\n",
    "    sid_input = Input(shape=(1,), dtype='int32')\n",
    "    embedded_song = song_embeddings(sid_input)\n",
    "    embedded_song = Reshape((64,))(embedded_song)\n",
    "\n",
    "    preds = dot([embedded_usr, embedded_song], axes=1)\n",
    "    preds = concatenate([embedded_usr, embedded_song, preds])\n",
    "    \n",
    "    preds = Dense(128, activation='relu')(preds)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    \n",
    "    preds = Dense(1, activation='sigmoid')(preds)\n",
    "\n",
    "    model = Model(inputs=[uid_input, sid_input], outputs=preds)\n",
    "    \n",
    "    opt = RMSprop(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "   \n",
    "model = get_model()\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=5)\n",
    "model_path = 'bst_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, \\\n",
    "        save_weights_only=True)\n",
    "\n",
    "hist = model.fit([uid_trn, sid_trn], target_trn, validation_data=([uid_val, sid_val], \\\n",
    "        target_val), epochs=100, batch_size=32768, shuffle=True, \\\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "model.load_weights(model_path)\n",
    "\n",
    "preds_val = model.predict([uid_val, sid_val], batch_size=32768)\n",
    "val_auc = roc_auc_score(target_val, preds_val)\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "\n",
    "preds_test = model.predict([uid_test, sid_test], batch_size=32768, verbose=1)\n",
    "sub = pd.DataFrame({'id': id_test, 'target': preds_test.ravel()})\n",
    "sub.to_csv('./sub_%.5f.csv'%(val_auc), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear algebra:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Graphics:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "# Frameworks:\n",
    "import lightgbm as lgb # LightGBM\n",
    "# Utils:\n",
    "import gc # garbage collector\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IDIR = '../input/' # main path\n",
    "members = pd.read_csv(IDIR + 'members.csv')\n",
    "songs = pd.read_csv(IDIR + 'songs.csv')\n",
    "song_extra_info = pd.read_csv(IDIR + 'song_extra_info.csv')\n",
    "train = pd.read_csv(IDIR + 'train.csv')\n",
    "test = pd.read_csv(IDIR + 'test.csv')\n",
    "\n",
    "# Adding songs' info:\n",
    "train_aug1 = pd.merge(left=train, right=songs, on='song_id', how='left')\n",
    "test_aug1 = pd.merge(left=test, right=songs, on='song_id', how='left')\n",
    "# Adding extra info about songs:\n",
    "train_aug2 = pd.merge(left=train_aug1, right=song_extra_info, on='song_id', how='left')\n",
    "test_aug2 = pd.merge(left=test_aug1, right=song_extra_info, on='song_id', how='left')\n",
    "del train_aug1, test_aug1\n",
    "# Addind users' info:\n",
    "train_aug3 = pd.merge(left=train_aug2, right=members, on='msno', how='left')\n",
    "test_aug3 = pd.merge(left=test_aug2, right=members, on='msno', how='left')\n",
    "del train_aug2, test_aug2\n",
    "# Merging train and test data:\n",
    "train_aug3.drop(['song_id'], axis=1, inplace=True)\n",
    "train_aug3['set'] = 0\n",
    "test_aug3.drop(['song_id'], axis=1, inplace=True)\n",
    "test_aug3['set'] = 1\n",
    "test_aug3['target'] = -1\n",
    "all_aug = pd.concat([train_aug3, test_aug3], axis=0)\n",
    "del train_aug3, test_aug3\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "\n",
    "# source_system_tab/source_screen_name/source_type/genre_ids/artist_name/composer/lyricist/name/isrc/gender 用'NA'填补并one-hot编码\n",
    "# genre_ids encoding:\n",
    "all_aug['genre_ids'] = all_aug.genre_ids.fillna('NA')\n",
    "all_aug['genre_ids'] = all_aug.genre_ids.astype(np.str)\n",
    "genre_ids_le = LabelEncoder()\n",
    "genre_ids_le.fit(all_aug.genre_ids)\n",
    "all_aug['genre_ids'] = genre_ids_le.transform(all_aug.genre_ids).astype(np.int16)\n",
    "\n",
    "# language encoding:\n",
    "all_aug['language'] = all_aug.language.fillna(-2)\n",
    "all_aug['language'] = all_aug.language.astype(np.int8)\n",
    "\n",
    "# city encoding:\n",
    "all_aug['city'] = all_aug.city.astype(np.int8)\n",
    "# bd encoding:\n",
    "all_aug['bd'] = all_aug.bd.astype(np.int16)\n",
    "\n",
    "# registered_via encoding:\n",
    "all_aug['registered_via'] = all_aug.registered_via.astype(np.int8)\n",
    "# registration_init_time encoding:\n",
    "all_aug['registration_init_time'] = all_aug.registration_init_time.astype(np.int32)\n",
    "# expiration_date encoding:\n",
    "all_aug['expiration_date'] = all_aug.expiration_date.astype(np.int32)\n",
    "# Info:\n",
    "all_aug.info(max_cols=0)\n",
    "all_aug.head(2)\n",
    "\n",
    "\n",
    "all_aug['exp_reg_time'] = all_aug.expiration_date - all_aug.registration_init_time\n",
    "\n",
    "\n",
    "\n",
    "gc.collect();\n",
    "d_train = lgb.Dataset(all_aug[all_aug.set == 0].drop(['target', 'msno', 'id', 'set'], axis=1), \n",
    "                      label=all_aug[all_aug.set == 0].pop('target'))\n",
    "ids_train = all_aug[all_aug.set == 0].pop('msno')\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 1.0,\n",
    "    'max_depth': 15,\n",
    "    'num_leaves': 250, \n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc'},\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 5,\n",
    "    'max_bin': 100}\n",
    "cv_result_lgb = lgb.cv(lgb_params, \n",
    "                       d_train, \n",
    "                       num_boost_round=5000, \n",
    "                       nfold=3, \n",
    "                       stratified=True, \n",
    "                       early_stopping_rounds=50, \n",
    "                       verbose_eval=100, \n",
    "                       show_stdv=True)\n",
    "\n",
    "num_boost_rounds_lgb = len(cv_result_lgb['auc-mean'])\n",
    "print('num_boost_rounds_lgb=' + str(num_boost_rounds_lgb))\n",
    "\n",
    "\n",
    "\n",
    "%%time\n",
    "ROUNDS = num_boost_rounds_lgb\n",
    "print('light GBM train :-)')\n",
    "bst = lgb.train(lgb_params, d_train, ROUNDS)\n",
    "# lgb.plot_importance(bst, figsize=(9,20))\n",
    "# del d_train\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "feature_imp = pd.Series(dict(zip(d_train.feature_name, \n",
    "                                 bst.feature_importance()))).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_imp.values, y=feature_imp.index.values, orient='h', color='g')\n",
    "plt.subplot(1,2,2)\n",
    "train_scores = np.array(cv_result_lgb['auc-mean'])\n",
    "train_stds = np.array(cv_result_lgb['auc-stdv'])\n",
    "plt.plot(train_scores, color='green')\n",
    "plt.fill_between(range(len(cv_result_lgb['auc-mean'])), \n",
    "                 train_scores - train_stds, train_scores + train_stds, \n",
    "                 alpha=0.1, color='green')\n",
    "plt.title('LightGMB CV-results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
