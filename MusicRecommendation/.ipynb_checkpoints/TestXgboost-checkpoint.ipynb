{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapok/pyenv35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "get_ipython().magic('env CUDA_VISIBLE_DEVICES = 1')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp      # will come in handy due to the size of the data\n",
    "import os.path\n",
    "import random\n",
    "import io\n",
    "from datetime import datetime\n",
    "import gc # garbage collector\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a pandas dataframe to disk as gunzip compressed csv\n",
    "- df.to_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Read from disk\n",
    "- df = pd.read_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Magic useful\n",
    "- %%timeit for the whole cell\n",
    "- %timeit for the specific line\n",
    "- %%latex to render the cell as a block of latex\n",
    "- %prun and %%prun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/WSDM/'\n",
    "TRAIN_FILE = DATASET_PATH + 'all_train_withextra.csv'\n",
    "TEST_FILE = DATASET_PATH + 'all_test_withextra.csv'\n",
    "MEMBER_FILE = DATASET_PATH + 'members.csv'\n",
    "SONG_FILE = DATASET_PATH + 'fix_songs.csv'\n",
    "ALL_ARTIST = DATASET_PATH + 'all_artist_name.csv'\n",
    "ALL_COMPOSER = DATASET_PATH + 'all_composer.csv'\n",
    "ALL_LYRICIST = DATASET_PATH + 'all_lyricist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_logging(logger_name, logger_file_name):\n",
    "    log = logging.getLogger(logger_name)\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create formatter and add it to the handlers\n",
    "    print_formatter = logging.Formatter('%(message)s')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s_%(levelname)s: %(message)s')\n",
    "\n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(logger_file_name, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(file_formatter)\n",
    "    log.addHandler(fh)\n",
    "    # both output to console and file\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(print_formatter)\n",
    "    log.addHandler(consoleHandler)\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "here is an info message.\n"
     ]
    }
   ],
   "source": [
    "log = set_logging('MUSIC', DATASET_PATH + 'music_test_xgboost.log')\n",
    "log.info('here is an info message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_FILE)\n",
    "test_data = pd.read_csv(TEST_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "member_data = pd.read_csv(MEMBER_FILE)\n",
    "song_data = pd.read_csv(SONG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "composer_df = pd.read_csv(ALL_COMPOSER)\n",
    "artist_name_df = pd.read_csv(ALL_ARTIST)\n",
    "lyricist_df = pd.read_csv(ALL_LYRICIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                           msno  \\\n",
      "0  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
      "1  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
      "2  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
      "3  Xumu+NIjS6QYVxDS4/t3SawvJ7viT9hPKXmf0RtLNx8=   \n",
      "4  FGtllVqz18RPiwJj/edr2gV78zirAiY/9SmYvia+kCg=   \n",
      "\n",
      "                                        song_id source_system_tab  \\\n",
      "0  BBzumQNXUHKdEBOB7mAJuzok+IJA1c2Ryg/yzTF6tik=           explore   \n",
      "1  bhp/MpSNoqoxOIB+/l8WPqu6jldth4DIpCm3ayXnJqM=        my library   \n",
      "2  JNWfrrC7zNN7BdMpsISKa4Mw+xVJYNnxXh3/Epw7QgY=        my library   \n",
      "3  2A87tzfnJTSWqD7gIZHisolhe4DMdzkbd6LzO1KHjNs=        my library   \n",
      "4  3qm6XTZ6MOCU11x8FIVbAGH5l5uMkT3/ZalWG1oo2Gc=           explore   \n",
      "\n",
      "    source_screen_name      source_type  target  city  bd  gender  \\\n",
      "0              Explore  online-playlist       1     1   0     NaN   \n",
      "1  Local playlist more   local-playlist       1    13  24  female   \n",
      "2  Local playlist more   local-playlist       1    13  24  female   \n",
      "3  Local playlist more   local-playlist       1    13  24  female   \n",
      "4              Explore  online-playlist       1     1   0     NaN   \n",
      "\n",
      "   registered_via  registration_init_time  expiration_date  song_length  \\\n",
      "0               7                20120102         20171005     206471.0   \n",
      "1               9                20110525         20170911     284584.0   \n",
      "2               9                20110525         20170911     225396.0   \n",
      "3               9                20110525         20170911     255512.0   \n",
      "4               7                20120102         20171005     187802.0   \n",
      "\n",
      "  genre_ids      artist_name                                 composer  \\\n",
      "0       359         Bastille                     Dan Smith| Mark Crew   \n",
      "1      1259  Various Artists                                      NaN   \n",
      "2      1259              Nas     N. Jones、W. Adams、J. Lordan、D. Ingle   \n",
      "3      1019         Soundway                            Kwadwo Donkoh   \n",
      "4      1011      Brett Young  Brett Young| Kelly Archer| Justin Ebach   \n",
      "\n",
      "  lyricist  language                                     name          isrc  \n",
      "0      NaN      52.0                               Good Grief  GBUM71602854  \n",
      "1      NaN      52.0                       Lords of Cardboard  US3C69910183  \n",
      "2      NaN      52.0  Hip Hop Is Dead(Album Version (Edited))  USUM70618761  \n",
      "3      NaN      -1.0                             Disco Africa  GBUQH1000063  \n",
      "4      NaN      52.0                        Sleep Without You  QM3E21606003  \n"
     ]
    }
   ],
   "source": [
    "log.info(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_by_percent(hist, num_percent):\n",
    "    return hist[(hist >= hist[int( len(hist.index) * num_percent )]) == True]\n",
    "def clip_by_value(hist, value):\n",
    "    return hist[(hist >= value) == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_words(input_df, percent, column_name):\n",
    "    input_hist = input_df[column_name].value_counts(sort=True, ascending=False)\n",
    "    input_select = clip_by_percent(input_hist, percent).index\n",
    "    print('{} item are selected.'.format(len(input_select)))\n",
    "    # the total number of the other items\n",
    "    total_others = np.sum(input_hist) - np.sum(input_hist[input_select])\n",
    "    # all hist values are log transformed accouting the popularity\n",
    "    clip_hist_with_log = defaultdict(lambda: np.log(total_others))\n",
    "    for k,v in dict(np.log(input_hist[input_select])).items():\n",
    "        clip_hist_with_log[k] = v\n",
    "#     print(input_hist[input_select])   \n",
    "#     print(dict(np.log(input_hist[input_select])))\n",
    "    input_map = defaultdict(lambda: column_name + ' ' + 'others')\n",
    "    for input_item in input_select:\n",
    "        input_map[input_item] = column_name + ' ' + input_item\n",
    "    # item name in input_map are \"column_name + ' ' + input_item\"\n",
    "    # item name in clip_hist_with_log are \"input_item\"\n",
    "    return input_map, clip_hist_with_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 181 ms ± 420 µs\n",
    "def word_bag_encode(input_data, column, word_map, word_hist):\n",
    "    col_index = input_data.columns.get_loc(column) + 1\n",
    "    count_list = [0 for _ in range(len(word_map))]\n",
    "    count_dict = dict(zip(list(word_map.keys()), count_list))\n",
    "    count_dict['others'] = 0\n",
    "    new_columns = [column + ' ' + s for s in count_dict.keys()]\n",
    "    all_df = pd.DataFrame(data = None, columns = new_columns)\n",
    "    delay_rate = 0.8 # must be less than 1\n",
    "    for cur_row in input_data.itertuples():\n",
    "        if isinstance(cur_row[col_index], str): \n",
    "            df = pd.DataFrame([list(count_dict.values())], columns=new_columns)\n",
    "            splited_list = re.split(r'[|/]+',cur_row[col_index])\n",
    "            list_len = len(splited_list)\n",
    "            # the weight of each position of the array, are decayed by the ratio delay_rate, and their sum are 1\n",
    "            # so according to the geometric series summation formula, the iniatial weight are caculate as follow\n",
    "            initial_weight = (1-delay_rate)/(1 - np.power(delay_rate, list_len))\n",
    "            for index, s in enumerate(splited_list): \n",
    "                word_stripped = s.strip(' \\\"\\t\\s\\n')\n",
    "                df[word_map.get(word_stripped, column + ' others')] += initial_weight / (word_hist.get(word_stripped, word_hist['others'])) #word_hist[word_stripped]\n",
    "                # defaultdict will auto insert missing key\n",
    "                #df[word_map[word_stripped]] += initial_weight / (word_hist.get(word_stripped, word_hist['others'])) #word_hist[word_stripped]\n",
    "                initial_weight *= delay_rate\n",
    "            all_df = all_df.append(df, ignore_index=True)\n",
    "        # NAN fix\n",
    "        else:\n",
    "            all_df = all_df.append(pd.DataFrame([[0] * len(new_columns)], columns=new_columns), ignore_index=True)\n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.09 ms ± 43.2 µs\n",
    "def word_bag_encode_apply(input_data, column, word_map, word_hist):\n",
    "    new_columns = [column + ' ' + s for s in word_map.keys()]\n",
    "    new_columns.append(column + ' ' + 'others')\n",
    "    delay_rate = 0.8 # must be less than 1\n",
    "    \n",
    "    def encode_routine(str_value):\n",
    "        series_dict = dict(zip(new_columns, [0.] * len(new_columns)))\n",
    "        if isinstance(str_value, str): \n",
    "            splited_list = re.split(r'[|/]+',str_value)\n",
    "            list_len = len(splited_list)\n",
    "            # the weight of each position of the array, are decayed by the ratio delay_rate, and their sum are 1\n",
    "            # so according to the geometric series summation formula, the iniatial weight are caculate as follow\n",
    "            initial_weight = (1-delay_rate)/(1 - np.power(delay_rate, list_len))\n",
    "            for index, s in enumerate(splited_list): \n",
    "                word_stripped = s.strip(' \\\"\\t\\s\\n')\n",
    "                series_dict[word_map.get(word_stripped, column + ' others')] += initial_weight / (word_hist.get(word_stripped, word_hist['others'])) #word_hist[word_stripped]\n",
    "                initial_weight *= delay_rate\n",
    "        return pd.Series(series_dict)\n",
    "    return input_data[column].apply(lambda s: encode_routine(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 171 µs ± 693 ns\n",
    "def word_bag_encode_numpy(input_data, column, word_map, word_hist):\n",
    "    new_columns = [s for s in word_map.keys()]\n",
    "    new_columns.append('others')\n",
    "    delay_rate = 0.8 # must be less than 1\n",
    "    num_columns = len(new_columns)\n",
    "    str_indice_dict = dict(zip(new_columns, list(range(num_columns))))\n",
    "    def encode_routine(str_value):\n",
    "        temp_hist = np.zeros(num_columns, dtype=float)\n",
    "        if isinstance(str_value, str): \n",
    "            splited_list = re.split(r'[|/]+',str_value)\n",
    "            list_len = len(splited_list)\n",
    "            # the weight of each position of the array, are decayed by the ratio delay_rate, and their sum are 1\n",
    "            # so according to the geometric series summation formula, the iniatial weight are caculate as follow\n",
    "            initial_weight = (1-delay_rate)/(1 - np.power(delay_rate, list_len))\n",
    "            for index, s in enumerate(splited_list): \n",
    "                word_stripped = s.strip(' \\\"\\t\\s\\n')\n",
    "                temp_hist[str_indice_dict.get(word_stripped, num_columns-1)] += initial_weight / (word_hist.get(word_stripped, word_hist['others'])) #word_hist[word_stripped]\n",
    "                initial_weight *= delay_rate\n",
    "        return temp_hist\n",
    "    # actually we cannot use vectorize #vf = np.vectorize(encode_routine)\n",
    "\n",
    "    #def fromiter(x):\n",
    "    #return np.fromiter((f(xi) for xi in x), x.dtype)\n",
    "\n",
    "    numpy_str = np.array(input_data[column].values, dtype=object)\n",
    "    #return np.array(map(encode_routine, numpy_str))\n",
    "    #return np.fromiter((encode_routine(xi) for xi in numpy_str), numpy_str.dtype, count=len(numpy_str))\n",
    "    return np.array([encode_routine(xi) for xi in numpy_str]), [column + ' ' + s for s in new_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example test\n",
    "#composer_map, composer_hist = create_bag_of_words(composer_df, 0.001, 'composer')\n",
    "#composer_array, head_name = word_bag_encode_numpy(train_data, 'composer', composer_map, composer_hist)\n",
    "#composer_encoder = pd.DataFrame(data = composer_array, columns = head_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#composer_map, composer_hist = create_bag_of_words(composer_df, 0.001, 'composer')\n",
    "#%timeit composer_encoder = word_bag_encode(train_data, 'composer', composer_map, composer_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 item are selected.\n"
     ]
    }
   ],
   "source": [
    "composer_map, composer_hist = create_bag_of_words(composer_df, 0.001, 'composer')\n",
    "#composer_encoder = word_bag_encode_apply(train_data, 'composer', composer_map, composer_hist)\n",
    "composer_array, composer_head_name = word_bag_encode_numpy(train_data, 'composer', composer_map, composer_hist)\n",
    "composer_encoder = pd.DataFrame(data = composer_array, columns = composer_head_name)\n",
    "del composer_array\n",
    "print('composer_encoder finished')\n",
    "artist_name_map, artist_name_hist = create_bag_of_words(artist_name_df, 0.001, 'artist_name')\n",
    "#artist_name_encoder = word_bag_encode_apply(train_data, 'artist_name', artist_name_map, artist_name_hist)\n",
    "artist_name_array, artist_name_head_name = word_bag_encode_numpy(train_data, 'artist_name', artist_name_map, artist_name_hist)\n",
    "artist_name_encoder = pd.DataFrame(data = artist_name_array, columns = artist_name_head_name)\n",
    "del artist_name_array\n",
    "print('artist_name_encoder finished')\n",
    "lyricist_map, lyricist_hist = create_bag_of_words(lyricist_df, 0.002, 'lyricist')\n",
    "#lyricist_encoder = word_bag_encode_apply(train_data, 'lyricist', lyricist_map, lyricist_hist)\n",
    "lyricist_array, lyricist_head_name = word_bag_encode_numpy(train_data, 'lyricist', lyricist_map, lyricist_hist)\n",
    "lyricist_encoder = pd.DataFrame(data = lyricist_array, columns = lyricist_head_name)\n",
    "del lyricist_array\n",
    "print('lyricist_encoder finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.drop('composer', axis=1, inplace=True)\n",
    "train_data.drop('artist_name', axis=1, inplace=True)\n",
    "train_data.drop('lyricist', axis=1, inplace=True)\n",
    "final_train_data = pd.concat([train_data, composer_encoder, artist_name_encoder, lyricist_encoder], join='inner', axis=1, copy=True)\n",
    "del train_data\n",
    "del composer_encoder\n",
    "del artist_name_encoder\n",
    "del lyricist_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#composer_encoder_test = word_bag_encode_apply(test_data, 'composer', composer_map, composer_hist)\n",
    "composer_array_test, composer_head_name_test = word_bag_encode_numpy(test_data, 'composer', composer_map, composer_hist)\n",
    "composer_encoder_test = pd.DataFrame(data = composer_array_test, columns = composer_head_name_test)\n",
    "del composer_array_test\n",
    "print('composer_encoder_test finished')\n",
    "#artist_name_encoder_test = word_bag_encode_apply(test_data, 'artist_name', artist_name_map, artist_name_hist)\n",
    "artist_name_array_test, artist_name_head_name_test = word_bag_encode_numpy(test_data, 'artist_name', artist_name_map, artist_name_hist)\n",
    "artist_name_encoder_test = pd.DataFrame(data = artist_name_array_test, columns = artist_name_head_name_test)\n",
    "del artist_name_array_test\n",
    "print('artist_name_encoder_test finished')\n",
    "#lyricist_encoder_test = word_bag_encode_apply(test_data, 'lyricist', lyricist_map, lyricist_hist)\n",
    "lyricist_array_test, lyricist_head_name_test = word_bag_encode_numpy(test_data, 'lyricist', lyricist_map, lyricist_hist)\n",
    "lyricist_encoder_test = pd.DataFrame(data = lyricist_array_test, columns = lyricist_head_name_test)\n",
    "del lyricist_array_test\n",
    "print('lyricist_encoder_test finished')\n",
    "test_data.drop('composer', axis=1, inplace=True)\n",
    "test_data.drop('artist_name', axis=1, inplace=True)\n",
    "test_data.drop('lyricist', axis=1, inplace=True)\n",
    "final_test_data = pd.concat([test_data, composer_encoder_test, artist_name_encoder_test, lyricist_encoder_test], join='inner', axis=1, copy=True)\n",
    "del test_data\n",
    "del composer_encoder_test\n",
    "del artist_name_encoder_test\n",
    "del lyricist_encoder_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_train_data.head())\n",
    "print(final_test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_train_data.to_csv(DATASET_PATH + 'all_train_featured.csv', compression='gzip')\n",
    "final_test_data.to_csv(DATASET_PATH + 'all_test_featured.csv', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
