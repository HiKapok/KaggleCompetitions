{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapok/pyenv35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/kapok/pyenv35/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "get_ipython().magic('env CUDA_VISIBLE_DEVICES =  ')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp      # will come in handy due to the size of the data\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import io\n",
    "from datetime import datetime\n",
    "import gc # garbage collector\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import logging\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a pandas dataframe to disk as gunzip compressed csv\n",
    "- df.to_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Read from disk\n",
    "- df = pd.read_csv('dfsavename.csv.gz', compression='gzip')\n",
    "\n",
    "## Magic useful\n",
    "- %%timeit for the whole cell\n",
    "- %timeit for the specific line\n",
    "- %%latex to render the cell as a block of latex\n",
    "- %prun and %%prun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/WSDM/'\n",
    "HDF_FILENAME = DATASET_PATH + 'datas.h5'\n",
    "SUBMISSION_FILENAME = DATASET_PATH + 'submission_{}.csv'\n",
    "VALIDATION_INDICE = DATASET_PATH + 'validation_indice.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_logging(logger_name, logger_file_name):\n",
    "    log = logging.getLogger(logger_name)\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create formatter and add it to the handlers\n",
    "    print_formatter = logging.Formatter('%(message)s')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s_%(levelname)s: %(message)s')\n",
    "\n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(logger_file_name, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(file_formatter)\n",
    "    log.addHandler(fh)\n",
    "    # both output to console and file\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(print_formatter)\n",
    "    log.addHandler(consoleHandler)\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "here is an info message.\n"
     ]
    }
   ],
   "source": [
    "log = set_logging('MUSIC', DATASET_PATH + 'music_gbm.log')\n",
    "log.info('here is an info message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN_FILE = DATASET_PATH + 'train.csv'\n",
    "# TEST_FILE = DATASET_PATH + 'test.csv'\n",
    "# MEMBER_FILE = DATASET_PATH + 'members.csv'\n",
    "# SONG_FILE = DATASET_PATH + 'fix_songs.csv'\n",
    "# SONG_EXTRA_FILE = DATASET_PATH + 'song_extra_info.csv'\n",
    "\n",
    "# train_data = pd.read_csv(TRAIN_FILE)\n",
    "# test_data = pd.read_csv(TEST_FILE)\n",
    "# member_data = pd.read_csv(MEMBER_FILE)\n",
    "# song_data = pd.read_csv(SONG_FILE)\n",
    "# song_extra_data = pd.read_csv(SONG_EXTRA_FILE)\n",
    "\n",
    "# songs_all = pd.merge(left = song_data, right = song_extra_data, how = 'left', on='song_id')\n",
    "# train_with_mem = pd.merge(left = train_data, right = member_data, how = 'left', on='msno')\n",
    "# train_all = pd.merge(left = train_with_mem, right = songs_all, how = 'left', on='song_id')\n",
    "# test_with_mem = pd.merge(left = test_data, right = member_data, how = 'left', on='msno')\n",
    "# test_all = pd.merge(left = test_with_mem, right = songs_all, how = 'left', on='song_id')\n",
    "# del train_with_mem, test_with_mem; gc.collect()\n",
    "\n",
    "# def convert_unicode_to_str(df):\n",
    "#     df.columns = df.columns.astype(str)\n",
    "#     types = df.apply(lambda x: pd.api.types.infer_dtype(df.values))\n",
    "#     #print(types)#mixed-integer\n",
    "#     for col in types[types == 'mixed-integer'].index:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     for col in types[types == 'mixed'].index:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     return df\n",
    "\n",
    "# store = pd.HDFStore(HDF_FILENAME)\n",
    "# store['train_data'] = convert_unicode_to_str(train_all)\n",
    "# store['test_data'] = convert_unicode_to_str(test_all)\n",
    "# store['song_data'] = convert_unicode_to_str(songs_all)\n",
    "# store['test_id'] = test_data.id\n",
    "# store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_test = pd.HDFStore(HDF_FILENAME)\n",
    "# train = store_test['train_data'][0:100]\n",
    "# test = store_test['test_data'][0:100]\n",
    "# test_id =  store_test['test_id'][0:100]\n",
    "# store_test.close()\n",
    "store_test = pd.HDFStore(HDF_FILENAME)\n",
    "#train = store_test['train_data']\n",
    "#test = store_test['test_data']\n",
    "test_id =  store_test['test_id']\n",
    "store_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_country(input_data):\n",
    "    def get_country(isrc):\n",
    "        if isinstance(isrc, str) and isrc != 'nan':\n",
    "            return isrc[0:2]\n",
    "        else:\n",
    "            return np.nan\n",
    "    countries = train['isrc'].apply(get_country)\n",
    "    country_list = list(countries.value_counts().index)\n",
    "    country_map = dict(zip(country_list, country_list))\n",
    "    country_map['QM'] = 'QZ'\n",
    "    country_map['US'] = 'QZ'\n",
    "    return countries.map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['country'] = split_country(train)\n",
    "test['country'] = split_country(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrc_to_year(isrc):\n",
    "    if isinstance(isrc, str) and isrc != 'nan':\n",
    "        if int(isrc[5:7]) > 17:\n",
    "            return 1900 + int(isrc[5:7])\n",
    "        else:\n",
    "            return 2000 + int(isrc[5:7])\n",
    "    else:\n",
    "        return np.nan\n",
    "        \n",
    "train['song_year'] = train['isrc'].apply(isrc_to_year)\n",
    "test['song_year'] = test['isrc'].apply(isrc_to_year)\n",
    "train.drop(['isrc'], axis = 1, inplace = True)\n",
    "test.drop(['isrc'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_reg_date(input_data):\n",
    "    input_data['registration_year'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[0:4]))\n",
    "    input_data['registration_year'] = pd.to_numeric(input_data['registration_year'], downcast='unsigned')\n",
    "\n",
    "    input_data['registration_month'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[4:6]))\n",
    "    input_data['registration_month'] = pd.to_numeric(input_data['registration_month'], downcast='unsigned')\n",
    "\n",
    "    input_data['registration_day'] = input_data['registration_init_time'].apply(lambda x : int(str(x)[6:8]))\n",
    "    input_data['registration_day'] = pd.to_numeric(input_data['registration_day'], downcast='unsigned')\n",
    "\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_expir_date(input_data):\n",
    "    input_data['expiration_year'] = input_data['expiration_date'].apply(lambda x : int(str(x)[0:4]))\n",
    "    input_data['expiration_year'] = pd.to_numeric(input_data['expiration_year'], downcast='unsigned')\n",
    "\n",
    "    input_data['expiration_month'] = input_data['expiration_date'].apply(lambda x : int(str(x)[4:6]))\n",
    "    input_data['expiration_month'] = pd.to_numeric(input_data['expiration_month'], downcast='unsigned')\n",
    "\n",
    "    input_data['expiration_day'] = input_data['expiration_date'].apply(lambda x : int(str(x)[6:8]))\n",
    "    input_data['expiration_day'] = pd.to_numeric(input_data['expiration_day'], downcast='unsigned')\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_to_day(input_data):\n",
    "    # 转换注册时间\n",
    "    input_data['registration_init_time'] = pd.to_datetime(input_data['registration_init_time'],format=\"%Y%m%d\")\n",
    "    input_data['expiration_date'] = pd.to_datetime(input_data['expiration_date'],format=\"%Y%m%d\")\n",
    "    days = input_data.expiration_date - input_data.registration_init_time\n",
    "    days = [d.days for d in days]\n",
    "    input_data['days']=days\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = split_reg_date(train)\n",
    "test = split_reg_date(test)\n",
    "train = split_expir_date(train)\n",
    "test = split_expir_date(test)\n",
    "\n",
    "train = date_to_day(train)\n",
    "test = date_to_day(test)\n",
    "\n",
    "train.drop('registration_init_time',axis=1,inplace=True)\n",
    "train.drop('expiration_date',axis=1,inplace=True)\n",
    "test.drop('registration_init_time',axis=1,inplace=True)\n",
    "test.drop('expiration_date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['song_length'] = pd.to_numeric(train['song_length'].replace('nan', '235415'), downcast='unsigned')\n",
    "test['song_length'] = pd.to_numeric(test['song_length'].replace('nan', '235415'), downcast='unsigned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in train.columns: print(col, ':', train[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in [col for col in test.columns if col != 'id' ]:\n",
    "    if train[col].dtype == object:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode registered_via, the less number of occurrences are merged into the top item which has the max number of occurrences\n",
    "# registered_via_hist = pd.concat([train['registered_via'], test['registered_via']], axis = 0).value_counts()\n",
    "# registered_via_map = dict(zip(registered_via_hist.index, [int(s) for s in registered_via_hist.index.values]))\n",
    "# registered_via_map[registered_via_hist.index[-1]] = int(str(registered_via_hist.index.values[0]))\n",
    "# train['registered_via'] = train['registered_via'].map(registered_via_map)\n",
    "# test['registered_via'] = test['registered_via'].map(registered_via_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode language, fill nan with most occurrences item\n",
    "# language_hist = pd.concat([train['language'], test['language']], axis = 0).value_counts()\n",
    "# language_map = dict(zip(language_hist.index, [int(float(s)) for s in language_hist.index.values if s != 'nan']))\n",
    "# language_map['nan'] = int(float(str(language_hist.index.values[0])))\n",
    "# train['language'] = train['language'].map(language_map)\n",
    "# test['language'] = test['language'].map(language_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # encode country, fill nan with most occurrences item\n",
    "# country_hist = pd.concat([train['country'], test['country']], axis = 0).value_counts()\n",
    "# merge_per = 0.25\n",
    "# country_map = dict(zip(country_hist.index, list(range(len(country_hist)))))\n",
    "# for key in list(country_hist[-int(len(country_hist)*merge_per):].index):\n",
    "#     country_map[key] = int(len(country_hist)*(1-merge_per)) + 1\n",
    "# train['country'] = train['country'].map(country_map)\n",
    "# test['country'] = test['country'].map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# msno : category ; uinque values: 30755\n",
    "# song_id : category ; uinque values: 359966\n",
    "# - source_system_tab : category ; uinque values: 10\n",
    "# - source_screen_name : category ; uinque values: 21\n",
    "# - source_type : category ; uinque values: 13\n",
    "# - target : object ; uinque values: 2\n",
    "# - city : category ; uinque values: 21\n",
    "# - bd : category ; uinque values: 92\n",
    "# - gender : category ; uinque values: 3\n",
    "# - registered_via : category ; uinque values: 5\n",
    "# song_length : uint32 ; uinque values: 60271\n",
    "# genre_ids : category ; uinque values: 573\n",
    "# artist_name : category ; uinque values: 40587\n",
    "# composer : category ; uinque values: 76072\n",
    "# lyricist : category ; uinque values: 33895\n",
    "# - language : category ; uinque values: 11\n",
    "# name : category ; uinque values: 234144\n",
    "# - country : category ; uinque values: 107\n",
    "# - song_year : float64 ; uinque values: 100\n",
    "# - registration_year : uint16 ; uinque values: 14\n",
    "# - registration_month : uint8 ; uinque values: 12\n",
    "# - registration_date : uint8 ; uinque values: 31\n",
    "# - expiration_year : uint16 ; uinque values: 18\n",
    "# - expiration_month : uint8 ; uinque values: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_transform(input_train_data, input_test_data, columns_to_transform):\n",
    "    for col in columns_to_transform:\n",
    "        le = LabelEncoder()\n",
    "        train_values = list(input_train_data[col].unique())\n",
    "        test_values = list(input_test_data[col].unique())\n",
    "        le.fit(train_values + test_values)\n",
    "        input_train_data[col] = le.transform(input_train_data[col])\n",
    "        input_test_data[col] = le.transform(input_test_data[col])\n",
    "    return input_train_data, input_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train, test = one_hot_transform(train, test, ['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender', 'name'])#, 'artist_name', 'composer', 'lyricist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: wether song_id should be merged like this or not? 231475 reserved and 188364 merged\n",
    "def encode_with_merge(input_train, input_test, columns, merge_value):\n",
    "    for index, col in enumerate(columns):\n",
    "        values_hist = pd.concat([input_train[col], input_train[col]], axis = 0).value_counts()\n",
    "        reserve_rows = values_hist[values_hist!=merge_value[index]]\n",
    "        merge_rows = values_hist[values_hist==merge_value[index]]\n",
    "\n",
    "        reserve_dict = dict(zip(list(reserve_rows.index), list(range(len(reserve_rows)))))\n",
    "        merge_dict = dict(zip(list(merge_rows.index), [len(reserve_rows)+1]*len(merge_rows.index)))\n",
    "        \n",
    "        map_dict = {**reserve_dict, **merge_dict}\n",
    "        \n",
    "        language_map['nan'] = int(float(str(language_hist.index.values[0])))\n",
    "        input_train[col] = input_train[col].map(map_dict)\n",
    "        input_test[col] = input_test[col].map(map_dict)\n",
    "    return input_train, input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train, test = encode_with_merge(train, test, ['msno', 'song_id', 'genre_ids'], [1, 1, 1])\n",
    "# print(train.head())\n",
    "# print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_org, test_org = train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = train_org.copy(deep=True)\n",
    "#test = test_org.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store_test = pd.HDFStore(VALIDATION_INDICE)\n",
    "validation_list = store_test['keep_index']['index'].values\n",
    "store_test.close()\n",
    "train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "#validation_use = train.iloc[validation_list].copy(deep=True).reset_index(drop=True)\n",
    "validation_use = train.iloc[list(range(7277417, 7377417))].copy(deep=True).reset_index(drop=True)\n",
    "#train_use = train.drop(validation_list)\n",
    "train_use = train.drop(list(range(7277417, 7377417)))\n",
    "# train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "# validation_use = train[50:].copy(deep=True).reset_index(drop=True)\n",
    "# train_use = train.drop(list(range(50,100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(train_data, validation_data, test_data):\n",
    "    train_data['song_length'] = np.log(pd.to_numeric(train_data['song_length'], downcast='float') + 1)\n",
    "    validation_data['song_length'] = np.log(pd.to_numeric(validation_data['song_length'], downcast='float') + 1)\n",
    "    test_data['song_length'] = np.log(pd.to_numeric(test_data['song_length'], downcast='float') + 1)\n",
    "    return train_data, validation_data, test_data\n",
    "train_use, validation_use, test = log_transform(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_composer_hot_rate(train_data, val_data, test_data):\n",
    "    \n",
    "    temp_data = pd.concat([train_data[['composer']], val_data[['composer']], test_data[['composer']]], axis=0, join=\"outer\")\n",
    "    temp_data['composer'] = temp_data['composer'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['composer'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['composer_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    temp_data = pd.concat([df_temp['composer_0'], df_temp['composer_1'], df_temp['composer_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "\n",
    "    composer_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    composer_hot['nan'] = 0.\n",
    "    composer_hot['nan'] = composer_hot.mean()\n",
    "    #print(composer_hot)\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['composer'] = input_data['composer'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['composer'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['composer_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['composer_0', 'composer_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_0')\n",
    "        hot_hist.columns = ['composer_1', 'composer_1_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_1')\n",
    "        hot_hist.columns = ['composer_2', 'composer_2_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_2')\n",
    "        df_temp['composer_score'] = df_temp[['composer_0_score','composer_1_score','composer_2_score']].max(axis=1)\n",
    "        #df_temp['composer_score'] = df_temp['composer_0_score']\n",
    "        \n",
    "        input_data['composer_score'] = df_temp['composer_score']\n",
    "        input_data.drop('composer', inplace=True, axis = 1)\n",
    "        #input_data = input_data.drop('composer', inplace=False, axis = 1)\n",
    "        input_data['composer'] = df_temp['composer_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, composer_hot)\n",
    "    val_data = encoder_each(val_data, composer_hot)\n",
    "    test_data = encoder_each(test_data, composer_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_composer_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_lyricist_hot_rate(train_data, val_data, test_data):\n",
    "    temp_data = pd.concat([train_data[['lyricist']], val_data[['lyricist']], test_data[['lyricist']]], axis=0, join=\"outer\")\n",
    "    temp_data['lyricist'] = temp_data['lyricist'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['lyricist'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['lyricist_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    #temp_data = df_temp['lyricist_0']\n",
    "    temp_data = pd.concat([df_temp['lyricist_0'], df_temp['lyricist_1'], df_temp['lyricist_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "    lyricist_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    lyricist_hot['nan'] = 0.\n",
    "    lyricist_hot['nan'] = lyricist_hot.mean()\n",
    "\n",
    "    #print(lyricist_hot)\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['lyricist'] = input_data['lyricist'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['lyricist'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['lyricist_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['lyricist_0', 'lyricist_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='lyricist_0')\n",
    "        \n",
    "        df_temp['lyricist_score'] = df_temp['lyricist_0_score']\n",
    "        \n",
    "        input_data['lyricist_score'] = df_temp['lyricist_score']\n",
    "        input_data.drop('lyricist', inplace=True, axis = 1)\n",
    "        input_data['lyricist'] = df_temp['lyricist_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, lyricist_hot)\n",
    "    val_data = encoder_each(val_data, lyricist_hot)\n",
    "    test_data = encoder_each(test_data, lyricist_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_lyricist_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_artist_hot_rate(train_data, val_data, test_data):\n",
    "    temp_data = pd.concat([train_data[['artist_name']], val_data[['artist_name']], test_data[['artist_name']]], axis=0, join=\"outer\")\n",
    "    temp_data['artist_name'] = temp_data['artist_name'].apply(lambda x : x.replace(u'、','|'))\n",
    "\n",
    "    df_temp = temp_data['artist_name'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "    df_temp.columns = ['artist_name_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    #temp_data = df_temp['artist_name_0']\n",
    "    temp_data = pd.concat([df_temp['artist_name_0'], df_temp['artist_name_1'], df_temp['artist_name_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "    artist_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    artist_hot['nan'] = 0.\n",
    "    artist_hot['nan'] = artist_hot.mean()\n",
    "    #print(artist_hot)\n",
    "\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        input_data['artist_name'] = input_data['artist_name'].apply(lambda x : x.replace(u'、','|'))\n",
    "        df_temp = input_data['artist_name'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['artist_name_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['artist_name_0', 'artist_name_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='artist_name_0')\n",
    "        \n",
    "        df_temp['artist_name_score'] = df_temp['artist_name_0_score']\n",
    "        \n",
    "        input_data['artist_name_score'] = df_temp['artist_name_score']\n",
    "        input_data.drop('artist_name', inplace=True, axis = 1)\n",
    "        input_data['artist_name'] = df_temp['artist_name_0']\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, artist_hot)\n",
    "    val_data = encoder_each(val_data, artist_hot)\n",
    "    test_data = encoder_each(test_data, artist_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_artist_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_use.head().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_data = pd.concat([train_use[['composer']], validation_use[['composer']], test[['composer']]], axis=0, join=\"inner\")\n",
    "\n",
    "# temp_data['composer'].apply(lambda x : len(x.replace(u'、','|').split('|'))).value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train_use.head(100000).copy(deep=True)\n",
    "# print(df['lyricist'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#time_wnd = [2018, 0]\n",
    "time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "def cal_song_listen_times(train_data, val_data, test_data):\n",
    "    all_data = pd.concat([train_data[['song_id', 'song_year', 'msno']], val_data[['song_id', 'song_year', 'msno']], test_data[['song_id', 'song_year', 'msno']]], axis=0, join=\"inner\")\n",
    "    #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "    #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "    for index, _ in enumerate(time_wnd[:-1]):\n",
    "        begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "#         begin_time = time_wnd[index]\n",
    "#         end_time = time_wnd[index+1]\n",
    "        select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "        #select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "        grouped = select_data[['song_id', 'msno']].groupby(['song_id'])\n",
    "\n",
    "        count_song = grouped.agg(['count'])\n",
    "        num_people_per_song = grouped.agg({\"msno\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "        popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "        popularity.columns = ['popular_{}'.format(index), 'num_people_{}'.format(index)]\n",
    "        popularity = popularity.reset_index(drop=False)\n",
    "        train_data = train_data.merge(popularity, on='song_id', how ='left')\n",
    "        test_data = test_data.merge(popularity, on='song_id', how ='left')\n",
    "        val_data = val_data.merge(popularity, on='song_id', how ='left')\n",
    "    return train_data, val_data, test_data\n",
    "def cal_song_listen_times_seperate(train_data, val_data, test_data):\n",
    "    def cal_each_of_them(input_data):\n",
    "        all_data = input_data[['song_id', 'song_year', 'msno']]\n",
    "        #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "        #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "        for index, _ in enumerate(time_wnd[:-1]):\n",
    "            begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "    #         begin_time = time_wnd[index]\n",
    "    #         end_time = time_wnd[index+1]\n",
    "            select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "            grouped = select_data[['song_id', 'msno']].groupby(['song_id'])\n",
    "\n",
    "            count_song = grouped.agg(['count'])\n",
    "            num_people_per_song = grouped.agg({\"msno\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "            popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "            popularity.columns = ['popular_{}'.format(index), 'num_people_{}'.format(index)]\n",
    "            popularity = popularity.reset_index(drop=False)\n",
    "            all_data = input_data.merge(popularity, on='song_id', how ='left')\n",
    "        return all_data\n",
    "    return cal_each_of_them(train_data), cal_each_of_them(val_data), cal_each_of_them(test_data)\n",
    "\n",
    "# time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "# def cal_song_listen_times(train_data, test_data):\n",
    "#     test_data['song_id'] = pd.to_numeric(test_data['song_id'], downcast='unsigned')\n",
    "#     for index, _ in enumerate(time_wnd[:-1]):\n",
    "#         begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "# #         begin_time = time_wnd[index]\n",
    "# #         end_time = time_wnd[index+1]\n",
    "#         select_data = train_data[train_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "#         select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "#         grouped = select_data[['song_id', 'target']].groupby(['song_id'])\n",
    "\n",
    "#         count_song = grouped.agg(['count'])\n",
    "#         mean_repeat_song = grouped['target'].mean()\n",
    "\n",
    "#         popularity = pd.concat([np.log(count_song+1), mean_repeat_song, np.log(count_song.multiply(mean_repeat_song, axis=0)+1)], axis=1, join=\"inner\")\n",
    "#         popularity.columns = ['popular_{}'.format(index), 'mean_repeat_{}'.format(index), 'replay_prob_{}'.format(index)]\n",
    "#         popularity = popularity.reset_index(drop=False)\n",
    "#         test_data = test_data.merge(popularity, on='song_id', how ='left')\n",
    "#         train_data = train_data.merge(popularity, on='song_id', how ='left')\n",
    "#     return train_data, test_data\n",
    "\n",
    "\n",
    "# time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "# def cal_song_listen_times(train_data):\n",
    "#     train_data['song_id'] = pd.to_numeric(train_data['song_id'], downcast='unsigned')\n",
    "#     for index, _ in enumerate(time_wnd[:-1]):\n",
    "#         begin_time, end_time = time_wnd[index] < time_wnd[index+1] and (time_wnd[index], time_wnd[index+1]) or (time_wnd[index+1], time_wnd[index])\n",
    "# #         begin_time = time_wnd[index]\n",
    "# #         end_time = time_wnd[index+1]\n",
    "#         select_data = train_data[train_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "#         select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "#         grouped = select_data[['song_id', 'target']].groupby(['song_id'])\n",
    "\n",
    "#         count_song = grouped.agg(['count'])\n",
    "#         mean_repeat_song = grouped['target'].mean()\n",
    "\n",
    "#         popularity = pd.concat([np.log(count_song+1), mean_repeat_song, np.log(count_song.multiply(mean_repeat_song, axis=0)+1)], axis=1, join=\"inner\")\n",
    "#         popularity.columns = ['popular_{}'.format(index), 'mean_repeat_{}'.format(index), 'replay_prob_{}'.format(index)]\n",
    "#         popularity = popularity.reset_index(drop=False)\n",
    "#         train_data = train_data.merge(popularity, on='song_id', how ='left')\n",
    "#     return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_song_listen_times(train_use, validation_use, test)\n",
    "# train = cal_song_listen_times(train)\n",
    "# test = cal_song_listen_times(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#people_time_wnd = [2018, 0]\n",
    "people_time_wnd = [2018, 0, 2000, 2010, 2014, 2018]\n",
    "def get_people_active(train_data, val_data, test_data):\n",
    "    all_data = pd.concat([train_data[['song_id', 'song_year', 'msno']], val_data[['song_id', 'song_year', 'msno']], test_data[['song_id', 'song_year', 'msno']]], axis=0, join=\"inner\")\n",
    "    #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "    #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "    for index, _ in enumerate(people_time_wnd[:-1]):\n",
    "        begin_time, end_time = people_time_wnd[index] < people_time_wnd[index+1] and (people_time_wnd[index], people_time_wnd[index+1]) or (people_time_wnd[index+1], people_time_wnd[index])\n",
    "#         begin_time = time_wnd[index]\n",
    "#         end_time = time_wnd[index+1]\n",
    "        select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "        #select_data['target'] = pd.to_numeric(select_data['target'], downcast='signed')\n",
    "        \n",
    "        grouped = select_data[['song_id', 'msno']].groupby(['msno'])\n",
    "\n",
    "        count_song = grouped.agg(['count'])\n",
    "        num_people_per_song = grouped.agg({\"song_id\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "        popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "        popularity.columns = ['active_{}'.format(index), 'num_song_{}'.format(index)]\n",
    "        popularity = popularity.reset_index(drop=False)\n",
    "        train_data = train_data.merge(popularity, on='msno', how ='left')\n",
    "        test_data = test_data.merge(popularity, on='msno', how ='left')\n",
    "        val_data = val_data.merge(popularity, on='msno', how ='left')\n",
    "    return train_data, val_data, test_data\n",
    "def get_people_active_seperate(train_data, val_data, test_data):\n",
    "    def cal_each_of_them(input_data):\n",
    "        all_data = input_data[['song_id', 'song_year', 'msno']]\n",
    "        #all_data['song_id'] = pd.to_numeric(all_data['song_id'], downcast='unsigned')\n",
    "        #all_data['msno'] = pd.to_numeric(all_data['msno'], downcast='unsigned')\n",
    "        for index, _ in enumerate(people_time_wnd[:-1]):\n",
    "            begin_time, end_time = people_time_wnd[index] < people_time_wnd[index+1] and (people_time_wnd[index], people_time_wnd[index+1]) or (people_time_wnd[index+1], people_time_wnd[index])\n",
    "    #         begin_time = time_wnd[index]\n",
    "    #         end_time = time_wnd[index+1]\n",
    "            select_data = all_data[all_data['song_year'].map(lambda x: x>=begin_time and x < end_time)]\n",
    "        \n",
    "            grouped = select_data[['song_id', 'msno']].groupby(['msno'])\n",
    "\n",
    "            count_song = grouped.agg(['count'])\n",
    "            num_people_per_song = grouped.agg({\"song_id\": lambda x: np.log(x.nunique()+1)})\n",
    "\n",
    "            popularity = pd.concat([np.log(count_song+1), num_people_per_song], axis=1, join=\"inner\")\n",
    "            popularity.columns = ['active_{}'.format(index), 'num_song_{}'.format(index)]\n",
    "            popularity = popularity.reset_index(drop=False)\n",
    "            all_data = input_data.merge(popularity, on='msno', how ='left')\n",
    "        return all_data\n",
    "    return cal_each_of_them(train_data), cal_each_of_them(val_data), cal_each_of_them(test_data)\n",
    "\n",
    "# test = test.reset_index(drop=False)\n",
    "# #test['msno'] = test['msno'].astype(int)   \n",
    "# train['target'] = pd.to_numeric(train['target'], downcast='signed')\n",
    "\n",
    "# grouped = train[['msno', 'target']].groupby(['msno'])\n",
    "\n",
    "# count_msno = grouped.agg(['count'])\n",
    "# mean_repeat_msno = grouped['target'].mean()\n",
    "\n",
    "# popularity = pd.concat([np.log(count_msno+1), mean_repeat_msno, np.log(count_msno.multiply(mean_repeat_msno, axis=0)+1)], axis=1, join=\"inner\")\n",
    "# popularity.columns = ['ms_popular', 'ms_mean_repeat', 'ms_replay_prob']\n",
    "# popularity = popularity.reset_index(drop=False)\n",
    "# test = test.merge(popularity, on='msno', how ='left')\n",
    "# train = train.merge(popularity, on='msno', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = get_people_active(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_by_different_city_lang_country(train_data, val_data, test_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']], val_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']], test_data[['composer', 'lyricist', 'artist_name', 'city', 'country', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "    count_dict = dict()\n",
    "    \n",
    "    for col in ['composer', 'lyricist', 'artist_name']:\n",
    "        temp_df = None\n",
    "        for target in ['city', 'country', 'language']:\n",
    "            grouped = temp_msno_songid.groupby([col])\n",
    "            df = grouped.agg({target: lambda x: x.nunique()})\n",
    "            df = np.log(df+1)\n",
    "#             print(df)\n",
    "#             break\n",
    "            if temp_df is not None:\n",
    "                temp_df = pd.concat([temp_df, df[target]], axis=1, join=\"inner\")\n",
    "            else:\n",
    "                temp_df = df\n",
    "        temp_df = temp_df.reset_index()\n",
    "        temp_df.index.name='index'\n",
    "        \n",
    "        temp_df.columns = [col, *[col + '_by_' + col_name for col_name in ['city', 'country', 'language']]]\n",
    "        #print(temp_df)    \n",
    "        train_data = train_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        test_data = test_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        val_data = val_data.merge(right = temp_df, how = 'left', on=col)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_use_org, test_org, validation_use_org = measure_by_different_city_lang_country(train_use, test, validation_use)\n",
    "train_use, validation_use, test = measure_by_different_city_lang_country(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_use, test, validation_use = train_use_org.copy(deep=True), test_org.copy(deep=True), validation_use_org.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def song_msno_by_different_city_lang_country(train_data, val_data, test_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['song_id', 'msno', 'city', 'country', 'language']], val_data[['song_id', 'msno', 'city', 'country', 'language']], test_data[['song_id', 'msno', 'city', 'country', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "    count_dict = dict()\n",
    "    \n",
    "    for col in ['song_id', 'msno']:\n",
    "        temp_df = None\n",
    "        if col == 'song_id':\n",
    "            target_list = ['city']\n",
    "        else:\n",
    "            target_list = ['country', 'language']\n",
    "        for target in target_list:\n",
    "            grouped = temp_msno_songid.groupby([col])\n",
    "            df = grouped.agg({target: lambda x: x.nunique()})\n",
    "            df = np.log(df+1)\n",
    "            #print(df)\n",
    "#             break\n",
    "            if temp_df is not None:\n",
    "                temp_df = pd.concat([temp_df, df[target]], axis=1, join=\"inner\")\n",
    "            else:\n",
    "                temp_df = df\n",
    "        temp_df = temp_df.reset_index()\n",
    "        temp_df.index.name='index'\n",
    "        \n",
    "        temp_df.columns = [col, *[col + '_by_' + col_name for col_name in target_list]]\n",
    "        #print(temp_df)    \n",
    "        train_data = train_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        test_data = test_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        val_data = val_data.merge(right = temp_df, how = 'left', on=col)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = song_msno_by_different_city_lang_country(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_use['genre_ids'].apply(lambda x : len(x.split('|'))).value_counts().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_genre_hot_rate(train_data, val_data, test_data):\n",
    "    # 0.685742\n",
    "#     temp_data = pd.concat([train_data[['genre_ids']], val_data[['genre_ids']], test_data[['genre_ids']]], axis=0, join=\"outer\")\n",
    "#     temp_data.reset_index(drop=True)\n",
    "\n",
    "#     genre_hot = np.log(temp_data['genre_ids'].value_counts()+1)\n",
    "#     #composer_hot = temp_data.value_counts()\n",
    "#     genre_hot['nan'] = 0.\n",
    "#     genre_hot['nan'] = genre_hot.mean()\n",
    "#     #print(composer_hot)\n",
    "#     genre_hot = genre_hot.reset_index()\n",
    "#     genre_hot.index.name='index'\n",
    "    \n",
    "#     genre_hot.columns = ['genre_ids', 'genre_ids_score']\n",
    "#     genre_hot['num_genre'] = genre_hot['genre_ids'].apply(lambda x : len(x.split('|')))\n",
    "#     train_data = train_data.merge(right = genre_hot, how = 'left', on='genre_ids')\n",
    "#     test_data = test_data.merge(right = genre_hot, how = 'left', on='genre_ids')\n",
    "#     val_data = val_data.merge(right = genre_hot, how = 'left', on='genre_ids')\n",
    "#     return train_data, test_data, val_data\n",
    "#     def encoder_each(input_data, hot_hist):\n",
    "#         input_data = input_data.copy()\n",
    "#         input_data['composer'] = input_data['genre_ids'].apply(lambda x : x.replace(u'、','|'))\n",
    "#         df_temp = input_data['composer'].str.split('\\s{0,}[\\|\\\\\\\\/]\\s{0,}', 3, expand=True)\n",
    "#         df_temp.columns = ['composer_{}'.format(x) for x in df_temp.columns]\n",
    "#         hot_hist = hot_hist.reset_index()\n",
    "#         hot_hist.index.name='index'\n",
    "        \n",
    "#         hot_hist.columns = ['composer_0', 'composer_0_score']\n",
    "#         df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_0')\n",
    "#         hot_hist.columns = ['composer_1', 'composer_1_score']\n",
    "#         df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_1')\n",
    "#         hot_hist.columns = ['composer_2', 'composer_2_score']\n",
    "#         df_temp = df_temp.merge(right = hot_hist, how = 'left', on='composer_2')\n",
    "#         df_temp['composer_score'] = df_temp[['composer_0_score','composer_1_score','composer_2_score']].max(axis=1)\n",
    "#         #df_temp['composer_score'] = df_temp['composer_0_score']\n",
    "        \n",
    "#         input_data['composer_score'] = df_temp['composer_score']\n",
    "#         input_data.drop('genre_id', inplace=True, axis = 1)\n",
    "#         #input_data = input_data.drop('composer', inplace=False, axis = 1)\n",
    "#         input_data['genre_id'] = df_temp['composer_0']\n",
    "#         return input_data\n",
    "#     train_data = encoder_each(train_data, genre_hot)\n",
    "#     val_data = encoder_each(val_data, genre_hot)\n",
    "#     val_data = encoder_each(test_data, genre_hot)\n",
    "    # 0.684454 with bd filled\n",
    "    temp_data = pd.concat([train_data[['genre_ids']], val_data[['genre_ids']], test_data[['genre_ids']]], axis=0, join=\"outer\")\n",
    "\n",
    "    df_temp = temp_data['genre_ids'].str.split('\\s{0,}\\|\\s{0,}', 3, expand=True)\n",
    "    \n",
    "    df_temp.columns = ['genre_ids_{}'.format(x) for x in df_temp.columns]\n",
    "   \n",
    "    temp_data = pd.concat([df_temp['genre_ids_0'], df_temp['genre_ids_1'], df_temp['genre_ids_2']], axis=0, join=\"outer\")\n",
    "    temp_data.reset_index(drop=True)\n",
    "\n",
    "    genre_hot = np.log(temp_data.value_counts()+1)\n",
    "    #composer_hot = temp_data.value_counts()\n",
    "    genre_hot['nan'] = 0.\n",
    "    genre_hot['nan'] = genre_hot.mean()\n",
    "    #print(composer_hot)\n",
    "    def encoder_each(input_data, hot_hist):\n",
    "        input_data = input_data.copy()\n",
    "        df_temp = input_data['genre_ids'].str.split('\\s{0,}\\|\\s{0,}', 3, expand=True)\n",
    "        df_temp.columns = ['genre_ids_{}'.format(x) for x in df_temp.columns]\n",
    "        hot_hist = hot_hist.reset_index()\n",
    "        hot_hist.index.name='index'\n",
    "        \n",
    "        hot_hist.columns = ['genre_ids_0', 'genre_ids_0_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='genre_ids_0')\n",
    "        hot_hist.columns = ['genre_ids_1', 'genre_ids_1_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='genre_ids_1')\n",
    "        hot_hist.columns = ['genre_ids_2', 'genre_ids_2_score']\n",
    "        df_temp = df_temp.merge(right = hot_hist, how = 'left', on='genre_ids_2')\n",
    "        df_temp['genre_ids_score'] = df_temp[['genre_ids_0_score','genre_ids_1_score','genre_ids_2_score']].max(axis=1)\n",
    "        idx_max = df_temp[['genre_ids_0_score','genre_ids_1_score','genre_ids_2_score']].idxmax(axis=1)\n",
    "        idx_max = idx_max.apply(lambda x : x.strip('_score'))\n",
    "        #print(idx_max)\n",
    "        #print(df_temp[['genre_ids_0','genre_ids_1','genre_ids_2']])\n",
    "        #print(df_temp[['genre_ids_0','genre_ids_1','genre_ids_2']].lookup(idx_max.index, idx_max.values))\n",
    "        \n",
    "        #return input_data\n",
    "        #df_temp['genre_ids_score'] = df_temp['genre_ids_0_score']\n",
    "        \n",
    "        input_data['genre_ids_score'] = df_temp['genre_ids_score']\n",
    "        #input_data.drop('genre_ids', inplace=True, axis = 1)\n",
    "        #input_data = input_data.drop('genre_ids', inplace=False, axis = 1)\n",
    "        input_data['genre_ids'] = df_temp[['genre_ids_0','genre_ids_1','genre_ids_2']].lookup(idx_max.index, idx_max.values)\n",
    "        return input_data\n",
    "    train_data = encoder_each(train_data, genre_hot)\n",
    "    val_data = encoder_each(val_data, genre_hot)\n",
    "    test_data = encoder_each(test_data, genre_hot)\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_use, validation_use, test = cal_genre_hot_rate(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use.to_csv(DATASET_PATH + 'temp_train.csv', index = False)\n",
    "validation_use.to_csv(DATASET_PATH + 'temp_validation.csv', index = False)\n",
    "test.to_csv(DATASET_PATH + 'temp_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_use = pd.read_csv(DATASET_PATH + 'temp_train.csv')\n",
    "validation_use = pd.read_csv(DATASET_PATH + 'temp_validation.csv')\n",
    "test = pd.read_csv(DATASET_PATH + 'temp_test.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "    if col not in ['song_length']:\n",
    "        if test[col].dtype == np.float64:\n",
    "            train_use[col] = train_use[col].astype(np.float32)\n",
    "            validation_use[col] = validation_use[col].astype(np.float32)\n",
    "            test[col] = test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_data_of_birth(train_data, val_data, test_data):\n",
    "    temp_data = pd.concat([train_data[['bd']], val_data[['bd']], test_data[['bd']]], axis=0, join=\"outer\")\n",
    "    mean_bd = temp_data['bd'].mean(axis=0)\n",
    "    #mean_bd = temp_data['bd'].replace(0, np.nan).mean(axis=0)\n",
    "    train_data = train_data.fillna(value={'bd': mean_bd})\n",
    "    val_data = val_data.fillna(value={'bd': mean_bd})\n",
    "    test_data = test_data.fillna(value={'bd': mean_bd})\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use, validation_use, test = fill_data_of_birth(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_city_lang_country_by_others(train_data, val_data, test_data):\n",
    "#     temp_msno_songid = pd.concat([train_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'country', 'language']], val_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'country', 'language']], test_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'country', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "#     count_dict = dict()\n",
    "    \n",
    "#     for col in ['city', 'country', 'language']:\n",
    "#         temp_df = None\n",
    "#         for target in ['song_id', 'msno', 'genre_ids', 'artist_name']:\n",
    "#             grouped = temp_msno_songid.groupby([col])\n",
    "#             df = grouped.agg({target: lambda x: x.nunique()})\n",
    "#             df = np.log(df+1)\n",
    "# #             print(df)\n",
    "# #             break\n",
    "#             if temp_df is not None:\n",
    "#                 temp_df = pd.concat([temp_df, df[target]], axis=1, join=\"inner\")\n",
    "#             else:\n",
    "#                 temp_df = df\n",
    "#         temp_df = temp_df.reset_index()\n",
    "#         temp_df.index.name='index'\n",
    "        \n",
    "#         temp_df.columns = [col, *[col + '_by_' + col_name for col_name in ['song_id', 'msno', 'genre_ids', 'artist_name']]]\n",
    "#         #print(temp_df)    \n",
    "#         train_data = train_data.merge(right = temp_df, how = 'left', on=col)\n",
    "#         test_data = test_data.merge(right = temp_df, how = 'left', on=col)\n",
    "#         val_data = val_data.merge(right = temp_df, how = 'left', on=col)\n",
    "#     return train_data, val_data, test_data\n",
    "    temp_msno_songid = pd.concat([train_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'language']], val_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'language']], test_data[['song_id', 'msno', 'genre_ids', 'artist_name', 'city', 'language']]], axis=0, join=\"outer\")\n",
    "    \n",
    "    count_dict = dict()\n",
    "    \n",
    "    for col in ['city', 'language']:\n",
    "        temp_df = None\n",
    "        for target in ['song_id', 'msno', 'genre_ids', 'artist_name']:\n",
    "            grouped = temp_msno_songid.groupby([col])\n",
    "            df = grouped.agg({target: lambda x: x.nunique()})\n",
    "            df = np.log(df+1)\n",
    "#             print(df)\n",
    "#             break\n",
    "            if temp_df is not None:\n",
    "                temp_df = pd.concat([temp_df, df[target]], axis=1, join=\"inner\")\n",
    "            else:\n",
    "                temp_df = df\n",
    "        temp_df = temp_df.reset_index()\n",
    "        temp_df.index.name='index'\n",
    "        \n",
    "        temp_df.columns = [col, *[col + '_by_' + col_name for col_name in ['song_id', 'msno', 'genre_ids', 'artist_name']]]\n",
    "        #print(temp_df)    \n",
    "        train_data = train_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        test_data = test_data.merge(right = temp_df, how = 'left', on=col)\n",
    "        val_data = val_data.merge(right = temp_df, how = 'left', on=col)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_use, validation_use, test = measure_city_lang_country_by_others(train_use, validation_use, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wheteher is the most genre of a artist or song or msno\n",
    "def binary_encode_genre_ids(train_data, test_data, val_data):\n",
    "    temp_msno_songid = pd.concat([train_data[['song_id', 'msno', 'artist_name', 'genre_ids']], val_data[['song_id', 'msno', 'artist_name', 'genre_ids']], test_data[['song_id', 'msno', 'artist_name', 'genre_ids']]], axis=0, join=\"outer\")\n",
    "    \n",
    "    mode = lambda x: x.mode() if len(x) > 2 else np.array(x)\n",
    "    grouped = temp_msno_songid.groupby('song_id')['genre_ids'].agg(mode)\n",
    "    print(grouped)\n",
    "    return \n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_use.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def encode_binary_feature(train_data, test_data, val_data):\n",
    "#     mode = lambda x: x.mode() if len(x) > 2 else np.array(x)\n",
    "# >>> df.groupby('tag')['category'].agg(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender',\\\n",
    "            'name', 'artist_name', 'composer', 'lyricist', 'msno', 'song_id', 'genre_ids',\\\n",
    "           'country', 'language', 'registered_via']:\n",
    "    train_use[col] = train_use[col].astype('category')\n",
    "    validation_use[col] = validation_use[col].astype('category')\n",
    "    test[col] = test[col].astype('category')\n",
    "for col in test.columns:\n",
    "    if col not in ['song_length']:\n",
    "        if test[col].dtype == np.float64:\n",
    "            train_use[col] = train_use[col].astype(np.float32)\n",
    "            validation_use[col] = validation_use[col].astype(np.float32)\n",
    "            test[col] = test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno : category ; uinque values: 30571\n",
      "song_id : category ; uinque values: 357496\n",
      "source_system_tab : category ; uinque values: 9\n",
      "source_screen_name : category ; uinque values: 20\n",
      "source_type : category ; uinque values: 12\n",
      "target : int64 ; uinque values: 2\n",
      "city : category ; uinque values: 21\n",
      "bd : int64 ; uinque values: 91\n",
      "gender : category ; uinque values: 2\n",
      "registered_via : category ; uinque values: 5\n",
      "song_length : float64 ; uinque values: 60028\n",
      "genre_ids : category ; uinque values: 153\n",
      "language : category ; uinque values: 10\n",
      "name : category ; uinque values: 232729\n",
      "country : category ; uinque values: 107\n",
      "song_year : float32 ; uinque values: 100\n",
      "registration_year : int64 ; uinque values: 14\n",
      "registration_month : int64 ; uinque values: 12\n",
      "registration_day : int64 ; uinque values: 31\n",
      "expiration_year : int64 ; uinque values: 18\n",
      "expiration_month : int64 ; uinque values: 12\n",
      "expiration_day : int64 ; uinque values: 31\n",
      "days : int64 ; uinque values: 4319\n",
      "composer_score : float32 ; uinque values: 2218\n",
      "composer : category ; uinque values: 49086\n",
      "lyricist_score : float32 ; uinque values: 1652\n",
      "lyricist : category ; uinque values: 24205\n",
      "artist_name_score : float32 ; uinque values: 1842\n",
      "artist_name : category ; uinque values: 39234\n",
      "popular_0 : float32 ; uinque values: 2041\n",
      "num_people_0 : float32 ; uinque values: 2003\n",
      "popular_1 : float32 ; uinque values: 457\n",
      "num_people_1 : float32 ; uinque values: 451\n",
      "popular_2 : float32 ; uinque values: 1018\n",
      "num_people_2 : float32 ; uinque values: 983\n",
      "popular_3 : float32 ; uinque values: 996\n",
      "num_people_3 : float32 ; uinque values: 967\n",
      "popular_4 : float32 ; uinque values: 1554\n",
      "num_people_4 : float32 ; uinque values: 1544\n",
      "active_0 : float32 ; uinque values: 1814\n",
      "num_song_0 : float32 ; uinque values: 1772\n",
      "active_1 : float32 ; uinque values: 346\n",
      "num_song_1 : float32 ; uinque values: 339\n",
      "active_2 : float32 ; uinque values: 707\n",
      "num_song_2 : float32 ; uinque values: 688\n",
      "active_3 : float32 ; uinque values: 620\n",
      "num_song_3 : float32 ; uinque values: 591\n",
      "active_4 : float32 ; uinque values: 1059\n",
      "num_song_4 : float32 ; uinque values: 1043\n",
      "composer_by_city : float32 ; uinque values: 21\n",
      "composer_by_country : float32 ; uinque values: 52\n",
      "composer_by_language : float32 ; uinque values: 8\n",
      "lyricist_by_city : float32 ; uinque values: 21\n",
      "lyricist_by_country : float32 ; uinque values: 52\n",
      "lyricist_by_language : float32 ; uinque values: 6\n",
      "artist_name_by_city : float32 ; uinque values: 21\n",
      "artist_name_by_country : float32 ; uinque values: 53\n",
      "artist_name_by_language : float32 ; uinque values: 8\n",
      "song_id_by_city : float32 ; uinque values: 21\n",
      "msno_by_country : float32 ; uinque values: 45\n",
      "msno_by_language : float32 ; uinque values: 10\n",
      "genre_ids_score : float32 ; uinque values: 132\n",
      "city_by_song_id : float32 ; uinque values: 21\n",
      "city_by_msno : float32 ; uinque values: 21\n",
      "city_by_genre_ids : float32 ; uinque values: 19\n",
      "city_by_artist_name : float32 ; uinque values: 21\n",
      "language_by_song_id : float32 ; uinque values: 10\n",
      "language_by_msno : float32 ; uinque values: 10\n",
      "language_by_genre_ids : float32 ; uinque values: 9\n",
      "language_by_artist_name : float32 ; uinque values: 10\n"
     ]
    }
   ],
   "source": [
    "for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : int64 ; uinque values: 2556790\n",
      "msno : category ; uinque values: 25131\n",
      "song_id : category ; uinque values: 224753\n",
      "source_system_tab : category ; uinque values: 9\n",
      "source_screen_name : category ; uinque values: 22\n",
      "source_type : category ; uinque values: 12\n",
      "city : category ; uinque values: 21\n",
      "bd : int64 ; uinque values: 89\n",
      "gender : category ; uinque values: 2\n",
      "registered_via : category ; uinque values: 6\n",
      "song_length : float64 ; uinque values: 45659\n",
      "genre_ids : category ; uinque values: 148\n",
      "language : category ; uinque values: 10\n",
      "name : category ; uinque values: 154715\n",
      "country : category ; uinque values: 94\n",
      "song_year : float32 ; uinque values: 100\n",
      "registration_year : int64 ; uinque values: 14\n",
      "registration_month : int64 ; uinque values: 12\n",
      "registration_day : int64 ; uinque values: 31\n",
      "expiration_year : int64 ; uinque values: 16\n",
      "expiration_month : int64 ; uinque values: 12\n",
      "expiration_day : int64 ; uinque values: 31\n",
      "days : int64 ; uinque values: 4240\n",
      "composer_score : float32 ; uinque values: 2207\n",
      "composer : category ; uinque values: 35087\n",
      "lyricist_score : float32 ; uinque values: 1654\n",
      "lyricist : category ; uinque values: 18136\n",
      "artist_name_score : float32 ; uinque values: 1848\n",
      "artist_name : category ; uinque values: 26901\n",
      "popular_0 : float32 ; uinque values: 2083\n",
      "num_people_0 : float32 ; uinque values: 2045\n",
      "popular_1 : float32 ; uinque values: 457\n",
      "num_people_1 : float32 ; uinque values: 451\n",
      "popular_2 : float32 ; uinque values: 1018\n",
      "num_people_2 : float32 ; uinque values: 983\n",
      "popular_3 : float32 ; uinque values: 996\n",
      "num_people_3 : float32 ; uinque values: 967\n",
      "popular_4 : float32 ; uinque values: 1616\n",
      "num_people_4 : float32 ; uinque values: 1600\n",
      "active_0 : float32 ; uinque values: 1811\n",
      "num_song_0 : float32 ; uinque values: 1767\n",
      "active_1 : float32 ; uinque values: 346\n",
      "num_song_1 : float32 ; uinque values: 339\n",
      "active_2 : float32 ; uinque values: 704\n",
      "num_song_2 : float32 ; uinque values: 686\n",
      "active_3 : float32 ; uinque values: 618\n",
      "num_song_3 : float32 ; uinque values: 589\n",
      "active_4 : float32 ; uinque values: 1058\n",
      "num_song_4 : float32 ; uinque values: 1042\n",
      "composer_by_city : float32 ; uinque values: 21\n",
      "composer_by_country : float32 ; uinque values: 52\n",
      "composer_by_language : float32 ; uinque values: 8\n",
      "lyricist_by_city : float32 ; uinque values: 21\n",
      "lyricist_by_country : float32 ; uinque values: 52\n",
      "lyricist_by_language : float32 ; uinque values: 6\n",
      "artist_name_by_city : float32 ; uinque values: 21\n",
      "artist_name_by_country : float32 ; uinque values: 53\n",
      "artist_name_by_language : float32 ; uinque values: 8\n",
      "song_id_by_city : float32 ; uinque values: 21\n",
      "msno_by_country : float32 ; uinque values: 45\n",
      "msno_by_language : float32 ; uinque values: 10\n",
      "genre_ids_score : float32 ; uinque values: 132\n",
      "city_by_song_id : float32 ; uinque values: 21\n",
      "city_by_msno : float32 ; uinque values: 21\n",
      "city_by_genre_ids : float32 ; uinque values: 19\n",
      "city_by_artist_name : float32 ; uinque values: 21\n",
      "language_by_song_id : float32 ; uinque values: 10\n",
      "language_by_msno : float32 ; uinque values: 10\n",
      "language_by_genre_ids : float32 ; uinque values: 9\n",
      "language_by_artist_name : float32 ; uinque values: 10\n"
     ]
    }
   ],
   "source": [
    "for col in test.columns: print(col, ':', test[col].dtype, '; uinque values:', len(test[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno : category ; uinque values: 30571\n",
      "song_id : category ; uinque values: 357496\n",
      "source_system_tab : category ; uinque values: 9\n",
      "source_screen_name : category ; uinque values: 20\n",
      "source_type : category ; uinque values: 12\n",
      "target : int64 ; uinque values: 2\n",
      "city : category ; uinque values: 21\n",
      "bd : int64 ; uinque values: 91\n",
      "gender : category ; uinque values: 2\n",
      "registered_via : category ; uinque values: 5\n",
      "song_length : float64 ; uinque values: 60028\n",
      "genre_ids : category ; uinque values: 153\n",
      "language : category ; uinque values: 10\n",
      "name : category ; uinque values: 232729\n",
      "country : category ; uinque values: 107\n",
      "song_year : float32 ; uinque values: 100\n",
      "registration_year : int64 ; uinque values: 14\n",
      "registration_month : int64 ; uinque values: 12\n",
      "registration_day : int64 ; uinque values: 31\n",
      "expiration_year : int64 ; uinque values: 18\n",
      "expiration_month : int64 ; uinque values: 12\n",
      "expiration_day : int64 ; uinque values: 31\n",
      "days : int64 ; uinque values: 4319\n",
      "composer_score : float32 ; uinque values: 2218\n",
      "composer : category ; uinque values: 49086\n",
      "lyricist_score : float32 ; uinque values: 1652\n",
      "lyricist : category ; uinque values: 24205\n",
      "artist_name_score : float32 ; uinque values: 1842\n",
      "artist_name : category ; uinque values: 39234\n",
      "popular_0 : float32 ; uinque values: 2041\n",
      "num_people_0 : float32 ; uinque values: 2003\n",
      "popular_1 : float32 ; uinque values: 457\n",
      "num_people_1 : float32 ; uinque values: 451\n",
      "popular_2 : float32 ; uinque values: 1018\n",
      "num_people_2 : float32 ; uinque values: 983\n",
      "popular_3 : float32 ; uinque values: 996\n",
      "num_people_3 : float32 ; uinque values: 967\n",
      "popular_4 : float32 ; uinque values: 1554\n",
      "num_people_4 : float32 ; uinque values: 1544\n",
      "active_0 : float32 ; uinque values: 1814\n",
      "num_song_0 : float32 ; uinque values: 1772\n",
      "active_1 : float32 ; uinque values: 346\n",
      "num_song_1 : float32 ; uinque values: 339\n",
      "active_2 : float32 ; uinque values: 707\n",
      "num_song_2 : float32 ; uinque values: 688\n",
      "active_3 : float32 ; uinque values: 620\n",
      "num_song_3 : float32 ; uinque values: 591\n",
      "active_4 : float32 ; uinque values: 1059\n",
      "num_song_4 : float32 ; uinque values: 1043\n",
      "composer_by_city : float32 ; uinque values: 21\n",
      "composer_by_country : float32 ; uinque values: 52\n",
      "composer_by_language : float32 ; uinque values: 8\n",
      "lyricist_by_city : float32 ; uinque values: 21\n",
      "lyricist_by_country : float32 ; uinque values: 52\n",
      "lyricist_by_language : float32 ; uinque values: 6\n",
      "artist_name_by_city : float32 ; uinque values: 21\n",
      "artist_name_by_country : float32 ; uinque values: 53\n",
      "artist_name_by_language : float32 ; uinque values: 8\n",
      "song_id_by_city : float32 ; uinque values: 21\n",
      "msno_by_country : float32 ; uinque values: 45\n",
      "msno_by_language : float32 ; uinque values: 10\n",
      "genre_ids_score : float32 ; uinque values: 132\n",
      "city_by_song_id : float32 ; uinque values: 21\n",
      "city_by_msno : float32 ; uinque values: 21\n",
      "city_by_genre_ids : float32 ; uinque values: 19\n",
      "city_by_artist_name : float32 ; uinque values: 21\n",
      "language_by_song_id : float32 ; uinque values: 10\n",
      "language_by_msno : float32 ; uinque values: 10\n",
      "language_by_genre_ids : float32 ; uinque values: 9\n",
      "language_by_artist_name : float32 ; uinque values: 10\n"
     ]
    }
   ],
   "source": [
    "for col in train_use.columns: print(col, ':', train_use[col].dtype, '; uinque values:', len(train_use[col].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556790 2556790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_id), len(test))\n",
    "#del train_use_org, test_org, validation_use_org\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kapok/pyenv35/lib/python3.5/site-packages/lightgbm/engine.py:98: UserWarning: Found `num_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.63702\n",
      "[2]\tvalid_0's auc: 0.642595\n",
      "[3]\tvalid_0's auc: 0.643955\n",
      "[4]\tvalid_0's auc: 0.646303\n",
      "[5]\tvalid_0's auc: 0.647612\n",
      "[6]\tvalid_0's auc: 0.649217\n",
      "[7]\tvalid_0's auc: 0.649965\n",
      "[8]\tvalid_0's auc: 0.650428\n",
      "[9]\tvalid_0's auc: 0.650768\n",
      "[10]\tvalid_0's auc: 0.651927\n",
      "[11]\tvalid_0's auc: 0.653172\n",
      "[12]\tvalid_0's auc: 0.654507\n",
      "[13]\tvalid_0's auc: 0.655216\n",
      "[14]\tvalid_0's auc: 0.65627\n",
      "[15]\tvalid_0's auc: 0.656494\n",
      "[16]\tvalid_0's auc: 0.65743\n",
      "[17]\tvalid_0's auc: 0.657874\n",
      "[18]\tvalid_0's auc: 0.65881\n",
      "[19]\tvalid_0's auc: 0.659233\n",
      "[20]\tvalid_0's auc: 0.659939\n",
      "[21]\tvalid_0's auc: 0.660387\n",
      "[22]\tvalid_0's auc: 0.661133\n",
      "[23]\tvalid_0's auc: 0.661535\n",
      "[24]\tvalid_0's auc: 0.662217\n",
      "[25]\tvalid_0's auc: 0.663002\n",
      "[26]\tvalid_0's auc: 0.663424\n",
      "[27]\tvalid_0's auc: 0.663833\n",
      "[28]\tvalid_0's auc: 0.664083\n",
      "[29]\tvalid_0's auc: 0.664677\n",
      "[30]\tvalid_0's auc: 0.664995\n",
      "[31]\tvalid_0's auc: 0.665289\n",
      "[32]\tvalid_0's auc: 0.665564\n",
      "[33]\tvalid_0's auc: 0.665822\n",
      "[34]\tvalid_0's auc: 0.666206\n",
      "[35]\tvalid_0's auc: 0.666669\n",
      "[36]\tvalid_0's auc: 0.666924\n",
      "[37]\tvalid_0's auc: 0.667196\n",
      "[38]\tvalid_0's auc: 0.667553\n",
      "[39]\tvalid_0's auc: 0.667913\n",
      "[40]\tvalid_0's auc: 0.668135\n",
      "[41]\tvalid_0's auc: 0.668233\n",
      "[42]\tvalid_0's auc: 0.6684\n",
      "[43]\tvalid_0's auc: 0.668644\n",
      "[44]\tvalid_0's auc: 0.66888\n",
      "[45]\tvalid_0's auc: 0.669332\n",
      "[46]\tvalid_0's auc: 0.669491\n",
      "[47]\tvalid_0's auc: 0.669659\n",
      "[48]\tvalid_0's auc: 0.66983\n",
      "[49]\tvalid_0's auc: 0.67006\n",
      "[50]\tvalid_0's auc: 0.670197\n",
      "[51]\tvalid_0's auc: 0.670528\n",
      "[52]\tvalid_0's auc: 0.670865\n",
      "[53]\tvalid_0's auc: 0.670997\n",
      "[54]\tvalid_0's auc: 0.671151\n",
      "[55]\tvalid_0's auc: 0.671223\n",
      "[56]\tvalid_0's auc: 0.671443\n",
      "[57]\tvalid_0's auc: 0.671661\n",
      "[58]\tvalid_0's auc: 0.671847\n",
      "[59]\tvalid_0's auc: 0.671971\n",
      "[60]\tvalid_0's auc: 0.672147\n",
      "[61]\tvalid_0's auc: 0.672503\n",
      "[62]\tvalid_0's auc: 0.672766\n",
      "[63]\tvalid_0's auc: 0.672928\n",
      "[64]\tvalid_0's auc: 0.673289\n",
      "[65]\tvalid_0's auc: 0.673319\n",
      "[66]\tvalid_0's auc: 0.6737\n",
      "[67]\tvalid_0's auc: 0.673849\n",
      "[68]\tvalid_0's auc: 0.673965\n",
      "[69]\tvalid_0's auc: 0.67417\n",
      "[70]\tvalid_0's auc: 0.674391\n",
      "[71]\tvalid_0's auc: 0.674448\n",
      "[72]\tvalid_0's auc: 0.674599\n",
      "[73]\tvalid_0's auc: 0.674642\n",
      "[74]\tvalid_0's auc: 0.674755\n",
      "[75]\tvalid_0's auc: 0.674896\n",
      "[76]\tvalid_0's auc: 0.675052\n",
      "[77]\tvalid_0's auc: 0.675173\n",
      "[78]\tvalid_0's auc: 0.675198\n",
      "[79]\tvalid_0's auc: 0.675261\n",
      "[80]\tvalid_0's auc: 0.675408\n",
      "[81]\tvalid_0's auc: 0.67555\n",
      "[82]\tvalid_0's auc: 0.675641\n",
      "[83]\tvalid_0's auc: 0.675722\n",
      "[84]\tvalid_0's auc: 0.675911\n",
      "[85]\tvalid_0's auc: 0.67596\n",
      "[86]\tvalid_0's auc: 0.676063\n",
      "[87]\tvalid_0's auc: 0.676272\n",
      "[88]\tvalid_0's auc: 0.676414\n",
      "[89]\tvalid_0's auc: 0.676543\n",
      "[90]\tvalid_0's auc: 0.676578\n",
      "[91]\tvalid_0's auc: 0.676564\n",
      "[92]\tvalid_0's auc: 0.676648\n",
      "[93]\tvalid_0's auc: 0.676762\n",
      "[94]\tvalid_0's auc: 0.67683\n",
      "[95]\tvalid_0's auc: 0.67698\n",
      "[96]\tvalid_0's auc: 0.677074\n",
      "[97]\tvalid_0's auc: 0.677189\n",
      "[98]\tvalid_0's auc: 0.677297\n",
      "[99]\tvalid_0's auc: 0.677374\n",
      "[100]\tvalid_0's auc: 0.677605\n",
      "[101]\tvalid_0's auc: 0.677666\n",
      "[102]\tvalid_0's auc: 0.677706\n",
      "[103]\tvalid_0's auc: 0.677879\n",
      "[104]\tvalid_0's auc: 0.677923\n",
      "[105]\tvalid_0's auc: 0.677961\n",
      "[106]\tvalid_0's auc: 0.678072\n",
      "[107]\tvalid_0's auc: 0.678165\n",
      "[108]\tvalid_0's auc: 0.678336\n",
      "[109]\tvalid_0's auc: 0.678433\n",
      "[110]\tvalid_0's auc: 0.678556\n",
      "[111]\tvalid_0's auc: 0.678752\n",
      "[112]\tvalid_0's auc: 0.67886\n",
      "[113]\tvalid_0's auc: 0.678989\n",
      "[114]\tvalid_0's auc: 0.679085\n",
      "[115]\tvalid_0's auc: 0.679339\n",
      "[116]\tvalid_0's auc: 0.679462\n",
      "[117]\tvalid_0's auc: 0.679616\n",
      "[118]\tvalid_0's auc: 0.679676\n",
      "[119]\tvalid_0's auc: 0.679733\n",
      "[120]\tvalid_0's auc: 0.679761\n",
      "[121]\tvalid_0's auc: 0.679813\n",
      "[122]\tvalid_0's auc: 0.679844\n",
      "[123]\tvalid_0's auc: 0.679868\n",
      "[124]\tvalid_0's auc: 0.679991\n",
      "[125]\tvalid_0's auc: 0.680202\n",
      "[126]\tvalid_0's auc: 0.680339\n",
      "[127]\tvalid_0's auc: 0.680452\n",
      "[128]\tvalid_0's auc: 0.680512\n",
      "[129]\tvalid_0's auc: 0.68058\n",
      "[130]\tvalid_0's auc: 0.680693\n",
      "[131]\tvalid_0's auc: 0.680732\n",
      "[132]\tvalid_0's auc: 0.680864\n",
      "[133]\tvalid_0's auc: 0.68097\n",
      "[134]\tvalid_0's auc: 0.681133\n",
      "[135]\tvalid_0's auc: 0.681218\n",
      "[136]\tvalid_0's auc: 0.681366\n",
      "[137]\tvalid_0's auc: 0.681393\n",
      "[138]\tvalid_0's auc: 0.681355\n",
      "[139]\tvalid_0's auc: 0.681437\n",
      "[140]\tvalid_0's auc: 0.681549\n",
      "[141]\tvalid_0's auc: 0.681595\n",
      "[142]\tvalid_0's auc: 0.681656\n",
      "[143]\tvalid_0's auc: 0.681649\n",
      "[144]\tvalid_0's auc: 0.681725\n",
      "[145]\tvalid_0's auc: 0.68186\n",
      "[146]\tvalid_0's auc: 0.681911\n",
      "[147]\tvalid_0's auc: 0.681935\n",
      "[148]\tvalid_0's auc: 0.681973\n",
      "[149]\tvalid_0's auc: 0.682083\n",
      "[150]\tvalid_0's auc: 0.682159\n",
      "[151]\tvalid_0's auc: 0.682227\n",
      "[152]\tvalid_0's auc: 0.682329\n",
      "[153]\tvalid_0's auc: 0.682367\n",
      "[154]\tvalid_0's auc: 0.682437\n",
      "[155]\tvalid_0's auc: 0.682504\n",
      "[156]\tvalid_0's auc: 0.682567\n",
      "[157]\tvalid_0's auc: 0.682682\n",
      "[158]\tvalid_0's auc: 0.682744\n",
      "[159]\tvalid_0's auc: 0.6828\n",
      "[160]\tvalid_0's auc: 0.682863\n",
      "[161]\tvalid_0's auc: 0.682854\n",
      "[162]\tvalid_0's auc: 0.683045\n",
      "[163]\tvalid_0's auc: 0.683134\n",
      "[164]\tvalid_0's auc: 0.683195\n",
      "[165]\tvalid_0's auc: 0.683277\n",
      "[166]\tvalid_0's auc: 0.683303\n",
      "[167]\tvalid_0's auc: 0.683402\n",
      "[168]\tvalid_0's auc: 0.683527\n",
      "[169]\tvalid_0's auc: 0.683482\n",
      "[170]\tvalid_0's auc: 0.683507\n",
      "[171]\tvalid_0's auc: 0.683583\n",
      "[172]\tvalid_0's auc: 0.683653\n",
      "[173]\tvalid_0's auc: 0.683866\n",
      "[174]\tvalid_0's auc: 0.683881\n",
      "[175]\tvalid_0's auc: 0.684101\n",
      "[176]\tvalid_0's auc: 0.684307\n",
      "[177]\tvalid_0's auc: 0.684367\n",
      "[178]\tvalid_0's auc: 0.684446\n",
      "[179]\tvalid_0's auc: 0.684477\n",
      "[180]\tvalid_0's auc: 0.684601\n",
      "[181]\tvalid_0's auc: 0.684683\n",
      "[182]\tvalid_0's auc: 0.684725\n",
      "[183]\tvalid_0's auc: 0.684786\n",
      "[184]\tvalid_0's auc: 0.68486\n",
      "[185]\tvalid_0's auc: 0.684977\n",
      "[186]\tvalid_0's auc: 0.685074\n",
      "[187]\tvalid_0's auc: 0.685193\n",
      "[188]\tvalid_0's auc: 0.685236\n",
      "[189]\tvalid_0's auc: 0.685243\n",
      "[190]\tvalid_0's auc: 0.685242\n",
      "[191]\tvalid_0's auc: 0.685288\n",
      "[192]\tvalid_0's auc: 0.685289\n",
      "[193]\tvalid_0's auc: 0.685408\n",
      "[194]\tvalid_0's auc: 0.68558\n",
      "[195]\tvalid_0's auc: 0.685622\n",
      "[196]\tvalid_0's auc: 0.685721\n",
      "[197]\tvalid_0's auc: 0.685812\n",
      "[198]\tvalid_0's auc: 0.685799\n",
      "[199]\tvalid_0's auc: 0.685884\n",
      "[200]\tvalid_0's auc: 0.685952\n",
      "[201]\tvalid_0's auc: 0.686015\n",
      "[202]\tvalid_0's auc: 0.686047\n",
      "[203]\tvalid_0's auc: 0.686073\n",
      "[204]\tvalid_0's auc: 0.686119\n",
      "[205]\tvalid_0's auc: 0.686119\n",
      "[206]\tvalid_0's auc: 0.686163\n",
      "[207]\tvalid_0's auc: 0.686372\n",
      "[208]\tvalid_0's auc: 0.68641\n",
      "[209]\tvalid_0's auc: 0.686456\n",
      "[210]\tvalid_0's auc: 0.686617\n",
      "[211]\tvalid_0's auc: 0.686709\n",
      "[212]\tvalid_0's auc: 0.686679\n",
      "[213]\tvalid_0's auc: 0.686736\n",
      "[214]\tvalid_0's auc: 0.686909\n",
      "[215]\tvalid_0's auc: 0.686931\n",
      "[216]\tvalid_0's auc: 0.687015\n",
      "[217]\tvalid_0's auc: 0.687023\n",
      "[218]\tvalid_0's auc: 0.687079\n",
      "[219]\tvalid_0's auc: 0.687098\n",
      "[220]\tvalid_0's auc: 0.687183\n",
      "[221]\tvalid_0's auc: 0.687215\n",
      "[222]\tvalid_0's auc: 0.687336\n",
      "[223]\tvalid_0's auc: 0.687417\n",
      "[224]\tvalid_0's auc: 0.687423\n",
      "[225]\tvalid_0's auc: 0.687439\n",
      "[226]\tvalid_0's auc: 0.687642\n",
      "[227]\tvalid_0's auc: 0.687635\n",
      "[228]\tvalid_0's auc: 0.687643\n",
      "[229]\tvalid_0's auc: 0.687862\n",
      "[230]\tvalid_0's auc: 0.68796\n",
      "[231]\tvalid_0's auc: 0.687987\n",
      "[232]\tvalid_0's auc: 0.687988\n",
      "[233]\tvalid_0's auc: 0.687986\n",
      "[234]\tvalid_0's auc: 0.688008\n",
      "[235]\tvalid_0's auc: 0.687996\n",
      "[236]\tvalid_0's auc: 0.688086\n",
      "[237]\tvalid_0's auc: 0.688093\n",
      "[238]\tvalid_0's auc: 0.688156\n",
      "[239]\tvalid_0's auc: 0.688202\n",
      "[240]\tvalid_0's auc: 0.688268\n",
      "[241]\tvalid_0's auc: 0.688339\n",
      "[242]\tvalid_0's auc: 0.688398\n",
      "[243]\tvalid_0's auc: 0.688413\n",
      "[244]\tvalid_0's auc: 0.688443\n",
      "[245]\tvalid_0's auc: 0.688503\n",
      "[246]\tvalid_0's auc: 0.6885\n",
      "[247]\tvalid_0's auc: 0.688541\n",
      "[248]\tvalid_0's auc: 0.688585\n",
      "[249]\tvalid_0's auc: 0.68867\n",
      "[250]\tvalid_0's auc: 0.688773\n",
      "[251]\tvalid_0's auc: 0.688772\n",
      "[252]\tvalid_0's auc: 0.688712\n",
      "[253]\tvalid_0's auc: 0.688683\n",
      "[254]\tvalid_0's auc: 0.688926\n",
      "[255]\tvalid_0's auc: 0.688907\n",
      "[256]\tvalid_0's auc: 0.688938\n",
      "[257]\tvalid_0's auc: 0.688929\n",
      "[258]\tvalid_0's auc: 0.68894\n",
      "[259]\tvalid_0's auc: 0.688941\n",
      "[260]\tvalid_0's auc: 0.688952\n",
      "[261]\tvalid_0's auc: 0.688927\n",
      "[262]\tvalid_0's auc: 0.688987\n",
      "[263]\tvalid_0's auc: 0.688936\n",
      "[264]\tvalid_0's auc: 0.688969\n",
      "[265]\tvalid_0's auc: 0.689028\n",
      "[266]\tvalid_0's auc: 0.689123\n",
      "[267]\tvalid_0's auc: 0.689191\n",
      "[268]\tvalid_0's auc: 0.689237\n",
      "[269]\tvalid_0's auc: 0.68932\n",
      "[270]\tvalid_0's auc: 0.689326\n",
      "[271]\tvalid_0's auc: 0.689357\n",
      "[272]\tvalid_0's auc: 0.689329\n",
      "[273]\tvalid_0's auc: 0.689392\n",
      "[274]\tvalid_0's auc: 0.689402\n",
      "[275]\tvalid_0's auc: 0.689401\n",
      "[276]\tvalid_0's auc: 0.68942\n",
      "[277]\tvalid_0's auc: 0.689497\n",
      "[278]\tvalid_0's auc: 0.689495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[279]\tvalid_0's auc: 0.689536\n",
      "[280]\tvalid_0's auc: 0.689564\n",
      "[281]\tvalid_0's auc: 0.689594\n",
      "[282]\tvalid_0's auc: 0.689743\n",
      "[283]\tvalid_0's auc: 0.689786\n",
      "[284]\tvalid_0's auc: 0.689772\n",
      "[285]\tvalid_0's auc: 0.689811\n",
      "[286]\tvalid_0's auc: 0.689847\n",
      "[287]\tvalid_0's auc: 0.689895\n",
      "[288]\tvalid_0's auc: 0.689943\n",
      "[289]\tvalid_0's auc: 0.689994\n",
      "[290]\tvalid_0's auc: 0.689998\n",
      "[291]\tvalid_0's auc: 0.690086\n",
      "[292]\tvalid_0's auc: 0.690116\n",
      "[293]\tvalid_0's auc: 0.690122\n",
      "[294]\tvalid_0's auc: 0.690131\n",
      "[295]\tvalid_0's auc: 0.690161\n",
      "[296]\tvalid_0's auc: 0.690178\n",
      "[297]\tvalid_0's auc: 0.690279\n",
      "[298]\tvalid_0's auc: 0.690382\n",
      "[299]\tvalid_0's auc: 0.690449\n",
      "[300]\tvalid_0's auc: 0.690439\n",
      "[301]\tvalid_0's auc: 0.690452\n",
      "[302]\tvalid_0's auc: 0.690458\n",
      "[303]\tvalid_0's auc: 0.690455\n",
      "[304]\tvalid_0's auc: 0.690476\n",
      "[305]\tvalid_0's auc: 0.690514\n",
      "[306]\tvalid_0's auc: 0.690583\n",
      "[307]\tvalid_0's auc: 0.690625\n",
      "[308]\tvalid_0's auc: 0.690817\n",
      "[309]\tvalid_0's auc: 0.69084\n",
      "[310]\tvalid_0's auc: 0.690823\n",
      "[311]\tvalid_0's auc: 0.690842\n",
      "[312]\tvalid_0's auc: 0.690825\n",
      "[313]\tvalid_0's auc: 0.690832\n",
      "[314]\tvalid_0's auc: 0.690888\n",
      "[315]\tvalid_0's auc: 0.690925\n",
      "[316]\tvalid_0's auc: 0.69097\n",
      "[317]\tvalid_0's auc: 0.690992\n",
      "[318]\tvalid_0's auc: 0.691005\n",
      "[319]\tvalid_0's auc: 0.691051\n",
      "[320]\tvalid_0's auc: 0.691048\n",
      "[321]\tvalid_0's auc: 0.691103\n",
      "[322]\tvalid_0's auc: 0.691077\n",
      "[323]\tvalid_0's auc: 0.691049\n",
      "[324]\tvalid_0's auc: 0.691049\n",
      "[325]\tvalid_0's auc: 0.6911\n",
      "[326]\tvalid_0's auc: 0.691147\n",
      "[327]\tvalid_0's auc: 0.691188\n",
      "[328]\tvalid_0's auc: 0.691165\n",
      "[329]\tvalid_0's auc: 0.691173\n",
      "[330]\tvalid_0's auc: 0.691208\n",
      "[331]\tvalid_0's auc: 0.691195\n",
      "[332]\tvalid_0's auc: 0.691225\n",
      "[333]\tvalid_0's auc: 0.691287\n",
      "[334]\tvalid_0's auc: 0.691299\n",
      "[335]\tvalid_0's auc: 0.691295\n",
      "[336]\tvalid_0's auc: 0.691326\n",
      "[337]\tvalid_0's auc: 0.691321\n",
      "[338]\tvalid_0's auc: 0.691379\n",
      "[339]\tvalid_0's auc: 0.691389\n",
      "[340]\tvalid_0's auc: 0.691378\n",
      "[341]\tvalid_0's auc: 0.691382\n",
      "[342]\tvalid_0's auc: 0.691362\n",
      "[343]\tvalid_0's auc: 0.691296\n",
      "[344]\tvalid_0's auc: 0.691316\n",
      "[345]\tvalid_0's auc: 0.691399\n",
      "[346]\tvalid_0's auc: 0.69144\n",
      "[347]\tvalid_0's auc: 0.691407\n",
      "[348]\tvalid_0's auc: 0.691408\n",
      "[349]\tvalid_0's auc: 0.691419\n",
      "[350]\tvalid_0's auc: 0.691642\n",
      "[351]\tvalid_0's auc: 0.691661\n",
      "[352]\tvalid_0's auc: 0.691687\n",
      "[353]\tvalid_0's auc: 0.691734\n",
      "[354]\tvalid_0's auc: 0.691766\n",
      "[355]\tvalid_0's auc: 0.691792\n",
      "[356]\tvalid_0's auc: 0.691761\n",
      "[357]\tvalid_0's auc: 0.691798\n",
      "[358]\tvalid_0's auc: 0.691806\n",
      "[359]\tvalid_0's auc: 0.691829\n",
      "[360]\tvalid_0's auc: 0.691842\n",
      "[361]\tvalid_0's auc: 0.691862\n",
      "[362]\tvalid_0's auc: 0.691924\n",
      "[363]\tvalid_0's auc: 0.691942\n",
      "[364]\tvalid_0's auc: 0.692002\n",
      "[365]\tvalid_0's auc: 0.692033\n",
      "[366]\tvalid_0's auc: 0.692047\n",
      "[367]\tvalid_0's auc: 0.692049\n",
      "[368]\tvalid_0's auc: 0.692019\n",
      "[369]\tvalid_0's auc: 0.692066\n",
      "[370]\tvalid_0's auc: 0.692099\n",
      "[371]\tvalid_0's auc: 0.692136\n",
      "[372]\tvalid_0's auc: 0.692165\n",
      "[373]\tvalid_0's auc: 0.692169\n",
      "[374]\tvalid_0's auc: 0.692146\n",
      "[375]\tvalid_0's auc: 0.69218\n",
      "[376]\tvalid_0's auc: 0.692209\n",
      "[377]\tvalid_0's auc: 0.692248\n",
      "[378]\tvalid_0's auc: 0.692274\n",
      "[379]\tvalid_0's auc: 0.692287\n",
      "[380]\tvalid_0's auc: 0.692343\n",
      "[381]\tvalid_0's auc: 0.692383\n",
      "[382]\tvalid_0's auc: 0.692377\n",
      "[383]\tvalid_0's auc: 0.692483\n",
      "[384]\tvalid_0's auc: 0.692466\n",
      "[385]\tvalid_0's auc: 0.692493\n",
      "[386]\tvalid_0's auc: 0.692451\n",
      "[387]\tvalid_0's auc: 0.692461\n",
      "[388]\tvalid_0's auc: 0.692484\n",
      "[389]\tvalid_0's auc: 0.692488\n",
      "[390]\tvalid_0's auc: 0.692491\n",
      "[391]\tvalid_0's auc: 0.69254\n",
      "[392]\tvalid_0's auc: 0.69254\n",
      "[393]\tvalid_0's auc: 0.692542\n",
      "[394]\tvalid_0's auc: 0.692573\n",
      "[395]\tvalid_0's auc: 0.692548\n",
      "[396]\tvalid_0's auc: 0.692551\n",
      "[397]\tvalid_0's auc: 0.692549\n",
      "[398]\tvalid_0's auc: 0.692518\n",
      "[399]\tvalid_0's auc: 0.692562\n",
      "[400]\tvalid_0's auc: 0.692597\n",
      "cur fold finished.\n"
     ]
    }
   ],
   "source": [
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "train_data = lgb.Dataset(train_use.drop(['target'],axis=1),label=train_use['target'])\n",
    "val_data = lgb.Dataset(validation_use.drop(['target'],axis=1),label=validation_use['target'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1 ,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 108,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 128,\n",
    "    'max_depth': 10,\n",
    "    'num_rounds': 400,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "predictions+=bst.predict(test.drop(['id'],axis=1))\n",
    "print('cur fold finished.')\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.455405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.425251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.202842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.111146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.110424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.141347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.141347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.680255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.177954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.702115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.753793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.141237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.287554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.568021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.253249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.561376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.508293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.533862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.358661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.728074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.891491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.614250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.608681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.621278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.440757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.576347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.502497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.506492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.514097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.490527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556760</th>\n",
       "      <td>2556760</td>\n",
       "      <td>0.827352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556761</th>\n",
       "      <td>2556761</td>\n",
       "      <td>0.659187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556762</th>\n",
       "      <td>2556762</td>\n",
       "      <td>0.715765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556763</th>\n",
       "      <td>2556763</td>\n",
       "      <td>0.507795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556764</th>\n",
       "      <td>2556764</td>\n",
       "      <td>0.513270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556765</th>\n",
       "      <td>2556765</td>\n",
       "      <td>0.659078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556766</th>\n",
       "      <td>2556766</td>\n",
       "      <td>0.598775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556767</th>\n",
       "      <td>2556767</td>\n",
       "      <td>0.535713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556768</th>\n",
       "      <td>2556768</td>\n",
       "      <td>0.602148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556769</th>\n",
       "      <td>2556769</td>\n",
       "      <td>0.339206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556770</th>\n",
       "      <td>2556770</td>\n",
       "      <td>0.324559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556771</th>\n",
       "      <td>2556771</td>\n",
       "      <td>0.496088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556772</th>\n",
       "      <td>2556772</td>\n",
       "      <td>0.430303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556773</th>\n",
       "      <td>2556773</td>\n",
       "      <td>0.641167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556774</th>\n",
       "      <td>2556774</td>\n",
       "      <td>0.436485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556775</th>\n",
       "      <td>2556775</td>\n",
       "      <td>0.358376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556776</th>\n",
       "      <td>2556776</td>\n",
       "      <td>0.376853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556777</th>\n",
       "      <td>2556777</td>\n",
       "      <td>0.365509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556778</th>\n",
       "      <td>2556778</td>\n",
       "      <td>0.429985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556779</th>\n",
       "      <td>2556779</td>\n",
       "      <td>0.257485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556780</th>\n",
       "      <td>2556780</td>\n",
       "      <td>0.295635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556781</th>\n",
       "      <td>2556781</td>\n",
       "      <td>0.257060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556782</th>\n",
       "      <td>2556782</td>\n",
       "      <td>0.169385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556783</th>\n",
       "      <td>2556783</td>\n",
       "      <td>0.514631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556784</th>\n",
       "      <td>2556784</td>\n",
       "      <td>0.155863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556785</th>\n",
       "      <td>2556785</td>\n",
       "      <td>0.115068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556786</th>\n",
       "      <td>2556786</td>\n",
       "      <td>0.419749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556787</th>\n",
       "      <td>2556787</td>\n",
       "      <td>0.417103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556788</th>\n",
       "      <td>2556788</td>\n",
       "      <td>0.371930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556789</th>\n",
       "      <td>2556789</td>\n",
       "      <td>0.364652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2556790 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id    target\n",
       "0              0  0.455405\n",
       "1              1  0.425251\n",
       "2              2  0.202842\n",
       "3              3  0.111146\n",
       "4              4  0.110424\n",
       "5              5  0.141347\n",
       "6              6  0.141347\n",
       "7              7  0.680255\n",
       "8              8  0.177954\n",
       "9              9  0.702115\n",
       "10            10  0.753793\n",
       "11            11  0.141237\n",
       "12            12  0.287554\n",
       "13            13  0.568021\n",
       "14            14  0.253249\n",
       "15            15  0.561376\n",
       "16            16  0.508293\n",
       "17            17  0.533862\n",
       "18            18  0.358661\n",
       "19            19  0.728074\n",
       "20            20  0.891491\n",
       "21            21  0.614250\n",
       "22            22  0.608681\n",
       "23            23  0.621278\n",
       "24            24  0.440757\n",
       "25            25  0.576347\n",
       "26            26  0.502497\n",
       "27            27  0.506492\n",
       "28            28  0.514097\n",
       "29            29  0.490527\n",
       "...          ...       ...\n",
       "2556760  2556760  0.827352\n",
       "2556761  2556761  0.659187\n",
       "2556762  2556762  0.715765\n",
       "2556763  2556763  0.507795\n",
       "2556764  2556764  0.513270\n",
       "2556765  2556765  0.659078\n",
       "2556766  2556766  0.598775\n",
       "2556767  2556767  0.535713\n",
       "2556768  2556768  0.602148\n",
       "2556769  2556769  0.339206\n",
       "2556770  2556770  0.324559\n",
       "2556771  2556771  0.496088\n",
       "2556772  2556772  0.430303\n",
       "2556773  2556773  0.641167\n",
       "2556774  2556774  0.436485\n",
       "2556775  2556775  0.358376\n",
       "2556776  2556776  0.376853\n",
       "2556777  2556777  0.365509\n",
       "2556778  2556778  0.429985\n",
       "2556779  2556779  0.257485\n",
       "2556780  2556780  0.295635\n",
       "2556781  2556781  0.257060\n",
       "2556782  2556782  0.169385\n",
       "2556783  2556783  0.514631\n",
       "2556784  2556784  0.155863\n",
       "2556785  2556785  0.115068\n",
       "2556786  2556786  0.419749\n",
       "2556787  2556787  0.417103\n",
       "2556788  2556788  0.371930\n",
       "2556789  2556789  0.364652\n",
       "\n",
       "[2556790 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.read_csv(DATASET_PATH+'submission_2017-11-06 22:25:54.csv')#'submission_2017-11-07 17:29:32.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Plot metrics during training...')\n",
    "ax = lgb.plot_metric(evals_result, metric='l1')\n",
    "plt.show()\n",
    "\n",
    "print('Plot feature importances...')\n",
    "ax = lgb.plot_importance(bst, max_num_features=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def param_tune_with_val(params, tune_param, param_list, data_list, val_data, less_prefered = False):\n",
    "    #data_list = {'train':{'x':train_d,'y':train_y}, 'validation':{'x':valid_d,'y':valid_y}}\n",
    "    best_metric = (less_prefered and sys.float_info.max or -sys.float_info.max)\n",
    "    best_param = param_list[0]\n",
    "\n",
    "    for par_value in param_list:\n",
    "        params[tune_param] = par_value\n",
    "        # , num_boost_round=params['num_boost_round'], early_stopping_rounds = params['early_stopping_rounds']\n",
    "        model = lgb.train(params, data_list['train']['x'], valid_sets=[data_list['validation']['x']], \\\n",
    "                feature_name='auto', #categorical_feature=['source_system_tab', 'source_screen_name', 'source_type', 'city', 'gender',\\\n",
    "                                     #                       'bd', 'name', 'artist_name', 'composer', 'lyricist', 'msno', 'song_id', 'genre_ids',\\\n",
    "                                     #                       'country', 'language', 'registered_via'],、\n",
    "                        )\n",
    "       \n",
    "        val_predprob = model.predict(val_data)\n",
    "        auroc_score = metrics.roc_auc_score(data_list['validation']['y'], val_predprob)\n",
    "\n",
    "        if (not less_prefered and auroc_score > best_metric) or (less_prefered and auroc_score < best_metric):\n",
    "            best_metric = auroc_score\n",
    "            best_param = par_value\n",
    "    log.info('best param for {}: {}, metric: {}'.format(tune_param, best_param, best_metric))\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#{'top_k': 20, 'feature_fraction': 0.8, 'bagging_freq': 1, 'min_data_in_bin': 3, 'min_sum_hessian_in_leaf': 0.001, 'bagging_fraction': 0.9, 'max_depth': 12, 'num_leaves': 100, 'learning_rate': 0.01, 'objective': 'binary', 'lambda_l2': 0.01, 'feature_fraction_seed': 1024, 'min_data_in_leaf': 15, 'max_bin': 100, 'verbose': 0, 'bagging_seed': 6666, 'max_cat_to_onehot': 4, 'metric': 'auc', 'lambda_l1': 1e-05, 'num_threads': 16, 'boosting': 'gbdt', 'min_split_gain': 0.3}\n",
    "\n",
    "#{'bagging_seed': 6666, 'lambda_l1': 1e-05, 'lambda_l2': 0.01, 'metric': 'auc', 'bagging_freq': 1, 'min_sum_hessian_in_leaf': 0.001, 'feature_fraction': 0.8, 'feature_fraction_seed': 1024, 'num_leaves': 90, 'boosting': 'gbdt', 'verbose': 0, 'min_data_in_leaf': 15, 'top_k': 20, 'objective': 'binary', 'min_data_in_bin': 3, 'num_threads': 16, 'max_cat_to_onehot': 4, 'max_depth': 10, 'bagging_fraction': 0.9, 'learning_rate': 0.01, 'max_bin': 80, 'min_split_gain': 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_for_best_params(train, validation, test):\n",
    "    \n",
    "    X_train = lgb.Dataset(np.array(train.drop(['target'], axis=1)), label=train['target'].values)\n",
    "    X_valid = lgb.Dataset(np.array(validation.drop(['target'], axis=1)), label=validation['target'].values)\n",
    "    \n",
    "    y_train = train['target'].values\n",
    "    y_valid = validation['target'].values\n",
    "\n",
    "    X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "    data_list = {'train':{'x':X_train,'y':y_train}, 'validation':{'x':X_valid,'y':y_valid}}\n",
    "######## for value rather than catogory ################\n",
    "#   params_to_eval = OrderedDict(\n",
    "#         ( \n",
    "#         ('num_boost_round', range(120,150,10)),\n",
    "#         ('num_leaves', range(80,100,10)), # number of leaves in one tree\n",
    "#         ('max_depth', range(8,12,1)),\n",
    "#         ('min_data_in_leaf', 15),\n",
    "#         ('min_sum_hessian_in_leaf', [0.001]),# too high will lead to under-fitting\n",
    "#         ('min_split_gain',[0.3]),# the minimum loss reduction required to make a split\n",
    "#         ('bagging_fraction',[0.9]),# [i/10.0 for i in range(6,10)]\n",
    "#         ('feature_fraction',[0.8]),# typical: 0.5-1\n",
    "#         ('max_bin', range(70,90,10)),\n",
    "#         ('lambda_l2',[0.01]),\n",
    "#         ('lambda_l1',[1e-5]),\n",
    "#         ('learning_rate',[0.01]), # typical: 0.01-0.2\n",
    "#         )\n",
    "#       )\n",
    "     \n",
    "#     initial_params = {\n",
    "#         'objective': 'binary',\n",
    "#         'boosting': 'gbdt',\n",
    "#         'num_boost_round': 140,\n",
    "#         'learning_rate': 0.01 ,\n",
    "#         'verbose': 0,\n",
    "#         'num_leaves': 90,\n",
    "#         'num_threads':16,\n",
    "#         'max_depth': 9,\n",
    "#         'min_data_in_leaf': 15, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "#         'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "#         'feature_fraction': 0.8, #colsample_bytree\n",
    "#         'feature_fraction_seed': 1024,\n",
    "#         'bagging_fraction': 0.9, #subsample\n",
    "#         'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "#         'bagging_seed': 6666,\n",
    "#         'early_stopping_rounds':10,   \n",
    "#         'lambda_l1': 1e-5, #L1 regularization\n",
    "#         'lambda_l2': 0.01, #L2 regularization\n",
    "#         'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "#         'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "#         'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "#         'max_bin': 70, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "#         'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "#         'metric' : 'auc',\n",
    "#     } \n",
    "    params_to_eval = OrderedDict(\n",
    "        ( \n",
    "        ('num_boost_round', range(100,400,50)),\n",
    "        ('num_leaves', range(80,160,10)), # number of leaves in one tree\n",
    "        ('max_depth', range(8,18,1)),\n",
    "        ('min_data_in_leaf', range(10,18,2)),\n",
    "        ('min_sum_hessian_in_leaf', [0.001]),# too high will lead to under-fitting\n",
    "        ('min_split_gain',[0.3]),# the minimum loss reduction required to make a split\n",
    "        ('bagging_fraction',[0.9]),# [i/10.0 for i in range(6,10)]\n",
    "        ('feature_fraction',[0.8]),# typical: 0.5-1\n",
    "        ('max_bin', range(80,200,10)),\n",
    "        ('lambda_l2',[0.01]),\n",
    "        ('lambda_l1',[1e-5]),\n",
    "        ('learning_rate',[0.01]), # typical: 0.01-0.2\n",
    "        )\n",
    "      )\n",
    "     \n",
    "    initial_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_boost_round': 200,\n",
    "        'learning_rate': 0.1 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 120,\n",
    "        'num_threads':16,\n",
    "        'max_depth': 14,\n",
    "        'min_data_in_leaf': 16, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.8, #colsample_bytree\n",
    "        'feature_fraction_seed': 1024,\n",
    "        'bagging_fraction': 0.9, #subsample\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "        'bagging_seed': 6666,\n",
    "        'early_stopping_rounds':10,   \n",
    "        'lambda_l1': 1e-5, #L1 regularization\n",
    "        'lambda_l2': 0.01, #L2 regularization\n",
    "        'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "        'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "        'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "        'max_bin': 140, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "        'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "        'metric' : 'auc',\n",
    "    } \n",
    "    # only param nin this list are tuned, total list are ['n_estimators', 'reg_alpha', 'reg_lambda', 'subsample', 'colsample_bytree', 'min_child_weight', 'max_depth', 'learning_rate', 'gamma']\n",
    "    #tuned_param_name = ['num_boost_round', 'num_leaves', 'max_depth', 'max_bin']\n",
    "    tuned_param_name = ['num_boost_round', 'num_leaves', 'max_depth', 'min_data_in_leaf', 'min_sum_hessian_in_leaf',\\\n",
    "                        'min_split_gain', 'bagging_fraction', 'feature_fraction', 'max_bin', 'lambda_l2', 'lambda_l1', 'learning_rate']\n",
    "    for par_name, par_list in params_to_eval.items():\n",
    "        if par_name in tuned_param_name:\n",
    "            log.info('tunning {}...'.format(par_name))\n",
    "            if len(par_list) > 1:\n",
    "                initial_params[par_name] = param_tune_with_val(initial_params, par_name, par_list, data_list, np.array(validation.drop(['target'], axis=1)))\n",
    "            else:\n",
    "                initial_params[par_name] = par_list[0]\n",
    "    \n",
    "    return initial_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "best_param = search_for_best_params(train_use, validation_use, test)\n",
    "log.info(best_param)\n",
    "time_elapsed = time.time() - start_time\n",
    "log.info('time used: {:.3f}sec'.format(time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'num_boost_round': 140,\n",
    "        'learning_rate': 0.01 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 90,\n",
    "        'num_threads':16,\n",
    "        'max_depth': 9,\n",
    "        'min_data_in_leaf': 15, #minimal number of data in one leaf. Can be used to deal with over-fitting\n",
    "        'min_sum_hessian_in_leaf': 1e-3, #minimal sum hessian in one leaf. Like min_data_in_leaf, it can be used to deal with over-fitting\n",
    "        'feature_fraction': 0.8, #colsample_bytree\n",
    "        'feature_fraction_seed': 1024,\n",
    "        'bagging_fraction': 0.9, #subsample\n",
    "        'bagging_freq': 1, #frequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration\n",
    "        'bagging_seed': 6666,\n",
    "        'early_stopping_rounds':10,   \n",
    "        'lambda_l1': 1e-5, #L1 regularization\n",
    "        'lambda_l2': 0.01, #L2 regularization\n",
    "        'max_cat_to_onehot': 4, #when number of categories of one feature smaller than or equal to max_cat_to_onehot, one-vs-other split algorithm will be used\n",
    "        'top_k': 20, #set this to larger value for more accurate result, but it will slow down the training speed\n",
    "        'min_split_gain': 0.3, #the minimal gain to perform split\n",
    "        'max_bin': 70, #max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)\n",
    "        'min_data_in_bin': 3, #min number of data inside one bin, use this to avoid one-data-one-bin (may over-fitting)       \n",
    "        'metric' : 'auc',\n",
    "    } \n",
    "X_train = lgb.Dataset(np.array(train_use.drop(['target'], axis=1)), label=train_use['target'].values)\n",
    "X_valid = lgb.Dataset(np.array(validation_use.drop(['target'], axis=1)), label=validation_use['target'].values)\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "model = lgb.train(params, X_train, valid_sets=[X_valid])\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': pred})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_use.drop(['target'], axis=1))\n",
    "y_train = train_use['target'].values\n",
    "\n",
    "X_valid = np.array(validation_use.drop(['target'], axis=1))\n",
    "y_valid = validation_use['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "# d_train = xgb.DMatrix(X_train)\n",
    "# d_valid = xgb.DMatrix(X_valid) \n",
    "# d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "data_list = {'train':{'x':X_train,'y':y_train}, 'validation':{'x':X_valid,'y':y_valid}}\n",
    "# Train model, evaluate and make predictions\n",
    "params={\n",
    "    'n_estimators':500,\n",
    "    'objective': 'binary:logistic',\n",
    "    'learning_rate': 0.75,\n",
    "    'gamma':0.1,\n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.3,\n",
    "    'min_child_weight':3,\n",
    "    'max_depth':16,\n",
    "    'seed':1024,\n",
    "    }\n",
    "\n",
    "param_tune_with_val(params, 'max_depth', [5,1,6], data_list, 'auc', 20)\n",
    "\n",
    "# model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=20, \\\n",
    "#     maximize=True, verbose_eval=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(train_use.drop(['target'], axis=1))\n",
    "y_train = train_use['target'].values\n",
    "\n",
    "X_valid = np.array(validation_use.drop(['target'], axis=1))\n",
    "y_valid = validation_use['target'].values\n",
    "\n",
    "X_test = np.array(test.drop(['id'], axis=1))\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_valid, label=y_valid) \n",
    "d_test = xgb.DMatrix(X_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "# Train model, evaluate and make predictions\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.75\n",
    "params['max_depth'] = 16\n",
    "params['silent'] = 1\n",
    "params['eval_metric'] = 'auc'\n",
    "\n",
    "model = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=20, \\\n",
    "    maximize=True, verbose_eval=5)\n",
    "\n",
    "#Predict training set:\n",
    "train_predictions = model.predict(X_train)\n",
    "train_predprob = model.predict_proba(X_train)[:,1]\n",
    "\n",
    "val_predictions = model.predict(X_valid)\n",
    "val_predprob = model.predict_proba(X_valid)[:,1]\n",
    "\n",
    "#Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"Train Accuracy : %.4g\" % metrics.accuracy_score(y_train, train_predictions))\n",
    "print(\"Train AUC Score (Train): %f\" % metrics.roc_auc_score(y_train, train_predprob))\n",
    "print(\"ValAccuracy : %.4g\" % metrics.accuracy_score(y_valid, val_predictions))\n",
    "print(\"Validation AUC Score (Train): %f\" % metrics.roc_auc_score(y_valid, val_predprob))\n",
    "\n",
    "feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "\n",
    "p_test = model.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "modelfit(xgb1, train_use.drop(['target'],axis=1), train_use['target'], validation_use.drop(['target'],axis=1), validation_use['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'], eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, train, label, validation, val_label, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train.values, label=label.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds, metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train, label, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = alg.predict(train)\n",
    "    train_predprob = alg.predict_proba(train)[:,1]\n",
    "    \n",
    "    val_predictions = alg.predict(validation)\n",
    "    val_predprob = alg.predict_proba(validation)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Train Accuracy : %.4g\" % metrics.accuracy_score(label.values, train_predictions))\n",
    "    print(\"Train AUC Score (Train): %f\" % metrics.roc_auc_score(label, train_predprob))\n",
    "    print(\"ValAccuracy : %.4g\" % metrics.accuracy_score(val_label.values, val_predictions))\n",
    "    print(\"Validation AUC Score (Train): %f\" % metrics.roc_auc_score(val_label, val_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "modelfit(xgb1, train_use.drop(['target'],axis=1), train_use['target'], validation_use.drop(['target'],axis=1), validation_use['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "\n",
    "train_data = lgb.Dataset(train_use.drop(['target'],axis=1), label=train_use['target'])\n",
    "val_data = lgb.Dataset(validation_use.drop(['target'],axis=1), label=validation_use['target'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.1 ,\n",
    "    'verbose': 0,\n",
    "    'num_leaves': 108,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_seed': 1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'feature_fraction_seed': 1,\n",
    "    'max_bin': 128,\n",
    "    'max_depth': 10,\n",
    "    'num_rounds': 200,\n",
    "    'metric' : 'auc',\n",
    "    } \n",
    "\n",
    "bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "predictions=bst.predict(test.drop(['id'],axis=1))\n",
    "print('finished.')\n",
    "\n",
    "    \n",
    "predictions = predictions/3\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "predictions = np.zeros(shape=[len(test)])\n",
    "\n",
    "for train_indices,val_indices in kf.split(train) : \n",
    "    train_data = lgb.Dataset(train.drop(['target'],axis=1).loc[train_indices,:],label=train.loc[train_indices,'target'])\n",
    "    val_data = lgb.Dataset(train.drop(['target'],axis=1).loc[val_indices,:],label=train.loc[val_indices,'target'])\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.1 ,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 108,\n",
    "        'bagging_fraction': 0.95,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': 128,\n",
    "        'max_depth': 10,\n",
    "        'num_rounds': 200,\n",
    "        'metric' : 'auc',\n",
    "        } \n",
    "    \n",
    "    bst = lgb.train(params, train_data, 100, valid_sets=[val_data])\n",
    "    predictions+=bst.predict(test.drop(['id'],axis=1))\n",
    "    print('cur fold finished.')\n",
    "    del bst\n",
    "    \n",
    "predictions = predictions/3\n",
    "\n",
    "submission = pd.DataFrame({'id': test_id, 'target': predictions})\n",
    "submission.to_csv(SUBMISSION_FILENAME.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess songs data\n",
    "songs_genres = np.array(songs['genre_ids']\\\n",
    "    .apply(lambda x: [int(v) for v in str(x).split('|')]))\n",
    "genres_list = songs_genres.ravel().unique()\n",
    "print('Number of genres: ' + str(len(genres_list)))\n",
    "\n",
    "ohe_genres = np.zeros((len(songs_genres), len(genres_list)))\n",
    "for s_i, s_genres in enumerate(songs_genres):\n",
    "    for genre in s_genres:\n",
    "        g_i = genres_list.find(genre)\n",
    "        ohe_genres[s_i, g_i] = 1\n",
    "        \n",
    "for g_i, g in enumerate(genres_list):\n",
    "    songs['genre_' + str(g)] = ohe_genres[:, g_i]\n",
    "print(songs.head())\n",
    "songs = songs.drop(['genre_ids'], axis=1)\n",
    "\n",
    "song_cols = songs.columns\n",
    "\n",
    "# Preprocess dataset\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "cols = list(train.columns)\n",
    "cols.remove('target')\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    if train[col].dtype == 'object':\n",
    "        train[col] = train[col].apply(str)\n",
    "        test[col] = test[col].apply(str)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        train_vals = list(train[col].unique())\n",
    "        test_vals = list(test[col].unique())\n",
    "        le.fit(train_vals + test_vals)\n",
    "        train[col] = le.transform(train[col])\n",
    "        test[col] = le.transform(test[col])\n",
    "\n",
    "        print(col + ': ' + str(len(train_vals)) + ', ' + str(len(test_vals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Reshape\n",
    "from keras.layers.merge import concatenate, dot\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "\n",
    "########################################\n",
    "## load the data\n",
    "########################################\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "uid = train.msno\n",
    "sid = train.song_id\n",
    "target = train.target\n",
    "\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "id_test = test.id\n",
    "uid_test = test.msno\n",
    "sid_test = test.song_id\n",
    "\n",
    "########################################\n",
    "## encoding\n",
    "########################################\n",
    "\n",
    "usr_encoder = LabelEncoder()\n",
    "usr_encoder.fit(uid.append(uid_test))\n",
    "uid = usr_encoder.transform(uid)\n",
    "uid_test = usr_encoder.transform(uid_test)\n",
    "\n",
    "sid_encoder = LabelEncoder()\n",
    "sid_encoder.fit(sid.append(sid_test))\n",
    "sid = sid_encoder.transform(sid)\n",
    "sid_test = sid_encoder.transform(sid_test)\n",
    "\n",
    "u_cnt = int(max(uid.max(), uid_test.max()) + 1)\n",
    "s_cnt = int(max(sid.max(), sid_test.max()) + 1)\n",
    "\n",
    "########################################\n",
    "## train-validation split\n",
    "########################################\n",
    "\n",
    "perm = np.random.permutation(len(train))\n",
    "trn_cnt = int(len(train) * 0.85)\n",
    "uid_trn = uid[perm[:trn_cnt]]\n",
    "uid_val = uid[perm[trn_cnt:]]\n",
    "sid_trn = sid[perm[:trn_cnt]]\n",
    "sid_val = sid[perm[trn_cnt:]]\n",
    "target_trn = target[perm[:trn_cnt]]\n",
    "target_val = target[perm[trn_cnt:]]\n",
    "\n",
    "########################################\n",
    "## define the model\n",
    "########################################\n",
    "\n",
    "def get_model():\n",
    "    user_embeddings = Embedding(u_cnt,\n",
    "            64,\n",
    "            embeddings_initializer=RandomUniform(minval=-0.1, maxval=0.1),\n",
    "            embeddings_regularizer=l2(1e-4),\n",
    "            input_length=1,\n",
    "            trainable=True)\n",
    "    song_embeddings = Embedding(s_cnt,\n",
    "            64,\n",
    "            embeddings_initializer=RandomUniform(minval=-0.1, maxval=0.1),\n",
    "            embeddings_regularizer=l2(1e-4),\n",
    "            input_length=1,\n",
    "            trainable=True)\n",
    "\n",
    "    uid_input = Input(shape=(1,), dtype='int32')\n",
    "    embedded_usr = user_embeddings(uid_input)\n",
    "    embedded_usr = Reshape((64,))(embedded_usr)\n",
    "\n",
    "    sid_input = Input(shape=(1,), dtype='int32')\n",
    "    embedded_song = song_embeddings(sid_input)\n",
    "    embedded_song = Reshape((64,))(embedded_song)\n",
    "\n",
    "    preds = dot([embedded_usr, embedded_song], axes=1)\n",
    "    preds = concatenate([embedded_usr, embedded_song, preds])\n",
    "    \n",
    "    preds = Dense(128, activation='relu')(preds)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    \n",
    "    preds = Dense(1, activation='sigmoid')(preds)\n",
    "\n",
    "    model = Model(inputs=[uid_input, sid_input], outputs=preds)\n",
    "    \n",
    "    opt = RMSprop(lr=1e-3)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "   \n",
    "model = get_model()\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=5)\n",
    "model_path = 'bst_model.h5'\n",
    "model_checkpoint = ModelCheckpoint(model_path, save_best_only=True, \\\n",
    "        save_weights_only=True)\n",
    "\n",
    "hist = model.fit([uid_trn, sid_trn], target_trn, validation_data=([uid_val, sid_val], \\\n",
    "        target_val), epochs=100, batch_size=32768, shuffle=True, \\\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "model.load_weights(model_path)\n",
    "\n",
    "preds_val = model.predict([uid_val, sid_val], batch_size=32768)\n",
    "val_auc = roc_auc_score(target_val, preds_val)\n",
    "\n",
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "\n",
    "preds_test = model.predict([uid_test, sid_test], batch_size=32768, verbose=1)\n",
    "sub = pd.DataFrame({'id': id_test, 'target': preds_test.ravel()})\n",
    "sub.to_csv('./sub_%.5f.csv'%(val_auc), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear algebra:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Graphics:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  \n",
    "# Frameworks:\n",
    "import lightgbm as lgb # LightGBM\n",
    "# Utils:\n",
    "import gc # garbage collector\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "IDIR = '../input/' # main path\n",
    "members = pd.read_csv(IDIR + 'members.csv')\n",
    "songs = pd.read_csv(IDIR + 'songs.csv')\n",
    "song_extra_info = pd.read_csv(IDIR + 'song_extra_info.csv')\n",
    "train = pd.read_csv(IDIR + 'train.csv')\n",
    "test = pd.read_csv(IDIR + 'test.csv')\n",
    "\n",
    "# Adding songs' info:\n",
    "train_aug1 = pd.merge(left=train, right=songs, on='song_id', how='left')\n",
    "test_aug1 = pd.merge(left=test, right=songs, on='song_id', how='left')\n",
    "# Adding extra info about songs:\n",
    "train_aug2 = pd.merge(left=train_aug1, right=song_extra_info, on='song_id', how='left')\n",
    "test_aug2 = pd.merge(left=test_aug1, right=song_extra_info, on='song_id', how='left')\n",
    "del train_aug1, test_aug1\n",
    "# Addind users' info:\n",
    "train_aug3 = pd.merge(left=train_aug2, right=members, on='msno', how='left')\n",
    "test_aug3 = pd.merge(left=test_aug2, right=members, on='msno', how='left')\n",
    "del train_aug2, test_aug2\n",
    "# Merging train and test data:\n",
    "train_aug3.drop(['song_id'], axis=1, inplace=True)\n",
    "train_aug3['set'] = 0\n",
    "test_aug3.drop(['song_id'], axis=1, inplace=True)\n",
    "test_aug3['set'] = 1\n",
    "test_aug3['target'] = -1\n",
    "all_aug = pd.concat([train_aug3, test_aug3], axis=0)\n",
    "del train_aug3, test_aug3\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "\n",
    "# source_system_tab/source_screen_name/source_type/genre_ids/artist_name/composer/lyricist/name/isrc/gender 用'NA'填补并one-hot编码\n",
    "# genre_ids encoding:\n",
    "all_aug['genre_ids'] = all_aug.genre_ids.fillna('NA')\n",
    "all_aug['genre_ids'] = all_aug.genre_ids.astype(np.str)\n",
    "genre_ids_le = LabelEncoder()\n",
    "genre_ids_le.fit(all_aug.genre_ids)\n",
    "all_aug['genre_ids'] = genre_ids_le.transform(all_aug.genre_ids).astype(np.int16)\n",
    "\n",
    "# language encoding:\n",
    "all_aug['language'] = all_aug.language.fillna(-2)\n",
    "all_aug['language'] = all_aug.language.astype(np.int8)\n",
    "\n",
    "# city encoding:\n",
    "all_aug['city'] = all_aug.city.astype(np.int8)\n",
    "# bd encoding:\n",
    "all_aug['bd'] = all_aug.bd.astype(np.int16)\n",
    "\n",
    "# registered_via encoding:\n",
    "all_aug['registered_via'] = all_aug.registered_via.astype(np.int8)\n",
    "# registration_init_time encoding:\n",
    "all_aug['registration_init_time'] = all_aug.registration_init_time.astype(np.int32)\n",
    "# expiration_date encoding:\n",
    "all_aug['expiration_date'] = all_aug.expiration_date.astype(np.int32)\n",
    "# Info:\n",
    "all_aug.info(max_cols=0)\n",
    "all_aug.head(2)\n",
    "\n",
    "\n",
    "all_aug['exp_reg_time'] = all_aug.expiration_date - all_aug.registration_init_time\n",
    "\n",
    "\n",
    "\n",
    "gc.collect();\n",
    "d_train = lgb.Dataset(all_aug[all_aug.set == 0].drop(['target', 'msno', 'id', 'set'], axis=1), \n",
    "                      label=all_aug[all_aug.set == 0].pop('target'))\n",
    "ids_train = all_aug[all_aug.set == 0].pop('msno')\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 1.0,\n",
    "    'max_depth': 15,\n",
    "    'num_leaves': 250, \n",
    "    'objective': 'binary',\n",
    "    'metric': {'auc'},\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.75,\n",
    "    'bagging_freq': 5,\n",
    "    'max_bin': 100}\n",
    "cv_result_lgb = lgb.cv(lgb_params, \n",
    "                       d_train, \n",
    "                       num_boost_round=5000, \n",
    "                       nfold=3, \n",
    "                       stratified=True, \n",
    "                       early_stopping_rounds=50, \n",
    "                       verbose_eval=100, \n",
    "                       show_stdv=True)\n",
    "\n",
    "num_boost_rounds_lgb = len(cv_result_lgb['auc-mean'])\n",
    "print('num_boost_rounds_lgb=' + str(num_boost_rounds_lgb))\n",
    "\n",
    "\n",
    "\n",
    "%%time\n",
    "ROUNDS = num_boost_rounds_lgb\n",
    "print('light GBM train :-)')\n",
    "bst = lgb.train(lgb_params, d_train, ROUNDS)\n",
    "# lgb.plot_importance(bst, figsize=(9,20))\n",
    "# del d_train\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "feature_imp = pd.Series(dict(zip(d_train.feature_name, \n",
    "                                 bst.feature_importance()))).sort_values(ascending=False)\n",
    "sns.barplot(x=feature_imp.values, y=feature_imp.index.values, orient='h', color='g')\n",
    "plt.subplot(1,2,2)\n",
    "train_scores = np.array(cv_result_lgb['auc-mean'])\n",
    "train_stds = np.array(cv_result_lgb['auc-stdv'])\n",
    "plt.plot(train_scores, color='green')\n",
    "plt.fill_between(range(len(cv_result_lgb['auc-mean'])), \n",
    "                 train_scores - train_stds, train_scores + train_stds, \n",
    "                 alpha=0.1, color='green')\n",
    "plt.title('LightGMB CV-results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
