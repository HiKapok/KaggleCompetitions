{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n"
     ]
    }
   ],
   "source": [
    "# Running %env without any arguments\n",
    "# lists all environment variables\n",
    "\n",
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "%env CUDA_VISIBLE_DEVICES = \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "import bson                       # this is installed with the pymongo package\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imsave, imshow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging\n",
    "import os.path\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.python.slim.nets import inception\n",
    "import inception_preprocessing\n",
    "import logging\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/'\n",
    "PRETRAINED_MODEL_PATH = DATASET_PATH + 'logs/before/inception_v3_model.ckpt-917169'\n",
    "LOG_PATH = DATASET_PATH + 'logs/'\n",
    "TRAIN_PATH = DATASET_PATH + 'Split1/Train/'\n",
    "VAL_PATH = DATASET_PATH + 'Split1/Validation/'\n",
    "TEST_PATH = DATASET_PATH + 'Test/'\n",
    "CATEGORY_NAME_PATH = DATASET_PATH + 'category_names.csv'\n",
    "BATCH_SIZE = 2\n",
    "IMAGE_WIDTH = 180\n",
    "IMAGE_HEIGHT = 180\n",
    "NUM_CLASS = 5270\n",
    "# validation examples num: 2319624\n",
    "# train examples num: 10051704\n",
    "# total step: 157057\n",
    "TOTAL_EXAMPLES = 10051704\n",
    "# validation num = 2319624\n",
    "NUM_EPOCHES = 7\n",
    "INPUT_THREADS = 6\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "# initial_learning_rate = 0.000003#0.00001\n",
    "# learning_rate_decay_factor = 0.94\n",
    "initial_learning_rate = 0.001#0.00001\n",
    "learning_rate_decay_factor = 0.8\n",
    "num_epochs_before_decay = 1\n",
    "momentum = 0.4\n",
    "#Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "num_steps_per_epoch = TOTAL_EXAMPLES / BATCH_SIZE\n",
    "decay_steps = int(num_epochs_before_decay * num_steps_per_epoch / 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get TF logger\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(DATASET_PATH + 'tensorflow_inception_train.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_inception(input_image, is_training = True):\n",
    "    # inception_v3.default_image_size = 299\n",
    "    return inception_preprocessing.preprocess_image(input_image, 299, 299, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabelMapping(object):\n",
    "    def __init__(self, catogory_file_path):\n",
    "        super(LabelMapping, self).__init__()\n",
    "        self._category_level_csv = catogory_file_path\n",
    "        self._category_map, self._category_level0_map, self._category_level1_map, self._len_level0, self._len_level1 = self.cvt_csv2tfrecord()\n",
    "        self._mapping_strings = tf.constant( [ str(key) for key in self._category_map.keys() ] )\n",
    "        #print(list(self._category_map.keys())[0])\n",
    "        self._mapping_table = tf.contrib.lookup.index_table_from_tensor(mapping=self._mapping_strings, default_value=0) \n",
    "        \n",
    "        self._level0_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._category_level0_map.keys()), list(self._category_level0_map.values()), tf.int64, tf.int64), 0)\n",
    "        self._level1_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._category_level1_map.keys()), list(self._category_level1_map.values()), tf.int64, tf.int64), 0)\n",
    "\n",
    "    @property\n",
    "    def category_map(self):\n",
    "        return self._category_map\n",
    "    @property\n",
    "    def level0_table(self):\n",
    "        return self._level0_table\n",
    "    @property\n",
    "    def level1_table(self):\n",
    "        return self._level1_table\n",
    "    @property\n",
    "    def len_level0(self):\n",
    "        return self._len_level0\n",
    "    @property\n",
    "    def len_level1(self):\n",
    "        return self._len_level1\n",
    "    @property\n",
    "    def mapping_table(self):\n",
    "        return self._mapping_table\n",
    "    \n",
    "    def cvt_csv2tfrecord(self):\n",
    "        level0_map, level1_map = self.create_level_map()\n",
    "        count = 0\n",
    "        category_map = dict()\n",
    "        category_level0_map = dict()\n",
    "        category_level1_map = dict()\n",
    "        csv = pd.read_csv(self._category_level_csv).values\n",
    "        for row in csv:  \n",
    "            category_id, level0, level1 = row[0], row[1], row[2]\n",
    "            category_map[category_id] = count\n",
    "            category_level0_map[int(category_id)] = level0_map[level0]\n",
    "            category_level1_map[int(category_id)] = level1_map[level1]\n",
    "            count += 1\n",
    "            \n",
    "        return category_map, category_level0_map, category_level1_map, len(level0_map), len(level1_map)\n",
    "\n",
    "    def create_level_map(self):\n",
    "        csv = pd.read_csv(self._category_level_csv).values\n",
    "        level_list = [list(), list()]\n",
    "        for row in csv: \n",
    "            for level in range(1,3):\n",
    "                if row[level] not in level_list[level-1]:\n",
    "                    level_list[level-1].append(row[level])\n",
    "        return dict(zip(level_list[0], range(len(level_list[0])))), dict(zip(level_list[1], range(len(level_list[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CdiscountDataset(object):\n",
    "    def __init__(self, data_path, file_begin_match, label_mapping, num_examples, num_classes, buffer_size, batch_size, num_epochs, is_training):\n",
    "        super(CdiscountDataset, self).__init__()\n",
    "        self._data_file_list = [ os.path.join(data_path, x) for x in os.listdir(data_path) if lambda x: os.path.isfile(x) and x.startswith(file_begin_match) ]\n",
    "        self._num_examples = num_examples\n",
    "        self._num_classes = num_classes\n",
    "        self._batch_size = batch_size\n",
    "        self._buffer_size = buffer_size\n",
    "        self._num_epochs = num_epochs\n",
    "        self._is_training = is_training\n",
    "        self._category_map = label_mapping.category_map\n",
    "        self._level0_table = label_mapping.level0_table\n",
    "        self._level1_table = label_mapping.level1_table\n",
    "        self._len_level0 = label_mapping.len_level0\n",
    "        self._len_level1 = label_mapping.len_level1\n",
    "        self._mapping_table = label_mapping.mapping_table\n",
    "    \n",
    "    def _parse_function(self, example_proto):\n",
    "        features = {'img_raw': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "            'product_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            'category_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64))}\n",
    "                \n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        image = preprocess_for_inception(tf.image.decode_image(parsed_features[\"img_raw\"]), self._is_training)\n",
    "        raw_label = parsed_features[\"category_id\"]\n",
    "        #raw_label = tf.constant(1000018736, dtype=tf.int64)\n",
    "        #image = tf.image.decode_image(parsed_features[\"img_raw\"])\n",
    "        \n",
    "        return image, tf.one_hot(self._mapping_table.lookup(tf.as_string(raw_label)), self._num_classes, axis=-1),\\\n",
    "                tf.one_hot(self._level0_table.lookup(raw_label), self._len_level0, axis=-1),\\\n",
    "                tf.one_hot(self._level1_table.lookup(raw_label), self._len_level0, axis=-1)\n",
    "    \n",
    "    def get_next(self):\n",
    "        #next_example, next_label, next_level0_label, next_level1_label \n",
    "        return self._iterator.get_next()\n",
    "    def create_dataset(self):\n",
    "        self._dataset = tf.data.TFRecordDataset(self._data_file_list, compression_type='ZLIB', buffer_size = 409600)\n",
    "        parse_func = lambda example : self._parse_function(example)\n",
    "        self._dataset = self._dataset.map(parse_func)\n",
    "        self._dataset = self._dataset.prefetch(self._batch_size)\n",
    "        self._dataset = self._dataset.shuffle(buffer_size=self._buffer_size)\n",
    "        #self._dataset = self._dataset.batch(self._batch_size)\n",
    "        self._dataset = self._dataset.repeat(self._num_epochs)\n",
    "        self._iterator = self._dataset.make_initializable_iterator()\n",
    "#         map(\n",
    "#             map_func,\n",
    "#             num_threads=None,\n",
    "#             output_buffer_size=None,\n",
    "#             num_parallel_calls=None\n",
    "#         )\n",
    "#         Maps map_func across this datset. (deprecated arguments)\n",
    "\n",
    "#         SOME ARGUMENTS ARE DEPRECATED. They will be removed in a future version. Instructions for updating: Replace num_threads=T with num_parallel_calls=T. Replace output_buffer_size=N with ds.prefetch(N) on the returned dataset.\n",
    "\n",
    "#         Args:\n",
    "\n",
    "#         map_func: A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.\n",
    "#         num_threads: (Optional.) Deprecated, use num_parallel_calls instead.\n",
    "#         output_buffer_size: (Optional.) A tf.int64 scalar tf.Tensor, representing the maximum number of processed elements that will be buffered.\n",
    "#         num_parallel_calls: (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process in parallel. If not specified, elements will be processed sequentially.\n",
    "        return self._iterator.initializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000018736\n",
      "(array([[[[ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         ..., \n",
      "         [-0.22280616, -0.16608578, -0.0850566 ],\n",
      "         [-0.22280616, -0.16608578, -0.0850566 ],\n",
      "         [-0.22280616, -0.16608578, -0.0850566 ]],\n",
      "\n",
      "        [[ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         ..., \n",
      "         [-0.24714875, -0.19042832, -0.1093992 ],\n",
      "         [-0.24714875, -0.19042832, -0.1093992 ],\n",
      "         [-0.24714875, -0.19042832, -0.1093992 ]],\n",
      "\n",
      "        [[ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         ..., \n",
      "         [-0.27149135, -0.21477097, -0.13374174],\n",
      "         [-0.27149135, -0.21477097, -0.13374174],\n",
      "         [-0.27149135, -0.21477097, -0.13374174]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         ..., \n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308]],\n",
      "\n",
      "        [[ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         ..., \n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308]],\n",
      "\n",
      "        [[ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         ..., \n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308],\n",
      "         [ 0.98945308,  0.98945308,  0.98945308]]],\n",
      "\n",
      "\n",
      "       [[[ 0.35585725,  0.36114895,  0.33469033],\n",
      "         [ 0.36404145,  0.36933315,  0.34287465],\n",
      "         [ 0.37222552,  0.37751722,  0.35105872],\n",
      "         ..., \n",
      "         [-0.66998672, -0.5959028 , -0.5800277 ],\n",
      "         [-0.66998672, -0.5959028 , -0.5800277 ],\n",
      "         [-0.66998672, -0.5959028 , -0.5800277 ]],\n",
      "\n",
      "        [[ 0.33460987,  0.33990169,  0.31344306],\n",
      "         [ 0.34279406,  0.34808588,  0.32162726],\n",
      "         [ 0.35097826,  0.35626996,  0.32981133],\n",
      "         ..., \n",
      "         [-0.67211151, -0.59802759, -0.58215237],\n",
      "         [-0.67211151, -0.59802759, -0.58215237],\n",
      "         [-0.67211151, -0.59802759, -0.58215237]],\n",
      "\n",
      "        [[ 0.3133626 ,  0.3186543 ,  0.2921958 ],\n",
      "         [ 0.32154667,  0.32683849,  0.30037987],\n",
      "         [ 0.32973087,  0.33502269,  0.30856407],\n",
      "         ..., \n",
      "         [-0.67423618, -0.60015225, -0.58427715],\n",
      "         [-0.67423618, -0.60015225, -0.58427715],\n",
      "         [-0.67423618, -0.60015225, -0.58427715]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.57546508,  0.58075678,  0.54900658],\n",
      "         [ 0.58569527,  0.59098697,  0.55923665],\n",
      "         [ 0.59592545,  0.60121715,  0.56946683],\n",
      "         ..., \n",
      "         [ 0.80036449,  0.80036449,  0.80036449],\n",
      "         [ 0.80036449,  0.80036449,  0.80036449],\n",
      "         [ 0.80036449,  0.80036449,  0.80036449]],\n",
      "\n",
      "        [[ 0.57546508,  0.58075678,  0.54900658],\n",
      "         [ 0.58569527,  0.59098697,  0.55923665],\n",
      "         [ 0.59592545,  0.60121715,  0.56946683],\n",
      "         ..., \n",
      "         [ 0.80036449,  0.80036449,  0.80036449],\n",
      "         [ 0.80036449,  0.80036449,  0.80036449],\n",
      "         [ 0.80036449,  0.80036449,  0.80036449]],\n",
      "\n",
      "        [[ 0.57546508,  0.58075678,  0.54900658],\n",
      "         [ 0.58569527,  0.59098697,  0.55923665],\n",
      "         [ 0.59592545,  0.60121715,  0.56946683],\n",
      "         ..., \n",
      "         [ 0.80036449,  0.80036449,  0.80036449],\n",
      "         [ 0.80036449,  0.80036449,  0.80036449],\n",
      "         [ 0.80036449,  0.80036449,  0.80036449]]]], dtype=float32), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32))\n",
      "(array([[[[ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         ..., \n",
      "         [ 0.58536685,  0.58536685,  0.58536685],\n",
      "         [ 0.58536685,  0.58536685,  0.58536685],\n",
      "         [ 0.58536685,  0.58536685,  0.58536685]],\n",
      "\n",
      "        [[ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         ..., \n",
      "         [ 0.58536685,  0.58536685,  0.58536685],\n",
      "         [ 0.58536685,  0.58536685,  0.58536685],\n",
      "         [ 0.58536685,  0.58536685,  0.58536685]],\n",
      "\n",
      "        [[ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         ..., \n",
      "         [ 0.58536685,  0.58536685,  0.58536685],\n",
      "         [ 0.58536685,  0.58536685,  0.58536685],\n",
      "         [ 0.58536685,  0.58536685,  0.58536685]],\n",
      "\n",
      "        ..., \n",
      "        [[ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         ..., \n",
      "         [-0.3986395 , -0.36670816, -0.37735194],\n",
      "         [-0.40035129, -0.36842   , -0.37906373],\n",
      "         [-0.40206319, -0.37013185, -0.38077563]],\n",
      "\n",
      "        [[ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         ..., \n",
      "         [-0.39928275, -0.36735141, -0.37799519],\n",
      "         [-0.40135503, -0.36942363, -0.38006747],\n",
      "         [-0.40342724, -0.37149596, -0.38213968]],\n",
      "\n",
      "        [[ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         [ 1.        ,  1.        ,  1.        ],\n",
      "         ..., \n",
      "         [-0.39928275, -0.36735141, -0.37799519],\n",
      "         [-0.40135503, -0.36942363, -0.38006747],\n",
      "         [-0.40342724, -0.37149596, -0.38213968]]],\n",
      "\n",
      "\n",
      "       [[[-0.92073411, -0.92073411, -0.92073411],\n",
      "         [-0.92073411, -0.92073411, -0.92073411],\n",
      "         [-0.92073411, -0.92073411, -0.92073411],\n",
      "         ..., \n",
      "         [-0.80728412, -0.92623913, -1.        ],\n",
      "         [-0.80308706, -0.92857724, -1.        ],\n",
      "         [-0.80308706, -0.92857724, -1.        ]],\n",
      "\n",
      "        [[-0.92073411, -0.92073411, -0.92073411],\n",
      "         [-0.92073411, -0.92073411, -0.92073411],\n",
      "         [-0.92073411, -0.92073411, -0.92073411],\n",
      "         ..., \n",
      "         [-0.74473166, -0.88668221, -1.        ],\n",
      "         [-0.73680073, -0.88671219, -1.        ],\n",
      "         [-0.73680073, -0.88671219, -1.        ]],\n",
      "\n",
      "        [[-0.92073411, -0.92073411, -0.92073411],\n",
      "         [-0.92073411, -0.92073411, -0.92073411],\n",
      "         [-0.92073411, -0.92073411, -0.92073411],\n",
      "         ..., \n",
      "         [-0.68217915, -0.84685129, -1.        ],\n",
      "         [-0.6705144 , -0.84484714, -1.        ],\n",
      "         [-0.6705144 , -0.84484714, -1.        ]],\n",
      "\n",
      "        ..., \n",
      "        [[ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         ..., \n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505]],\n",
      "\n",
      "        [[ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         ..., \n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505]],\n",
      "\n",
      "        [[ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         ..., \n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505],\n",
      "         [ 0.97730505,  0.97730505,  0.97730505]]]], dtype=float32), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "label_mapping = LabelMapping(CATEGORY_NAME_PATH)\n",
    "train_dataset = CdiscountDataset(TRAIN_PATH, 'output_file', label_mapping, TOTAL_EXAMPLES, NUM_CLASS, 2000, BATCH_SIZE, NUM_EPOCHES, True)\n",
    "val_dataset = CdiscountDataset(VAL_PATH, 'test_output_file', label_mapping, TOTAL_EXAMPLES, NUM_CLASS, 2000, BATCH_SIZE, 1, True)\n",
    "train_iterator_initializer = train_dataset.create_dataset()\n",
    "val_iterator_initializer = val_dataset.create_dataset()\n",
    "init_op = tf.group(train_iterator_initializer, val_iterator_initializer, tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer())\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "print(sess.run(train_dataset.get_next()))\n",
    "print(sess.run(val_dataset.get_next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
