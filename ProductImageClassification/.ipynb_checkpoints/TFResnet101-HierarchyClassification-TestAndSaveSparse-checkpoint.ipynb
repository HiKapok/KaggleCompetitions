{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n"
     ]
    }
   ],
   "source": [
    "# Running %env without any arguments\n",
    "# lists all environment variables\n",
    "\n",
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "%env CUDA_VISIBLE_DEVICES = \n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "import bson                       # this is installed with the pymongo package\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imsave, imshow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.training import add_gradients_summaries\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.training import optimizer as tf_optimizer\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "import os.path\n",
    "import tensorflow.contrib.slim as slim\n",
    "import inception_preprocessing\n",
    "import logging\n",
    "import resnet2\n",
    "from scipy.sparse import *\n",
    "import tables as tb\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/'\n",
    "LOG_PATH = DATASET_PATH + 'Resnet/logs101-hierarchy/'\n",
    "TEST_PATH = DATASET_PATH + 'Test/'\n",
    "\n",
    "OUTPUT_PATH = DATASET_PATH + 'Resnet/output_{}.csv'\n",
    "CATEGORY_NAME_PATH = DATASET_PATH + 'category_names.csv'\n",
    "RESNET_MODEL_PATH = DATASET_PATH + 'Resnet/logs101-hierarchy/resnet101_v2_model.ckpt-0'\n",
    "PROB_SAVE_FILE = DATASET_PATH + 'Resnet/probs_{}.h5'\n",
    "ID_SAVE_FILE = DATASET_PATH + 'Resnet/logs101-hierarchy/ids.csv'\n",
    "\n",
    "NUM_OF_TOPK = 20\n",
    "BATCH_SIZE = 2#256#256\n",
    "IMAGE_WIDTH = 180\n",
    "IMAGE_HEIGHT = 180\n",
    "NUM_CLASS = 5270\n",
    "\n",
    "LEVEL1_CLASS = 49\n",
    "\n",
    "CATEGORY_ENCODE_PATH = DATASET_PATH + 'hierarchy_encode.csv'\n",
    "\n",
    "LEVEL1_NUM_LIST = [555, 441, 440, 237, 230, 230, 220, 206, 196, 184, 180, 162, 158, 137, 106, 104, 103, 101, 99, 89, 88, 85, 84, 83, 81, 74, 57, 50, 48, 45, 43, 42, 42, 38, 33, 33, 30, 29, 26, 25, 19, 16, 9, 6, 2, 1, 1, 1, 1]\n",
    "\n",
    "# validation examples num: 2319624\n",
    "# train examples num: 10051704\n",
    "# total step: 157057\n",
    "TOTAL_EXAMPLES = 3095080\n",
    "INPUT_THREADS = 1\n",
    "NUM_STEPS = int(TOTAL_EXAMPLES / BATCH_SIZE) + 1\n",
    "\n",
    "moving_average_decay = 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get TF logger\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(DATASET_PATH + 'tensorflow_resnet_test.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MiniDataSet(object):\n",
    "    def __init__(self, file_path_pattern, num_examples, num_classes, is_training = True, min_after_dequeue=1000, batch_size = BATCH_SIZE, num_reader = INPUT_THREADS):\n",
    "        super(MiniDataSet, self).__init__()\n",
    "        self._num_examples = num_examples\n",
    "        self._num_classes = num_classes\n",
    "        self._file_path_pattern = file_path_pattern\n",
    "        self._num_reader = num_reader\n",
    "        self._batch_size = batch_size\n",
    "        self._min_after_dequeue = min_after_dequeue\n",
    "        self._is_training = is_training\n",
    "        \n",
    "    def create_dataset(self):\n",
    "        opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "        reader = lambda : tf.TFRecordReader(options=opts)\n",
    "        keys_to_features = {\n",
    "            'product_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            'img_raw': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "            # notice that we don't have this feature in our TFRecord, so always default provided\n",
    "            'format': tf.FixedLenFeature([], tf.string, default_value='jpg')\n",
    "        }\n",
    "\n",
    "        items_to_handlers = {\n",
    "            # automated decode image from features in FixedLenFeature\n",
    "            'image': slim.tfexample_decoder.Image(image_key='img_raw', format_key='format'),\n",
    "            'product': slim.tfexample_decoder.Tensor('product_id'),\n",
    "        }\n",
    "\n",
    "        decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "        self._dataset = slim.dataset.Dataset(\n",
    "            data_sources = self._file_path_pattern,\n",
    "            decoder = decoder,\n",
    "            reader = reader,\n",
    "            # num_readers = 8,\n",
    "            num_samples = self._num_examples,\n",
    "            #num_classes = self._num_classes,\n",
    "            items_to_descriptions = None)\n",
    "        \n",
    "        # notice that DatasetDataProvider can automate shuffle the examples by ParallelReader using its RandomShuffleQueue\n",
    "        self._data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            self._dataset,\n",
    "            num_readers = self._num_reader,\n",
    "            shuffle = False, # default is True\n",
    "            num_epochs = 1,\n",
    "            common_queue_capacity = self._min_after_dequeue + 3 * self._batch_size,\n",
    "            common_queue_min = self._min_after_dequeue,\n",
    "            scope = 'test_files')\n",
    "        \n",
    "        return self._data_provider.get(['image', 'product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def_graph = tf.Graph()\n",
    "with def_graph.as_default() as graph:   \n",
    "    def test_step(input_examples):  \n",
    "        # inputs has shape [batch, 224, 224, 3]\n",
    "        with slim.arg_scope(resnet2.resnet_arg_scope()):\n",
    "            logits, end_points = resnet2.resnet_v2_101(input_examples, None, is_training=False) \n",
    "\n",
    "            end_points_list = list()\n",
    "            for softmax_index, num_classes in enumerate(LEVEL1_NUM_LIST):\n",
    "                if num_classes > 1:\n",
    "                    net = tf.squeeze(layers.conv2d(\n",
    "                          logits,\n",
    "                          num_classes, [1, 1],\n",
    "                          activation_fn=None,\n",
    "                          normalizer_fn=None,\n",
    "                          scope='logits_{}'.format(softmax_index)))\n",
    "                    end_points_list.append(tf.nn.softmax(net, name='predictions_{}'.format(softmax_index)))\n",
    "                else:\n",
    "                    net = tf.squeeze(layers.conv2d(\n",
    "                          logits,\n",
    "                          num_classes, [1, 1],\n",
    "                          activation_fn=None,\n",
    "                          normalizer_fn=None,\n",
    "                          scope='logits_{}'.format(softmax_index)))\n",
    "                    end_points_list.append(tf.ones_like(tf.expand_dims(net, 1), dtype=tf.float32, optimize=False))\n",
    "\n",
    "            net = tf.squeeze(layers.conv2d(\n",
    "                  logits,\n",
    "                  LEVEL1_CLASS, [1, 1],\n",
    "                  activation_fn=None,\n",
    "                  normalizer_fn=None,\n",
    "                  scope='logits_level1'))\n",
    "            level1_prob = tf.nn.softmax(net, name='predictions_level1')\n",
    "            \n",
    "            indices = tf.constant([index for index in range(len(LEVEL1_NUM_LIST)) for  _ in range(LEVEL1_NUM_LIST[index])], name='gather_indices')\n",
    "            class_prob = tf.multiply(tf.concat(end_points_list, 1), tf.gather(level1_prob, indices, axis = 1))\n",
    "            predictions = tf.argmax(class_prob, 1)\n",
    "\n",
    "            variable_averages = tf.train.ExponentialMovingAverage(moving_average_decay)\n",
    "            variables_to_restore = variable_averages.variables_to_restore()\n",
    "            #variables_to_restore = slim.get_variables_to_restore()\n",
    "\n",
    "            return predictions, class_prob, variables_to_restore\n",
    "    def preprocess_for_inception(input_image):\n",
    "        return inception_preprocessing.preprocess_image(input_image, 160, 160, False)\n",
    "    def init_dataset(file_path_pattern):\n",
    "        dataset = MiniDataSet(file_path_pattern, TOTAL_EXAMPLES, NUM_CLASS)\n",
    "        org_image, product_id = dataset.create_dataset()\n",
    "        image = preprocess_for_inception(org_image)\n",
    "        batch_images, batch_id = tf.train.batch([image, product_id], BATCH_SIZE,\\\n",
    "                                            num_threads = INPUT_THREADS,\\\n",
    "                                            capacity = 1000 + 3 * BATCH_SIZE,\\\n",
    "                                            allow_smaller_final_batch = True, name = 'test_batch')\n",
    "        \n",
    "        return batch_images, batch_id\n",
    "    class LabelMapping(object):\n",
    "        def __init__(self, catogory_file_path):\n",
    "            super(LabelMapping, self).__init__()\n",
    "            self._category_encode_csv = catogory_file_path\n",
    "            self._total_onehot_to_catogory = self.cvt_csv2tfrecord()\n",
    "            self._to_catogory_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._total_onehot_to_catogory.keys()), list(self._total_onehot_to_catogory.values()), tf.int64, tf.int64), 1000000000)\n",
    "\n",
    "        @property\n",
    "        def to_catogory_table(self):\n",
    "            return self._to_catogory_table\n",
    "\n",
    "        def cvt_csv2tfrecord(self):\n",
    "            csv = pd.read_csv(self._category_encode_csv)[['category_id', 'catogoty_encode_total']].values\n",
    "\n",
    "            total_onehot_to_catogory = dict()\n",
    "            for row in csv:  \n",
    "                category_id, total_onehot= row[0], row[1]\n",
    "                total_onehot_to_catogory[int(total_onehot)] = int(category_id)\n",
    "\n",
    "            return total_onehot_to_catogory\n",
    "    def slices_to_dims(slice_indices):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        slice_indices: An [N, k] Tensor mapping to column indices.\n",
    "        Returns:\n",
    "        An index Tensor with shape [N * k, 2], corresponding to indices suitable for\n",
    "        passing to SparseTensor.\n",
    "        \"\"\"\n",
    "        slice_indices = tf.cast(slice_indices, tf.int64)\n",
    "        num_rows = tf.shape(slice_indices, out_type=tf.int64)[0]\n",
    "        row_range = tf.range(num_rows)\n",
    "        # row_range expanded from [num_rows] into [num_rows, 1]\n",
    "        # every item in k_th row of slice_indices are multiplied by num_rows, then added by k with broadcast\n",
    "        item_numbers = slice_indices * num_rows + tf.expand_dims(row_range, axis=1)\n",
    "        # flaten so that each row represent each element\n",
    "        item_numbers_flat = tf.reshape(item_numbers, [-1])\n",
    "        # convert back by zip op\n",
    "        return item_numbers_flat % num_rows, item_numbers_flat // num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-17 23:11:21\n",
      "INFO:tensorflow:Restoring parameters from /media/rs/0E06CD1706CD0127/Kapok/kaggle/Resnet/logs101-hierarchy/resnet101_v2_model.ckpt-0\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Step: 0 of 1547541.\n",
      "INFO:tensorflow:Validation Speed: 1.964sec/batch\n",
      "INFO:tensorflow:Roughly 844.247 hours to go.\n",
      "INFO:tensorflow:Test Label: []\n",
      "INFO:tensorflow:Test Prob: []\n",
      "INFO:tensorflow:Step: 200 of 1547541.\n",
      "INFO:tensorflow:Validation Speed: 0.873sec/batch\n",
      "INFO:tensorflow:Roughly 375.373 hours to go.\n",
      "INFO:tensorflow:Test Label: [1000021794]\n",
      "INFO:tensorflow:Test Prob: [[  3.95765528e-06   1.42430645e-06   1.70821784e-06 ...,   1.33303944e-02\n",
      "    4.60331189e-03   1.94651075e-02]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d66b336a621c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m#test_pred, test_prob, test_batch_id = sess.run([test_predictions_values, test_probabilities, batch_id])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mcur_batch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_feed_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_feed_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlats_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_batch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_id_tail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_prob_tail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtail_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_id_tail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_indice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_indice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_prob_shape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlast_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlast_feed_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlast_feed_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0;31m#print(csr_matrix((sparse_value, (sparse_row, sparse_col)), shape=sparse_shape).toarray())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m#total_prob_store[last_row_to_save:last_row_to_save + sparse_shape[0],:] = csr_matrix((sparse_value, (sparse_row, sparse_col)), shape=sparse_shape).toarray()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with def_graph.as_default() as graph:\n",
    "    inv_table = LabelMapping(CATEGORY_ENCODE_PATH).to_catogory_table\n",
    "    \n",
    "#     mapping_strings = tf.constant( [ str(key) for key in cvt_csv2tfrecord().keys() ] )\n",
    "#     mapping_table = tf.contrib.lookup.index_table_from_tensor(mapping=mapping_strings, default_value=0)\n",
    "    \n",
    "#     inv_table = tf.contrib.lookup.index_to_string_table_from_tensor(mapping_strings, default_value=\"0000000000\")\n",
    "\n",
    "    batch_images, batch_id = init_dataset(TEST_PATH + \"output_file*.tfrecords\")#test_output_file6.tfrecords\n",
    "    #batch_images, batch_id = init_dataset(TEST_PATH+'test_output_file6*')\n",
    "    \n",
    "    # use placeholder instead\n",
    "    #last_prob = tf.constant(0, shape=[0,NUM_CLASS], dtype=tf.float32)\n",
    "    #last_id = tf.constant(0, shape=[0], dtype=tf.int64)  \n",
    "    last_prob = tf.placeholder(tf.float32)\n",
    "    last_id = tf.placeholder(tf.int64)\n",
    "    with tf.device('/gpu:0'):\n",
    "        test_predictions, test_probabilities, variables_to_restore = test_step(batch_images)\n",
    "        test_predictions_values = inv_table.lookup(test_predictions)\n",
    "        \n",
    "        top_values, top_indices = tf.nn.top_k(test_probabilities, k = NUM_OF_TOPK, sorted=True)\n",
    "        (row_indice, col_indice), value_array = slices_to_dims(top_indices), tf.reshape(top_values, [-1])\n",
    "        cur_prob_shape = tf.shape(test_probabilities)\n",
    "        # concat betweent batches\n",
    "        _, idx, count = tf.unique_with_counts(batch_id)\n",
    "        #print(tf.dynamic_partition(batch_id, tf.not_equal(idx, tf.shape(count)[0] - 1).eval(), 2)[1].eval())  \n",
    "        cur_id_tail, _cur_id_head = tf.dynamic_partition(batch_id, tf.cast(tf.not_equal(idx, tf.shape(count)[0] - 1), tf.int32), 2)\n",
    "        with tf.control_dependencies([cur_id_tail, _cur_id_head]):\n",
    "            cur_id_head = tf.concat([last_id, _cur_id_head], axis = 0)\n",
    "        #cur_id_head = tf.concat([last_id, tf.concat(tf.split(batch_id, count)[0:-1], axis = 0)], axis = 0)\n",
    "        #cur_id_tail = tf.split(batch_id, count)[-1]\n",
    "        \n",
    "        cur_prob_tail, _cur_prob_head = tf.dynamic_partition(test_probabilities, tf.cast(tf.not_equal(idx, tf.shape(count)[0] - 1), tf.int32), 2)\n",
    "        with tf.control_dependencies([last_prob, _cur_prob_head]):\n",
    "            cur_prob_head = tf.concat([last_prob, _cur_prob_head], axis = 0)\n",
    "        #cur_prob_head = tf.concat([last_prob, tf.concat(tf.split(test_probabilities, count[0:-1]), axis = 0)], axis = 0)\n",
    "        #cur_prob_tail = tf.split(test_probabilities, count)[-1]\n",
    "        with tf.control_dependencies([cur_id_head, cur_prob_head]):\n",
    "            raw_id, idx, _ = tf.unique_with_counts(cur_id_head)\n",
    "            mean_prob = tf.segment_mean(cur_prob_head, idx)\n",
    "            mean_label = inv_table.lookup(tf.argmax(mean_prob, 1))\n",
    "        with tf.control_dependencies([mean_prob, mean_label]):\n",
    "            #last_id = cur_id_tail\n",
    "            #last_prob = cur_prob_tail\n",
    "            # last partition may have nothing to concat\n",
    "            raw_id_tail, idx_tail, _ = tf.unique_with_counts(cur_id_tail)\n",
    "            mean_prob_tail = tf.segment_mean(cur_prob_tail, idx_tail)\n",
    "            tail_label = inv_table.lookup(tf.argmax(mean_prob_tail, 1))\n",
    "    restore_saver = tf.train.Saver(variables_to_restore)\n",
    "    def load_pretrain(sess):\n",
    "        restore_saver.restore(sess, INCEPTION_MODEL_PATH)\n",
    "\n",
    "    # no need for specify local_variables_initializer and tables_initializer, Supervisor will do this via default local_init_op\n",
    "    init_op = tf.group(tf.global_variables_initializer())\n",
    "    # Pass the init function to the supervisor.\n",
    "    # - The init function is called _after_ the variables have been initialized by running the init_op.\n",
    "    # - use default tf.Saver() for ordinary save and restore\n",
    "    # - save checkpoint every 1.3 hours\n",
    "    # - manage summary in current process by ourselves for memory saving\n",
    "    # - no need to specify global_step, supervisor will find this automately\n",
    "    # - initialize order: checkpoint -> local_init_op -> init_op -> init_func\n",
    "    sv = tf.train.Supervisor(logdir=LOG_PATH, init_fn = load_pretrain, init_op = init_op, summary_op = None, save_model_secs=0)\n",
    "    \n",
    "    step = 0\n",
    "    lats_pred = []\n",
    "    last_batch_id = []\n",
    "    last_feed_id = np.empty([0])\n",
    "    last_feed_prob = np.empty([0, NUM_CLASS])\n",
    "    save_file_name = OUTPUT_PATH.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    prob_save_file_name = PROB_SAVE_FILE.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    hdf5_file = tb.open_file(prob_save_file_name, 'w')\n",
    "    total_prob_store = hdf5_file.create_earray(hdf5_file.root, 'prob', tb.Float32Atom(), shape=(0,), filters=tb.Filters(complevel=5, complib='zlib'))\n",
    "    total_row_indice = hdf5_file.create_earray(hdf5_file.root, 'row', tb.Int64Atom(), shape=(0,), filters=tb.Filters(complevel=5, complib='zlib'))\n",
    "    total_col_indice = hdf5_file.create_earray(hdf5_file.root, 'col', tb.Int64Atom(), shape=(0,), filters=tb.Filters(complevel=5, complib='zlib'))\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    #last_row_to_save = 0\n",
    "    with sv.managed_session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "    #with sv.prepare_or_wait_for_session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "\n",
    "        # Here sess was either initialized from the pre-trained-checkpoint or\n",
    "        # recovered from a checkpoint saved in a previous run of this code.\n",
    "        while True:       \n",
    "            if sv.should_stop():\n",
    "                tf_logging.info('Supervisor emit finished!')\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            with tf.device('/gpu:0'):\n",
    "                #test_pred, test_prob, test_batch_id = sess.run([test_predictions_values, test_probabilities, batch_id])\n",
    "                cur_batch_id, last_feed_id, last_feed_prob, test_pred, test_prob, test_batch_id, lats_pred, last_batch_id, sparse_row, sparse_col, sparse_value, sparse_shape = sess.run([batch_id, cur_id_tail, cur_prob_tail, mean_label, mean_prob, raw_id, tail_label, raw_id_tail, row_indice, col_indice, value_array, cur_prob_shape], feed_dict = {last_prob: last_feed_prob, last_id: last_feed_id})\n",
    "            #print(csr_matrix((sparse_value, (sparse_row, sparse_col)), shape=sparse_shape).toarray())\n",
    "            #total_prob_store[last_row_to_save:last_row_to_save + sparse_shape[0],:] = csr_matrix((sparse_value, (sparse_row, sparse_col)), shape=sparse_shape).toarray()\n",
    "            total_prob_store.append(sparse_value)\n",
    "            total_row_indice.append(sparse_row)\n",
    "            total_col_indice.append(sparse_col)\n",
    "            #last_row_to_save += sparse_shape[0]\n",
    "            time_elapsed = time.time() - start_time\n",
    "            if step % 200 == 0:\n",
    "                tf_logging.info('Step: {} of {}.'.format(step, NUM_STEPS))\n",
    "                tf_logging.info('Validation Speed: {:5.3f}sec/batch'.format(time_elapsed))\n",
    "                tf_logging.info('Roughly {:6.3f} hours to go.'.format(  time_elapsed*( (NUM_STEPS-step) > 0 and (NUM_STEPS-step)/3600. or 0.001 )  ))\n",
    "                tf_logging.info('Test Label: {}'.format(test_pred))\n",
    "                tf_logging.info('Test Prob: {}'.format(test_prob))\n",
    "                #tf_logging.info('Test Ids: {}'.format(test_batch_id))\n",
    "            #print(len(test_prob[0]))\n",
    "            df = pd.DataFrame({'_id' : test_batch_id, 'category_id' : test_pred})\n",
    "            #df = pd.DataFrame([test_batch_id, test_pred], columns=[\"_id\", 'category_id'])\n",
    "\n",
    "            if not os.path.isfile(save_file_name):\n",
    "                df.to_csv(save_file_name, mode='a', index=False, sep=',')\n",
    "            else:\n",
    "                df.to_csv(save_file_name, mode='a', index=False, sep=',', header=False)\n",
    "#             if not os.path.isfile(ID_SAVE_FILE):\n",
    "#                 pd.DataFrame({'_id' : cur_batch_id}).to_csv(ID_SAVE_FILE, mode='a', index=False, sep=',')\n",
    "#             else:\n",
    "#                 pd.DataFrame({'_id' : cur_batch_id}).to_csv(ID_SAVE_FILE, mode='a', index=False, sep=',', header=False)\n",
    "            step += 1\n",
    "\n",
    "#             tf_logging.info('BB ID: {}'.format(bb_id))\n",
    "#             tf_logging.info('Test Label: {}'.format(test_pred))\n",
    "#             #tf_logging.info('Test Prob: {}'.format(test_prob))\n",
    "#             tf_logging.info('Test Ids: {}'.format(test_batch_id))\n",
    "#             tf_logging.info('Last Label: {}'.format(lats_pred))\n",
    "#             #tf_logging.info('Test Prob: {}'.format(test_prob))\n",
    "#             tf_logging.info('Last Ids: {}'.format(last_batch_id))\n",
    "#             if step > 3:\n",
    "#                 break\n",
    "            \n",
    "    df = pd.DataFrame({'_id' : last_batch_id, 'category_id' : lats_pred})\n",
    "    df.to_csv(save_file_name, mode='a', index=False, sep=',', header=False)\n",
    "    hdf5_file.close()\n",
    "    tf_logging.info('Finished evaluation! ')\n",
    "    print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(save_file_name)\n",
    "print(test_data.tail())\n",
    "# 1768182 \n",
    "#(1768182, 3095080)\n",
    "#Test11111 - (700, 1195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# h5 = tb.open_file(DATASET_PATH + 'logs/probs_2017-10-19 21:30:55.h5', 'r')\n",
    "# print(h5.root.prob.shape)\n",
    "# print(h5.root.row.shape)\n",
    "# print(h5.root.col.shape)\n",
    "# print(csr_matrix((h5.root.prob[:], (h5.root.row[:], h5.root.col[:])), shape=(TOTAL_EXAMPLES,NUM_CLASS)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
