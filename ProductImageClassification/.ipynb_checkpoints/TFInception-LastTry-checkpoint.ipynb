{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# Running %env without any arguments\n",
    "# lists all environment variables\n",
    "\n",
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "from datetime import datetime\n",
    "import bson                       # this is installed with the pymongo package\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imsave, imshow, imresize\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.training import add_gradients_summaries\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.training import optimizer as tf_optimizer\n",
    "from tensorflow.python.ops import variables as tf_variables\n",
    "import os.path\n",
    "import tensorflow.contrib.slim as slim\n",
    "import inception_preprocessing\n",
    "import logging\n",
    "import inception_v3_Last\n",
    "import cdscount_preprocessing\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/'\n",
    "\n",
    "PRETRAINED_MODEL_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/inception-v3/20160828/inception_v3.ckpt'\n",
    "#PRETRAINED_MODEL_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/logs_last/before debug/inception_model.ckpt-151025'\n",
    "LOG_PATH = DATASET_PATH + 'logs_last/'\n",
    "LR_FILE_PATH = DATASET_PATH + 'logs_last/lr_setting/inception_lr_setting'\n",
    "TRAIN_PATH = DATASET_PATH + 'Split/Train/'\n",
    "#TRAIN_PATH = '/media/rs/FC6CDC6F6CDC25E4/resample_dataset2/'\n",
    "VAL_PATH = DATASET_PATH + 'Split/Validation/'\n",
    "TEST_PATH = DATASET_PATH + 'Test/'\n",
    "CATEGORY_NAME_PATH = DATASET_PATH + 'category_names.csv'\n",
    "CATEGORY_WEIGHT_PATH = DATASET_PATH + 'catogory_with_weight.csv'\n",
    "\n",
    "LEVEL1_NUM_LIST = [555, 441, 440, 237, 230, 230, 220, 206, 196, 184, 180, 162, 158, 137, 106, 104, 103, 101, 99, 89, 88, 85, 84, 83, 81, 74, 57, 50, 48, 45, 43, 42, 42, 38, 33, 33, 30, 29, 26, 25, 19, 16, 9, 6, 2, 1, 1, 1, 1]\n",
    "# following is non-resample\n",
    "LEVEL1_WEIGHT = [1.817, 1.528, 1.056, 2.174, 1.622, 2.531, 2.663, 1.103, 1.935, 1.937, 2.898, 1.279, 2.923, 1.095, 2.086, 3.0, 2.218, 3.0, 3.0, 2.123, 3.0, 1.038, 3.0, 3.0, 3.0, 3.0, 1.817, 1.448, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.955, 3.0, 1.946, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0]\n",
    "\n",
    "BATCH_SIZE = 160#128#256\n",
    "\n",
    "\n",
    "IMAGE_WIDTH = 180\n",
    "IMAGE_HEIGHT = 180\n",
    "NUM_CLASS = 5270\n",
    "LEVEL1_CLASS = 49\n",
    "LEVEL2_CLASS = 483\n",
    "# validation examples num: 2319624\n",
    "# train examples num: 10051704\n",
    "# total step: 157057\n",
    "TOTAL_EXAMPLES = 12371328\n",
    "\n",
    "NUM_EPOCHES = 12\n",
    "EPOCHES_OVER = 8\n",
    "\n",
    "INPUT_THREADS = 12\n",
    "\n",
    "initial_learning_rate = 0.01#0.0004\n",
    "stop_learning_rate = 0.0000001\n",
    "moving_average_decay = 0.96# use large to be more stable?\n",
    "momentum = 0.9\n",
    "num_steps_per_epoch = TOTAL_EXAMPLES / BATCH_SIZE + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get TF logger\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(DATASET_PATH + 'tensorflow_inception_hierachy_train.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_learning_rate(cur_step, num_steps_per_epoch):\n",
    "    def inner_lr_parser(interval_start, interval_end, lr, dict_in, default_lr, use_epoch_percent, num_steps_per_epoch):\n",
    "        lr = default_lr * lr\n",
    "        if use_epoch_percent:\n",
    "            interval_start = num_steps_per_epoch * interval_start\n",
    "            interval_end = num_steps_per_epoch * interval_end\n",
    "        interval_start = int(interval_start)\n",
    "        interval_end = int(interval_end)\n",
    "        if (interval_start < interval_end) and (lr > 0):\n",
    "            dict_in[(interval_start, interval_end)] = lr\n",
    "            \n",
    "    lr_map = dict()\n",
    "    default_lr = initial_learning_rate\n",
    "    stop_lr = stop_learning_rate\n",
    "    line_index = -1\n",
    "    use_epoch_percent = True\n",
    "    if os.path.exists(LR_FILE_PATH):\n",
    "        with open(LR_FILE_PATH, 'r') as lr_setting_file:\n",
    "            for _, line in enumerate(lr_setting_file):\n",
    "                line = line.strip()\n",
    "                if (line != '') and (not line.startswith('#')):\n",
    "                    line_index += 1\n",
    "                    if line_index == 0:\n",
    "                        default_lr = float(line.split(':')[-1].strip())\n",
    "                        continue\n",
    "                    if line_index == 1:\n",
    "                        stop_lr = float(line.split(':')[-1].strip())\n",
    "                        continue\n",
    "                    if line_index == 2:\n",
    "                        use_epoch_percent = ('EPOCHES_PERCENT' in (line.split(':')[-1].strip()))\n",
    "                        continue\n",
    "                    # this is a list desciption\n",
    "                    if line.startswith('['):\n",
    "                        line = [float(s.strip()) for s in line[1:-1].strip().split()]\n",
    "                        step_interval = (line[1] - line[0])/line[-1]\n",
    "                        lr_interval = (line[3] - line[2])/line[-1]\n",
    "                        begin = line[0]\n",
    "                        lr_begin = line[2]\n",
    "                        for index in range(int(line[-1])):\n",
    "                            inner_lr_parser(begin, begin+step_interval, lr_begin, lr_map, default_lr, use_epoch_percent, num_steps_per_epoch)\n",
    "                            begin += step_interval\n",
    "                            lr_begin += lr_interval\n",
    "                    else:\n",
    "                        interval_start, interval_end, lr = [float(s) for s in line.strip().split()]\n",
    "                        inner_lr_parser(interval_start, interval_end, lr, lr_map, default_lr, use_epoch_percent, num_steps_per_epoch)\n",
    "    lr_ret = default_lr\n",
    "#     print(use_epoch_percent)\n",
    "    for (start, end), lr in lr_map.items():\n",
    "        if (cur_step >= start) and (cur_step <= end):\n",
    "            if (lr < lr_ret):\n",
    "                lr_ret = lr\n",
    "    if lr_ret < stop_lr: lr_ret = stop_lr      \n",
    "    return lr_ret\n",
    "# _ = read_learning_rate(1, num_steps_per_epoch)\n",
    "# lr = []\n",
    "# num_epoches_to_show = 10\n",
    "# num_point = 100\n",
    "# for i in [i*num_epoches_to_show*num_steps_per_epoch/num_point for i in range(num_point)]:\n",
    "#     lr.append(read_learning_rate(i, num_steps_per_epoch))\n",
    "# plt.plot(lr)\n",
    "# plt.ylabel('learning rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_inception(input_image, is_training = True):\n",
    "    return cdscount_preprocessing.preprocess_image(input_image, 180, 180, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabelMapping(object):\n",
    "    def __init__(self, catogory_file_path):\n",
    "        super(LabelMapping, self).__init__()\n",
    "        self._category_level_csv = catogory_file_path\n",
    "        self._category_map, self._category_level1_map, self._category_level2_map, self._len_level1, self._len_level2 = self.cvt_csv2tfrecord()\n",
    "        \n",
    "        assert (LEVEL1_CLASS == self._len_level1) and (LEVEL2_CLASS == self._len_level2), 'Other two levels are not mapped correctly.'\n",
    "        self._catogory_weight_map = self.cvt_catogory_weight()\n",
    "        self._mapping_strings = tf.constant( [ str(key) for key in self._category_map.keys() ] )\n",
    "        #print(list(self._category_map.keys())[0])\n",
    "        self._mapping_table = tf.contrib.lookup.index_table_from_tensor(mapping=self._mapping_strings, default_value=0) \n",
    "        \n",
    "        self._level1_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._category_level1_map.keys()), list(self._category_level1_map.values()), tf.int64, tf.int64), 0)\n",
    "        self._level2_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._category_level2_map.keys()), list(self._category_level2_map.values()), tf.int64, tf.int64), 0)\n",
    "        self._weight_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._catogory_weight_map.keys()), list(self._catogory_weight_map.values()), tf.int64, tf.float32), 0)\n",
    "\n",
    "    @property\n",
    "    def category_map(self):\n",
    "        return self._category_map\n",
    "    @property\n",
    "    def level1_table(self):\n",
    "        return self._level1_table\n",
    "    @property\n",
    "    def level2_table(self):\n",
    "        return self._level2_table\n",
    "    @property\n",
    "    def len_level1(self):\n",
    "        return self._len_level1\n",
    "    @property\n",
    "    def len_level2(self):\n",
    "        return self._len_level2\n",
    "    @property\n",
    "    def mapping_table(self):\n",
    "        return self._mapping_table\n",
    "    @property\n",
    "    def weight_table(self):\n",
    "        return self._weight_table\n",
    "    \n",
    "    def cvt_catogory_weight(self):\n",
    "        category_weight_map = dict()\n",
    "        csv = pd.read_csv(CATEGORY_WEIGHT_PATH).values\n",
    "        for row in csv:  \n",
    "            category_id, weight = row[0], row[2]\n",
    "#             if weight > 1.5:\n",
    "#                 weight = 1.5\n",
    "            category_weight_map[int(category_id)] = 1.\n",
    "\n",
    "        return category_weight_map\n",
    "\n",
    "    def cvt_csv2tfrecord(self):\n",
    "        level1_map, level2_map = self.create_level_map()\n",
    "        count = 0\n",
    "        category_map = dict()\n",
    "        category_level1_map = dict()\n",
    "        category_level2_map = dict()\n",
    "        csv = pd.read_csv(self._category_level_csv).values\n",
    "        for row in csv:  \n",
    "            category_id, level1, level2 = row[0], row[1], row[2]\n",
    "            category_map[category_id] = count\n",
    "            category_level1_map[int(category_id)] = level1_map[level1]\n",
    "            category_level2_map[int(category_id)] = level2_map[level2]\n",
    "            count += 1\n",
    "\n",
    "        return category_map, category_level1_map, category_level2_map, len(level1_map), len(level2_map)\n",
    "\n",
    "    def create_level_map(self):\n",
    "        csv = pd.read_csv(self._category_level_csv).values\n",
    "        level_list = [list(), list()]\n",
    "        for row in csv: \n",
    "            for level in range(1,3):\n",
    "                if row[level] not in level_list[level-1]:\n",
    "                    level_list[level-1].append(row[level])\n",
    "        return dict(zip(level_list[0], range(len(level_list[0])))), dict(zip(level_list[1], range(len(level_list[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CdiscountDataset(object):\n",
    "    def __init__(self, data_path, file_begin_match, label_mapping, num_examples, num_classes, buffer_size, batch_size, num_epochs, is_training):\n",
    "        super(CdiscountDataset, self).__init__()\n",
    "        #self._data_file_list = [ os.path.join(data_path, x) for x in os.listdir(data_path) if lambda x: os.path.isfile(x) and x.startswith(file_begin_match) ]\n",
    "        self._data_file_list = data_path + file_begin_match + '*'\n",
    "        self._num_examples = num_examples\n",
    "        self._num_classes = num_classes\n",
    "        self._batch_size = batch_size\n",
    "        self._buffer_size = buffer_size\n",
    "        self._num_epochs = num_epochs\n",
    "        self._is_training = is_training\n",
    "        self._category_map = label_mapping.category_map\n",
    "        self._level1_table = label_mapping.level1_table\n",
    "        self._level2_table = label_mapping.level2_table\n",
    "        self._len_level1 = label_mapping.len_level1\n",
    "        self._len_level2 = label_mapping.len_level2\n",
    "        self._mapping_table = label_mapping.mapping_table\n",
    "        self._weight_table = label_mapping.weight_table\n",
    "    \n",
    "    \n",
    "    def create_dataset(self):\n",
    "        opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "        reader = lambda : tf.TFRecordReader(options=opts)\n",
    "        keys_to_features = {\n",
    "            'img_raw': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "            'product_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            # notice that we don't have this feature in our TFRecord, so always default provided\n",
    "            'format': tf.FixedLenFeature([], tf.string, default_value='jpg'),\n",
    "            'category_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64))\n",
    "        }\n",
    "\n",
    "        items_to_handlers = {\n",
    "            # automated decode image from features in FixedLenFeature\n",
    "            'image': slim.tfexample_decoder.Image(image_key='img_raw', format_key='format'),\n",
    "            'label': slim.tfexample_decoder.Tensor('category_id'),\n",
    "        }\n",
    "\n",
    "        decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "        #print(len(tf.gfile.Glob(self._data_file_list)))\n",
    "        self._dataset = slim.dataset.Dataset(\n",
    "            data_sources = self._data_file_list,\n",
    "            decoder = decoder,\n",
    "            reader = reader,\n",
    "            # num_readers = 8,\n",
    "            num_samples = self._num_examples,\n",
    "            #num_classes = self._num_classes,\n",
    "            items_to_descriptions = None)\n",
    "        \n",
    "        # notice that DatasetDataProvider can automate shuffle the examples by ParallelReader using its RandomShuffleQueue\n",
    "        self._data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            self._dataset,\n",
    "            num_readers = INPUT_THREADS,\n",
    "            shuffle = True, # default is True\n",
    "            num_epochs = self._num_epochs,\n",
    "            common_queue_capacity = self._buffer_size + 4 * self._batch_size,\n",
    "            common_queue_min = self._buffer_size,\n",
    "            scope = self._is_training and 'train_files' or 'validation_files')\n",
    "        \n",
    "        org_image, org_label = self._data_provider.get(['image', 'label'])\n",
    "\n",
    "        image = preprocess_for_inception(org_image, self._is_training) # final image to train\n",
    " \n",
    "        # no need for shuffle, DatasetDataProvider do this for us\n",
    "        batch_images, batch_labels, batch_labels_level1, batch_labels_level2, batch_weight = \\\n",
    "                tf.train.batch([image, tf.one_hot(self._mapping_table.lookup(tf.as_string(org_label)), self._num_classes, axis=-1),\\\n",
    "                tf.one_hot(self._level1_table.lookup(org_label), self._len_level1, axis=-1),\\\n",
    "                tf.one_hot(self._level2_table.lookup(org_label), self._len_level2, axis=-1), self._weight_table.lookup(org_label)],\\\n",
    "                self._batch_size,\\\n",
    "                num_threads = INPUT_THREADS,\\\n",
    "                capacity = self._buffer_size + 4 * self._batch_size,\\\n",
    "                allow_smaller_final_batch = self._is_training, name = self._is_training and 'train_batch' or 'validation_batch')\n",
    "        \n",
    "        return batch_images, batch_labels, batch_labels_level1, batch_labels_level2, batch_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def_graph = tf.Graph()\n",
    "with def_graph.as_default() as graph:\n",
    "    def train_step(input_examples, batch_labels, level1_onehot_labels, level2_onehot_labels, batch_weight):   \n",
    "        with tf.name_scope(\"InceptionLast\"):\n",
    "            with slim.arg_scope(inception_v3_Last.inception_v3_arg_scope()):\n",
    "                # here logits is the pre-softmax activations\n",
    "                logits, end_points = inception_v3_Last.inception_v3(\n",
    "                    input_examples,\n",
    "                    num_classes = NUM_CLASS,\n",
    "                    dropout_keep_prob=0.5,\n",
    "                    is_training = True)\n",
    "                \n",
    "                #pre_logits = end_points['PreLogits']\n",
    "                #pre_logits = tf.stop_gradient(end_points['PreLogits'])\n",
    "                \n",
    "                tvars = tf.trainable_variables()\n",
    "\n",
    "                #org_vars = [var for var in tvars if 'level1_' not in var.name]\n",
    "                org_head_only_vars = [var for var in tvars if (('Conv2d_1c_1x1' in var.name) or ('Mixed_7c' in var.name))]\n",
    "                \n",
    "\n",
    "                #variables_to_restore = slim.get_variables_to_restore()\n",
    "                variables_to_restore = slim.get_variables_to_restore(exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits'])\n",
    "                                #                 variable_averages = tf.train.ExponentialMovingAverage(moving_average_decay)\n",
    "#                 variables_to_restore_ = variable_averages.variables_to_restore()\n",
    "\n",
    "                #print(variables_to_restore)\n",
    "    \n",
    "                global_step = tf.train.get_or_create_global_step(graph = graph)\n",
    "                \n",
    "            \n",
    "                loss_level3 = tf.losses.softmax_cross_entropy(onehot_labels = batch_labels, logits = logits, label_smoothing = 0.)\n",
    "                #aux_loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = end_points['AuxLogits'], weights=0.2)\n",
    "                total_loss = tf.losses.get_total_loss(add_regularization_losses=True)    # obtain the regularization losses as well\n",
    "\n",
    "                custom_learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "                #Now we can define the optimizer that takes on the learning rate\n",
    "                #optimizer = tf.train.AdamOptimizer(learning_rate = custom_learning_rate)\n",
    "                optimizer = tf.train.MomentumOptimizer(learning_rate = custom_learning_rate, momentum=momentum)\n",
    "\n",
    "                #moving_average_variables = slim.get_model_variables()\n",
    "                #variable_averages = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)\n",
    "                # Use an alternative set of update ops in addition to the default updates:\n",
    "                #tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, variable_averages.apply(moving_average_variables))\n",
    "\n",
    "\n",
    "                #train_op_level3 = slim.learning.create_train_op(total_loss, optimizer, summarize_gradients=False, variables_to_train=org_vars)#, clip_gradient_norm=10)\n",
    "                #train_op_level1 = slim.learning.create_train_op(total_loss, optimizer, summarize_gradients=False, variables_to_train=level1_only_vars)#, clip_gradient_norm=10)\n",
    "                train_op = slim.learning.create_train_op(total_loss, optimizer, summarize_gradients=False, variables_to_train=org_head_only_vars)#, clip_gradient_norm=10)\n",
    "\n",
    "                variables_to_restore_checkpoint = slim.get_variables_to_restore()\n",
    "\n",
    "                #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "\n",
    "                probabilities = end_points['Predictions']\n",
    "                predictions = tf.argmax(probabilities, 1)\n",
    "                target = batch_labels\n",
    "                \n",
    "                accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, tf.argmax(target, 1), name='train_accuracy')\n",
    "                metrics_op = tf.group(accuracy_update)\n",
    "\n",
    "                real_time_accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(target, 1)), tf.float32))\n",
    "                \n",
    "                #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "                tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "                tf.summary.scalar('accuracy', accuracy)\n",
    "                tf.summary.scalar('train/real_time_accuracy', real_time_accuracy)\n",
    "                tf.summary.scalar('learning_rate', custom_learning_rate)\n",
    "\n",
    "                return train_op, global_step, metrics_op, variables_to_restore, variables_to_restore_checkpoint, predictions, custom_learning_rate, accuracy, real_time_accuracy, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot infer Tensor's rank: Tensor(\"PyFuncStateless:0\", dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3ba3602905ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCdiscountDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*output_file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOTAL_EXAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels_level1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels_level2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4f676afadc78>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# no need for shuffle, DatasetDataProvider do this for us\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels_level1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels_level2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_weight\u001b[0m \u001b[0;34m=\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mapping_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_level1_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_len_level1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_level2_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_len_level2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mnum_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mINPUT_THREADS\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mcapacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m,\u001b[0m                \u001b[0mallow_smaller_final_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_training\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'train_batch'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'validation_batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels_level1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels_level2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/training/input.py\u001b[0m in \u001b[0;36mbatch\u001b[0;34m(tensors, batch_size, num_threads, capacity, enqueue_many, shapes, dynamic_pad, allow_smaller_final_batch, shared_name, name)\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0mallow_smaller_final_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_smaller_final_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m       \u001b[0mshared_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshared_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m       name=name)\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/training/input.py\u001b[0m in \u001b[0;36m_batch\u001b[0;34m(tensors, batch_size, keep_input, num_threads, capacity, enqueue_many, shapes, dynamic_pad, allow_smaller_final_batch, shared_name, name)\u001b[0m\n\u001b[1;32m    709\u001b[0m         tensor_list, enqueue_many, keep_input)\n\u001b[1;32m    710\u001b[0m     \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menqueue_many\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m     \u001b[0;31m# TODO(josh11b,mrry): Switch to BatchQueue once it is written.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m     queue = _which_queue(dynamic_pad)(\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/training/input.py\u001b[0m in \u001b[0;36m_shapes\u001b[0;34m(tensor_list_list, shapes, enqueue_many)\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot infer Tensor's rank: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     shapes = [_merge_shapes(\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot infer Tensor's rank: Tensor(\"PyFuncStateless:0\", dtype=float32)"
     ]
    }
   ],
   "source": [
    "with def_graph.as_default() as graph:\n",
    "    label_mapping = LabelMapping(CATEGORY_NAME_PATH)\n",
    "    train_dataset = CdiscountDataset(TRAIN_PATH, '*output_file', label_mapping, TOTAL_EXAMPLES, NUM_CLASS, 20000, BATCH_SIZE, NUM_EPOCHES, True)\n",
    "\n",
    "    batch_images, batch_labels, batch_labels_level1, batch_labels_level2, batch_weight = train_dataset.create_dataset()\n",
    "   \n",
    "    with tf.device('/gpu:0'):\n",
    "        train_op, global_step, metrics_op, variables_to_restore, variables_to_restore_checkpoint, pred_op, lr, accuracy, real_time_accuracy, total_loss = train_step(batch_images, batch_labels, batch_labels_level1, batch_labels_level2, batch_weight)\n",
    "         \n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    checkpoint_saver = tf.train.Saver(variables_to_restore_checkpoint)\n",
    "    \n",
    "    pre_train_saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "    def load_pretrain(sess):\n",
    "        pre_train_saver.restore(sess, PRETRAINED_MODEL_PATH)\n",
    "\n",
    "    # no need for specify local_variables_initializer and tables_initializer, Supervisor will do this via default local_init_op\n",
    "    # init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer())\n",
    "    init_op = tf.group(tf.global_variables_initializer())\n",
    "    #init_op = tf.group(train_iterator_initializer, val_iterator_initializer, tf.global_variables_initializer())\n",
    "    \n",
    "    # Pass the init function to the supervisor.\n",
    "    # - The init function is called _after_ the variables have been initialized by running the init_op.\n",
    "    # - use default tf.Saver() for ordinary save and restore\n",
    "    # - save checkpoint every 1.3 hours(4800)\n",
    "    # - manage summary in current process by ourselves for memory saving\n",
    "    # - no need to specify global_step, supervisor will find this automately\n",
    "    # - initialize order: checkpoint -> local_init_op -> init_op -> init_func\n",
    "    sv = tf.train.Supervisor(logdir=LOG_PATH, init_fn = load_pretrain, init_op = init_op, summary_op = None, saver = checkpoint_saver, save_model_secs=7200, checkpoint_basename='inception_model.ckpt')\n",
    "    \n",
    "    final_loss = 0.\n",
    "    final_accuracy = 0.\n",
    "    training_state = True\n",
    "\n",
    "    cur_readed_lr = initial_learning_rate\n",
    "    tf_logging.info(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    with sv.managed_session(config=config) as sess:\n",
    "    #with sv.prepare_or_wait_for_session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "        #sess.run(iterator_initalizer)\n",
    "        # Here sess was either initialized from the pre-trained-checkpoint or\n",
    "        # recovered from a checkpoint saved in a previous run of this code.\n",
    "        for step in range(int(num_steps_per_epoch * NUM_EPOCHES)):         \n",
    "            if sv.should_stop():\n",
    "                tf_logging.info('Supervisor emit finished!')\n",
    "                tf_logging.info('Current Loss: %s', loss)\n",
    "                tf_logging.info('Current Accuracy: %s', accuracy)\n",
    "                tf_logging.info('Saving current model to disk(maybe invalid).')\n",
    "                training_state = False\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                summ, cur_global_step = sess.run([summary_op, global_step], feed_dict={lr: cur_readed_lr})\n",
    "                sv.summary_computed(sess, summ)\n",
    "                if step > EPOCHES_OVER * num_steps_per_epoch:\n",
    "                    raise StopIteration(\"over epoches reached.\")\n",
    "                cur_readed_lr = read_learning_rate(cur_global_step, num_steps_per_epoch)\n",
    "\n",
    "            with tf.device('/gpu:0'):\n",
    "                f, _, _, cur_loss, cur_acc, rt_accuracy, total_step, cur_lr = sess.run([batch_images,train_op, metrics_op, total_loss, accuracy, real_time_accuracy, global_step, lr], feed_dict={lr: cur_readed_lr})\n",
    "            time_elapsed = time.time() - start_time\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                #print(f)\n",
    "                #break\n",
    "                final_loss = cur_loss\n",
    "                final_accuracy = cur_acc\n",
    "                tf_logging.info('Current Speed: {:5.3f}sec/batch'.format(time_elapsed))\n",
    "                tf_logging.info('Current Streaming Accuracy: {:5.3f}%'.format(cur_acc*100.))\n",
    "                tf_logging.info('Current Realtime Accuracy: {:5.3f}%'.format(rt_accuracy*100.))\n",
    "                tf_logging.info('Current Loss: {:5.3f}'.format(cur_loss))\n",
    "                tf_logging.info('Epoch %s/%s, Global Step: %s', int(total_step / num_steps_per_epoch + 1), NUM_EPOCHES, total_step)\n",
    "                tf_logging.info('Current Learning Rate: {}'.format(cur_lr))\n",
    "\n",
    "        if training_state:\n",
    "            #We log the final training loss and accuracy\n",
    "            tf_logging.info('Final Loss: %s', final_loss)\n",
    "            tf_logging.info('Final Accuracy: %s', final_accuracy)\n",
    "            # Once all the training has been done, save the log files and checkpoint model\n",
    "            tf_logging.info('Finished training! Model saved.')\n",
    "        sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n",
    "        tf_logging.info(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
