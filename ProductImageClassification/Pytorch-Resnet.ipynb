{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "GPU_ID = 0\n",
    "# tensorboard --logdir=/media/rs/0E06CD1706CD0127/Kapok/kaggle/pytoh/cdiscount/logs_resnet --port=6008\n",
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "get_ipython().magic('env CUDA_VISIBLE_DEVICES = ')\n",
    "#%env CUDA_VISIBLE_DEVICES = 0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imsave, imshow, imresize\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.)\n",
    "tf_sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n",
    "# now tensorflow will assume there not exist gpu\n",
    "# use this tf_session following, and close in the end\n",
    "#tf_sess.close()\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "import io\n",
    "from datetime import datetime\n",
    "import gc # garbage collector\n",
    "import logging\n",
    "get_ipython().magic('env CUDA_VISIBLE_DEVICES = {}'.format(', '.join(map(str, GPU_ID))))\n",
    "from tensorboard_logger import configure, log_value\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch_resnet\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/'\n",
    "#PRETRAINED_MODEL_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/models/resnet101-5d3b4d8f.pth'\n",
    "PRETRAINED_MODEL_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/models/resnet152-b121ed2d.pth'\n",
    "\n",
    "LOG_DIR = DATASET_PATH + 'pytorch/cdiscount/logs_resnet'\n",
    "LR_FILE_PATH = DATASET_PATH + 'pytorch/cdiscount/logs_resnet/lr_setting/resnet_lr_setting'\n",
    "TRAIN_PATH = DATASET_PATH + 'Split/Train/'\n",
    "#TRAIN_PATH = '/media/rs/FC6CDC6F6CDC25E4/resample_dataset2/'\n",
    "VAL_PATH = DATASET_PATH + 'Split/Validation/'\n",
    "TEST_PATH = DATASET_PATH + 'Test/'\n",
    "CATEGORY_NAME_PATH = DATASET_PATH + 'category_names.csv'\n",
    "CATEGORY_WEIGHT_PATH = DATASET_PATH + 'catogory_with_weight.csv'\n",
    "\n",
    "# following is non-resampled, and only 10mil\n",
    "LEVEL1_WEIGHT = [1.817, 1.528, 1.056, 2.174, 1.622, 2.531, 2.663, 1.103, 1.935, 1.937, 2.898, 1.279, 2.923, 1.095, 2.086, 3.0, 2.218, 3.0, 3.0, 2.123, 3.0, 1.038, 3.0, 3.0, 3.0, 3.0, 1.817, 1.448, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.955, 3.0, 1.946, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0]\n",
    "\n",
    "BATCH_SIZE = 146#112#256\n",
    "VAL_BATCH_SIZE = 32\n",
    "\n",
    "IMAGE_WIDTH = 180\n",
    "IMAGE_HEIGHT = 180\n",
    "NUM_CLASS = 5270\n",
    "LEVEL1_CLASS = 49\n",
    "LEVEL2_CLASS = 483\n",
    "# validation examples num: 2319624\n",
    "# train examples num: 10051704\n",
    "# total step: 157057\n",
    "TOTAL_EXAMPLES = 12301740\n",
    "VAL_EXAMPLES = 69588\n",
    "\n",
    "NUM_EPOCHES = 12\n",
    "EPOCHES_OVER = 8\n",
    "\n",
    "INPUT_THREADS = 8\n",
    "\n",
    "save_time_interval = 7200\n",
    "log_step_interval = 200\n",
    "\n",
    "initial_learning_rate = 0.01#0.0004\n",
    "stop_learning_rate = 0.0000001\n",
    "momentum = 0.9\n",
    "num_steps_per_train_epoch = int(TOTAL_EXAMPLES / (BATCH_SIZE)) + 1\n",
    "num_steps_per_val_epoch = int(VAL_EXAMPLES / (VAL_BATCH_SIZE)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_logging(logger_name, logger_file_name):\n",
    "    log = logging.getLogger(logger_name)\n",
    "    log.setLevel(logging.DEBUG)\n",
    "\n",
    "    # create formatter and add it to the handlers\n",
    "    print_formatter = logging.Formatter('%(message)s')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s_%(levelname)s: %(message)s')\n",
    "\n",
    "    # create file handler which logs even debug messages\n",
    "    fh = logging.FileHandler(logger_file_name, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh.setFormatter(file_formatter)\n",
    "    log.addHandler(fh)\n",
    "    # both output to console and file\n",
    "    consoleHandler = logging.StreamHandler()\n",
    "    consoleHandler.setFormatter(print_formatter)\n",
    "    log.addHandler(consoleHandler)\n",
    "    \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "here is an info message.\n"
     ]
    }
   ],
   "source": [
    "log = set_logging('CDiscount', DATASET_PATH + 'pytorch/cdiscount/resnet_pytorch.log')\n",
    "log.info('here is an info message.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Saver(object):\n",
    "    def __init__(self, log_dir, ckeckpoint_name, max_to_keep=5):\n",
    "        super(Saver, self).__init__()\n",
    "        self._log_dir = log_dir\n",
    "        self._max_to_keep = max_to_keep\n",
    "        self._ckeckpoint_container_file = os.path.join(self._log_dir, 'checkpoint')\n",
    "        self._ckeckpoint_name = os.path.join(self._log_dir, ckeckpoint_name)\n",
    "        self._best_ckeckpoint_name = os.path.join(self._log_dir, 'best_checkpoint')\n",
    "        os.makedirs(self._log_dir, exist_ok=True)\n",
    "        os.makedirs(self._best_ckeckpoint_name, exist_ok=True)\n",
    "        self._best_ckeckpoint_name = os.path.join(self._best_ckeckpoint_name, ckeckpoint_name)\n",
    "    def save_checkpoint(self, model_state_dict, other_state_dict, tag, is_best=False):\n",
    "        checkpoint_list = list()\n",
    "        if os.path.exists(self._ckeckpoint_container_file):\n",
    "            with open(self._ckeckpoint_container_file, 'r') as checkpoint_file:\n",
    "                for index, line in enumerate(checkpoint_file):\n",
    "                    if index == 0: continue\n",
    "                    if line.strip() != '':\n",
    "                        checkpoint_list.append(line.strip())\n",
    "        if not is_best: \n",
    "            model_save_name = (self._ckeckpoint_name + '_pytorch_state_{}_{}.pth').format(tag, datetime.now().strftime('%Y-%m-%d_%H_%M_%S'))\n",
    "        else:\n",
    "            model_save_name = (self._best_ckeckpoint_name + '_pytorch_state_{}_{}.pth').format(tag, datetime.now().strftime('%Y-%m-%d_%H_%M_%S'))\n",
    "        checkpoint_list.append(model_save_name)\n",
    "\n",
    "        torch.save(model_state_dict, model_save_name)\n",
    "        torch.save(other_state_dict, model_save_name.replace('state', 'others'))\n",
    "\n",
    "        log.info('model saved: {}.'.format(model_save_name))\n",
    "\n",
    "        # remove checkpoint older than 5\n",
    "        if len(checkpoint_list) > self._max_to_keep:\n",
    "            checkpoint_list_to_delete = checkpoint_list[:-self._max_to_keep]\n",
    "            checkpoint_list = checkpoint_list[-self._max_to_keep:]\n",
    "            for model_file in checkpoint_list_to_delete:\n",
    "                if os.path.isfile(model_file): os.remove(model_file)\n",
    "                model_file = model_file.replace('state','others')\n",
    "                if os.path.isfile(model_file):  os.remove(model_file)\n",
    "        with open(self._ckeckpoint_container_file, 'w') as outfile:\n",
    "            outfile.write(model_save_name+'\\n')\n",
    "            for line in checkpoint_list:\n",
    "                outfile.write(line+'\\n')\n",
    "\n",
    "    def restore_from_checkpoint(self, model, step=None):\n",
    "        checkpoint_filename = None\n",
    "        if os.path.exists(self._ckeckpoint_container_file):\n",
    "            with open(self._ckeckpoint_container_file, 'r') as checkpoint_file:\n",
    "                for _, line in enumerate(checkpoint_file):\n",
    "                    line = line.strip()\n",
    "                    if line != '':\n",
    "                        # get the first one\n",
    "                        if step is None:\n",
    "                            checkpoint_filename = line\n",
    "                            break\n",
    "                        # get the specified one\n",
    "                        elif str(step) in line:\n",
    "                            checkpoint_filename = line\n",
    "                            break\n",
    "        if (not os.path.isdir(self._log_dir)) or (checkpoint_filename is None):\n",
    "            return None\n",
    "\n",
    "        model.load_state_dict(torch.load(checkpoint_filename, map_location=lambda storage, loc: storage))    \n",
    "\n",
    "        log.info('model resotred from: {}.'.format(checkpoint_filename))\n",
    "\n",
    "        return torch.load(checkpoint_filename.replace('state','others'))  \n",
    "\n",
    "class TimeRecorder(object):\n",
    "    def __init__(self):\n",
    "        super(TimeRecorder, self).__init__()\n",
    "        self._tick_map = dict()\n",
    "        self._recorder = dict()\n",
    "        self._use_time = dict()\n",
    "        self._last_interval = dict()\n",
    "        self.register_event('ticks', 1, True)\n",
    "    # register at the very begining\n",
    "    # when not use time, call reset_event with your initial value at each start\n",
    "    def register_event(self, name, tick, use_time=True):\n",
    "        self._tick_map[name] = tick\n",
    "        self._last_interval[name] = 0\n",
    "        self._recorder[name] = 0\n",
    "        if use_time:\n",
    "            self._recorder[name] = time.time() \n",
    "        self._use_time[name] = use_time\n",
    "    def reset_event(self, name, criterion=None):\n",
    "        if self._use_time[name]: self._recorder[name] = time.time()\n",
    "        elif criterion is not None: self._recorder[name] = criterion\n",
    "        self._last_interval[name] = 0\n",
    "    def cancel_event(self, name):\n",
    "        self._tick_map.pop(name, None)\n",
    "        self._last_interval.pop(name, None)\n",
    "        self._recorder.pop(name, None)\n",
    "        self._use_time.pop(name, None)\n",
    "    def get_ticks_passed(self):\n",
    "        passed = time.time() - self._recorder['ticks']\n",
    "        self._recorder['ticks'] = time.time()\n",
    "        return passed/self._tick_map['ticks']\n",
    "    # you can call this to get how log elapsed after you get True from check_for_me\n",
    "    def how_long_before(self, name):\n",
    "        return self._last_interval[name]\n",
    "    def check_for_me(self, name, criterion=None):\n",
    "        if self._use_time[name]:\n",
    "            if time.time() - self._recorder[name] >= self._tick_map[name]:\n",
    "                self._last_interval[name] = time.time() - self._recorder[name]\n",
    "                self._recorder[name] = time.time()\n",
    "                return True\n",
    "        elif criterion is not None:\n",
    "            if criterion - self._recorder[name] >= self._tick_map[name]:\n",
    "                self._last_interval[name] = criterion - self._recorder[name]\n",
    "                self._recorder[name] = criterion\n",
    "                return True\n",
    "        return False\n",
    "def load_pretrain_file(net, pretrain_file, skip=[]):\n",
    "    pretrain_state_dict = torch.load(pretrain_file)\n",
    "    state_dict = net.state_dict()\n",
    "    keys = list(state_dict.keys())\n",
    "    for key in keys:\n",
    "        if any(s in key for s in skip):\n",
    "            continue\n",
    "        pretrain_key = key\n",
    "        #if 'layer0.0.conv.' in key: pretrain_key=key.replace('layer0.0.conv.',  'conv1.' )\n",
    "        state_dict[key] = pretrain_state_dict[pretrain_key]\n",
    "        \n",
    "    net.load_state_dict(state_dict)\n",
    "    return net\n",
    "\n",
    "class CriterionSmooth(object):\n",
    "    def __init__(self):\n",
    "        super(CriterionSmooth, self).__init__()\n",
    "        self._history = dict()\n",
    "        self._factor = dict()\n",
    "        self._just_add = dict()\n",
    "    def register_smooth(self, name, factor=0.6):\n",
    "        self._history[name] = 0.\n",
    "        self._just_add[name] = False\n",
    "        if factor < 0: self._just_add[name] = True\n",
    "        if factor > 1.: factor=1.\n",
    "        self._factor[name] = 1. - factor\n",
    "    def push_new_value(self, name, value):\n",
    "        if self._just_add[name]: self._history[name] = self._history[name] + value\n",
    "        else: self._history[name] = self._history[name] * (1.-self._factor[name]) + value*self._factor[name]\n",
    "        return value\n",
    "    def smooth_value(self, name):\n",
    "        return self._history[name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.01, lr_decay_epoch=1):\n",
    "    lr = init_lr * (0.27**(epoch // lr_decay_epoch))\n",
    "\n",
    "    if lr < 0.000002: lr = 0.000002\n",
    "        \n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        log.info('LR is set to {}'.format(lr))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer\n",
    "def read_learning_rate(cur_step, num_steps_per_epoch):\n",
    "    def inner_lr_parser(interval_start, interval_end, lr, dict_in, default_lr, use_epoch_percent, num_steps_per_epoch):\n",
    "        lr = default_lr * lr\n",
    "        if use_epoch_percent:\n",
    "            interval_start = num_steps_per_epoch * interval_start\n",
    "            interval_end = num_steps_per_epoch * interval_end\n",
    "        interval_start = int(interval_start)\n",
    "        interval_end = int(interval_end)\n",
    "        if (interval_start < interval_end) and (lr > 0):\n",
    "            dict_in[(interval_start, interval_end)] = lr\n",
    "            \n",
    "    lr_map = dict()\n",
    "    default_lr = initial_learning_rate\n",
    "    stop_lr = stop_learning_rate\n",
    "    line_index = -1\n",
    "    use_epoch_percent = True\n",
    "    if os.path.exists(LR_FILE_PATH):\n",
    "        with open(LR_FILE_PATH, 'r') as lr_setting_file:\n",
    "            for _, line in enumerate(lr_setting_file):\n",
    "                line = line.strip()\n",
    "                if (line != '') and (not line.startswith('#')):\n",
    "                    line_index += 1\n",
    "                    if line_index == 0:\n",
    "                        default_lr = float(line.split(':')[-1].strip())\n",
    "                        continue\n",
    "                    if line_index == 1:\n",
    "                        stop_lr = float(line.split(':')[-1].strip())\n",
    "                        continue\n",
    "                    if line_index == 2:\n",
    "                        use_epoch_percent = ('EPOCHES_PERCENT' in (line.split(':')[-1].strip()))\n",
    "                        continue\n",
    "                    # this is a list desciption\n",
    "                    if line.startswith('['):\n",
    "                        line = [float(s.strip()) for s in line[1:-1].strip().split()]\n",
    "                        step_interval = (line[1] - line[0])/line[-1]\n",
    "                        lr_interval = (line[3] - line[2])/line[-1]\n",
    "                        begin = line[0]\n",
    "                        lr_begin = line[2]\n",
    "                        for index in range(int(line[-1])):\n",
    "                            inner_lr_parser(begin, begin+step_interval, lr_begin, lr_map, default_lr, use_epoch_percent, num_steps_per_epoch)\n",
    "                            begin += step_interval\n",
    "                            lr_begin += lr_interval\n",
    "                    else:\n",
    "                        interval_start, interval_end, lr = [float(s) for s in line.strip().split()]\n",
    "                        inner_lr_parser(interval_start, interval_end, lr, lr_map, default_lr, use_epoch_percent, num_steps_per_epoch)\n",
    "    lr_ret = default_lr\n",
    "#     print(use_epoch_percent)\n",
    "    for (start, end), lr in lr_map.items():\n",
    "        if (cur_step >= start) and (cur_step <= end):\n",
    "            if (lr < lr_ret):\n",
    "                lr_ret = lr\n",
    "    if lr_ret < stop_lr: lr_ret = stop_lr      \n",
    "    return lr_ret\n",
    "# _ = read_learning_rate(1, num_steps_per_epoch)\n",
    "# lr = []\n",
    "# num_epoches_to_show = 10\n",
    "# num_point = 100\n",
    "# for i in [i*num_epoches_to_show*num_steps_per_epoch/num_point for i in range(num_point)]:\n",
    "#     lr.append(read_learning_rate(i, num_steps_per_epoch))\n",
    "# plt.plot(lr)\n",
    "# plt.ylabel('learning rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabelMapping(object):\n",
    "    def __init__(self, catogory_file_path):\n",
    "        super(LabelMapping, self).__init__()\n",
    "        self._category_level_csv = catogory_file_path\n",
    "        self._category_map, self._category_level1_map, self._category_level2_map, self._len_level1, self._len_level2 = self.cvt_csv2tfrecord()\n",
    "        \n",
    "        assert (LEVEL1_CLASS == self._len_level1) and (LEVEL2_CLASS == self._len_level2), 'Other two levels are not mapped correctly.'\n",
    "        self._catogory_weight_map = self.cvt_catogory_weight()\n",
    "        self._mapping_strings = tf.constant( [ str(key) for key in self._category_map.keys() ] )\n",
    "        #print(list(self._category_map.keys())[0])\n",
    "        self._mapping_table = tf.contrib.lookup.index_table_from_tensor(mapping=self._mapping_strings, default_value=0) \n",
    "        \n",
    "        self._level1_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._category_level1_map.keys()), list(self._category_level1_map.values()), tf.int64, tf.int64), 0)\n",
    "        self._level2_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._category_level2_map.keys()), list(self._category_level2_map.values()), tf.int64, tf.int64), 0)\n",
    "        self._weight_table = tf.contrib.lookup.HashTable(tf.contrib.lookup.KeyValueTensorInitializer(list(self._catogory_weight_map.keys()), list(self._catogory_weight_map.values()), tf.int64, tf.float32), 0)\n",
    "\n",
    "    @property\n",
    "    def category_map(self):\n",
    "        return self._category_map\n",
    "    @property\n",
    "    def level1_table(self):\n",
    "        return self._level1_table\n",
    "    @property\n",
    "    def level2_table(self):\n",
    "        return self._level2_table\n",
    "    @property\n",
    "    def len_level1(self):\n",
    "        return self._len_level1\n",
    "    @property\n",
    "    def len_level2(self):\n",
    "        return self._len_level2\n",
    "    @property\n",
    "    def mapping_table(self):\n",
    "        return self._mapping_table\n",
    "    @property\n",
    "    def weight_table(self):\n",
    "        return self._weight_table\n",
    "    \n",
    "    def cvt_catogory_weight(self):\n",
    "        category_weight_map = dict()\n",
    "        csv = pd.read_csv(CATEGORY_WEIGHT_PATH).values\n",
    "        for row in csv:  \n",
    "            category_id, weight = row[0], row[2]\n",
    "#             if weight > 1.5:\n",
    "#                 weight = 1.5\n",
    "            category_weight_map[int(category_id)] = 1.\n",
    "\n",
    "        return category_weight_map\n",
    "\n",
    "    def cvt_csv2tfrecord(self):\n",
    "        level1_map, level2_map = self.create_level_map()\n",
    "        count = 0\n",
    "        category_map = dict()\n",
    "        category_level1_map = dict()\n",
    "        category_level2_map = dict()\n",
    "        csv = pd.read_csv(self._category_level_csv).values\n",
    "        for row in csv:  \n",
    "            category_id, level1, level2 = row[0], row[1], row[2]\n",
    "            category_map[category_id] = count\n",
    "            category_level1_map[int(category_id)] = level1_map[level1]\n",
    "            category_level2_map[int(category_id)] = level2_map[level2]\n",
    "            count += 1\n",
    "\n",
    "        return category_map, category_level1_map, category_level2_map, len(level1_map), len(level2_map)\n",
    "\n",
    "    def create_level_map(self):\n",
    "        csv = pd.read_csv(self._category_level_csv).values\n",
    "        level_list = [list(), list()]\n",
    "        for row in csv: \n",
    "            for level in range(1,3):\n",
    "                if row[level] not in level_list[level-1]:\n",
    "                    level_list[level-1].append(row[level])\n",
    "        return dict(zip(level_list[0], range(len(level_list[0])))), dict(zip(level_list[1], range(len(level_list[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CdiscountDataset(Dataset):\n",
    "    def __init__(self, sess, data_path, file_begin_match, label_mapping, num_examples, num_classes, buffer_size, batch_size, is_training):\n",
    "        super(CdiscountDataset, self).__init__()\n",
    "        self._data_file_list = [ os.path.join(data_path, x) for x in os.listdir(data_path) if lambda x: os.path.isfile(x) and file_begin_match in x ]\n",
    "        self._num_examples = num_examples\n",
    "        self._tf_sess = sess\n",
    "        self._num_classes = num_classes\n",
    "        self._batch_size = batch_size\n",
    "        self._buffer_size = buffer_size\n",
    "        self._is_training = is_training\n",
    "        self._category_map = label_mapping.category_map\n",
    "        self._level1_table = label_mapping.level1_table\n",
    "        self._level2_table = label_mapping.level2_table\n",
    "        self._len_level1 = label_mapping.len_level1\n",
    "        self._len_level2 = label_mapping.len_level2\n",
    "        self._mapping_table = label_mapping.mapping_table\n",
    "    def __len__(self):\n",
    "        return self._num_examples\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         #print('read',idx)\n",
    "#         try:\n",
    "#             next_example, next_label, next_level0_label, next_level1_label = self._tf_sess.run(self.get_next())\n",
    "#             #print(next_example, next_label, next_level0_label, next_level1_label)\n",
    "#         except tf.errors.OutOfRangeError:\n",
    "#             pass\n",
    "#         return (torch.from_numpy(next_example), torch.from_numpy(next_label), torch.from_numpy(next_level0_label), torch.from_numpy(next_level1_label))\n",
    "    \n",
    "    @staticmethod\n",
    "    def image_normalized(image):\n",
    "        mean = [0.485, 0.456, 0.406 ]\n",
    "        std  = [0.229, 0.224, 0.225 ]\n",
    "\n",
    "        image = image.transpose((2,0,1))\n",
    "        image = image.astype(float)/255.\n",
    "        \n",
    "        image[0] = (image[0] - mean[0]) / std[0]\n",
    "        image[1] = (image[1] - mean[1]) / std[1]\n",
    "        image[2] = (image[2] - mean[2]) / std[2]\n",
    "\n",
    "        return image.astype(np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_center_crop(image, size=(160,160)):\n",
    "\n",
    "        height, width = image.shape[0:2]\n",
    "        w,h = size\n",
    "\n",
    "        x0 = (width  -w)//2\n",
    "        y0 = (height -h)//2\n",
    "        x1 = x0 + w\n",
    "        y1 = y0 + h\n",
    "        image = image[y0:y1, x0:x1]\n",
    "\n",
    "        return image\n",
    "    @staticmethod\n",
    "    def random_horizontal_flip(image, u=0.5):\n",
    "        if random.random() < u:\n",
    "            image = np.flip(image,1)  #np.fliplr(img) ##left-right\n",
    "        return image\n",
    "    @staticmethod\n",
    "    def random_crop(image, size=(160,160), u=0.5):\n",
    "\n",
    "        height,width=image.shape[0:2]\n",
    "        w,h = size\n",
    "\n",
    "        if random.random() < u:\n",
    "            x0 = np.random.choice(width - w)\n",
    "            y0 = np.random.choice(height - h)\n",
    "        else:\n",
    "            x0 = (width  -w)//2\n",
    "            y0 = (height -h)//2\n",
    "\n",
    "        x1 = x0 + w\n",
    "        y1 = y0 + h\n",
    "        image = image[y0:y1, x0:x1]\n",
    "\n",
    "        return image\n",
    "    @staticmethod\n",
    "    def random_resize(image, scale_x_limits=[0.9, 1.1], scale_y_limits=[0.9, 1.1], u=0.5):\n",
    "        if random.random() < u:\n",
    "            height,width=image.shape[0:2]\n",
    "\n",
    "            scale_x  = random.uniform(scale_x_limits[0],scale_x_limits[1])\n",
    "            if scale_y_limits is not None:\n",
    "                scale_y  = random.uniform(scale_y_limits[0],scale_y_limits[1])\n",
    "            else:\n",
    "                scale_y = scale_x\n",
    "\n",
    "            w = int(scale_x*width )\n",
    "            h = int(scale_y*height)\n",
    "\n",
    "            image = imresize(image,(h,w))\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_by_pytorch_train(image):\n",
    "        image = CdiscountDataset.random_resize(image, scale_x_limits=[0.9,1.1], scale_y_limits=[0.9,1.1], u=0.5)\n",
    "        # flip  random ---------\n",
    "        image = CdiscountDataset.random_crop(image, size=(160,160), u=0.5) \n",
    "        image = CdiscountDataset.random_horizontal_flip(image, u=0.5)\n",
    "        return image.astype(np.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_by_pytorch_test(image):\n",
    "        image  = CdiscountDataset.fix_center_crop(image, size=(160,160))  \n",
    "        return image.astype(np.float32)\n",
    "    @staticmethod\n",
    "    def _preprocess_normalize(image):\n",
    "        return CdiscountDataset.image_normalized(image)\n",
    "    @staticmethod\n",
    "    def _array_to_image_transform(image):\n",
    "        mean = [0.485, 0.456, 0.406 ]\n",
    "        std  = [0.229, 0.224, 0.225 ]\n",
    "        if random.random() < 0.00001:\n",
    "            image_to_save = image\n",
    "            image_to_save[0] = image_to_save[0]*std[0] + mean[0]\n",
    "            image_to_save[1] = image_to_save[1]*std[1] + mean[1]\n",
    "            image_to_save[2] = image_to_save[2]*std[2] + mean[2]\n",
    "\n",
    "            image_to_save = image_to_save*255\n",
    "            image_to_save = np.transpose(image_to_save, (1, 2, 0))\n",
    "            image_to_save = image_to_save.astype(np.uint8)\n",
    "            imsave('/media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/Debug/{}.jpg'.format(int(random.random()*100000)), image_to_save)\n",
    "        return image\n",
    "    def _parse_function(self, example_proto):\n",
    "        features = {'img_raw': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "            'product_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            'category_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64))}\n",
    "                \n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        image = tf.image.decode_image(parsed_features[\"img_raw\"])\n",
    "        raw_label = parsed_features[\"category_id\"]\n",
    "        if self._is_training:\n",
    "            image = tf.py_func(CdiscountDataset._preprocess_by_pytorch_train, [image], tf.float32, stateful=True)\n",
    "            #tf.summary.image('final_train_image', tf.expand_dims(image, 0))\n",
    "        else:\n",
    "            image = tf.py_func(CdiscountDataset._preprocess_by_pytorch_test, [image], tf.float32, stateful=False)\n",
    "            #tf.summary.image('final_test_image', tf.expand_dims(image, 0))\n",
    "        image = tf.py_func(CdiscountDataset._preprocess_normalize, [image], tf.float32, stateful=False)\n",
    "        #image = tf.py_func(CdiscountDataset._array_to_image_transform, [image], tf.float32, stateful=False)\n",
    "        \n",
    "        return image, tf.one_hot(self._mapping_table.lookup(tf.as_string(raw_label)), self._num_classes, axis=-1, dtype=tf.int64),\\\n",
    "                tf.one_hot(self._level1_table.lookup(raw_label), self._len_level1, axis=-1, dtype=tf.int64),\\\n",
    "                tf.one_hot(self._level2_table.lookup(raw_label), self._len_level2, axis=-1, dtype=tf.int64)\n",
    "#         return image, self._mapping_table.lookup(tf.as_string(raw_label)),\\\n",
    "#                 self._level1_table.lookup(raw_label),\\\n",
    "#                 self._level2_table.lookup(raw_label)\n",
    "    \n",
    "    def get_next(self):\n",
    "        #next_example, next_label, next_level0_label, next_level1_label \n",
    "        return self._next_iter\n",
    "    def create_dataset(self):\n",
    "        self._dataset = tf.data.TFRecordDataset(self._data_file_list, compression_type='ZLIB', buffer_size = 409600)\n",
    "        parse_func = lambda example : self._parse_function(example)\n",
    "        self._dataset = self._dataset.map(parse_func, num_parallel_calls=INPUT_THREADS)\n",
    "        self._dataset = self._dataset.prefetch(self._batch_size * 3)\n",
    "        self._dataset = self._dataset.shuffle(buffer_size=self._buffer_size)\n",
    "        self._dataset = self._dataset.batch(self._batch_size)\n",
    "        # we don't want to repeat until finish training, instead we stop each one epoch finished\n",
    "        #self._dataset = self._dataset.repeat(self._num_epochs)\n",
    "        self._iterator = self._dataset.make_initializable_iterator()\n",
    "        self._next_iter = self._iterator.get_next()\n",
    "#             Compute for 100 epochs.\n",
    "#             for _ in range(100):\n",
    "#               sess.run(iterator.initializer)\n",
    "#               while True:\n",
    "#                 try:\n",
    "#                   sess.run(next_element)\n",
    "#                 except tf.errors.OutOfRangeError:\n",
    "#                   break\n",
    "#         map(\n",
    "#             map_func,\n",
    "#             num_threads=None,\n",
    "#             output_buffer_size=None,\n",
    "#             num_parallel_calls=None\n",
    "#         )\n",
    "#         Maps map_func across this datset. (deprecated arguments)\n",
    "\n",
    "#         SOME ARGUMENTS ARE DEPRECATED. They will be removed in a future version. Instructions for updating: Replace num_threads=T with num_parallel_calls=T. Replace output_buffer_size=N with ds.prefetch(N) on the returned dataset.\n",
    "\n",
    "#         Args:\n",
    "\n",
    "#         map_func: A function mapping a nested structure of tensors (having shapes and types defined by self.output_shapes and self.output_types) to another nested structure of tensors.\n",
    "#         num_threads: (Optional.) Deprecated, use num_parallel_calls instead.\n",
    "#         output_buffer_size: (Optional.) A tf.int64 scalar tf.Tensor, representing the maximum number of processed elements that will be buffered.\n",
    "#         num_parallel_calls: (Optional.) A tf.int32 scalar tf.Tensor, representing the number elements to process in parallel. If not specified, elements will be processed sequentially.\n",
    "        return self._iterator.initializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-12-03 23:10:59\n",
      "epoch 0 of 12 start...\n",
      "start from 0 of epoch 0.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.950\n",
      "\ttrain/batch_loss: 5.091\n",
      "\ttrain/batch_acc: 25.342\n",
      "\ttrain/smooth_train_acc: 18.096\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 4.057\n",
      "\ttrain/batch_acc: 36.301\n",
      "\ttrain/smooth_train_acc: 23.200\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 4.090\n",
      "\ttrain/batch_acc: 36.301\n",
      "\ttrain/smooth_train_acc: 25.898\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 3.835\n",
      "\ttrain/batch_acc: 31.507\n",
      "\ttrain/smooth_train_acc: 27.754\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 1000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 4.230\n",
      "\ttrain/batch_acc: 34.932\n",
      "\ttrain/smooth_train_acc: 29.202\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 1200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 4.059\n",
      "\ttrain/batch_acc: 28.767\n",
      "\ttrain/smooth_train_acc: 30.286\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 1400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 3.549\n",
      "\ttrain/batch_acc: 34.247\n",
      "\ttrain/smooth_train_acc: 31.341\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 1600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 3.488\n",
      "\ttrain/batch_acc: 36.301\n",
      "\ttrain/smooth_train_acc: 32.111\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 1800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.868\n",
      "\ttrain/batch_loss: 3.370\n",
      "\ttrain/batch_acc: 40.411\n",
      "\ttrain/smooth_train_acc: 32.767\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 2000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.868\n",
      "\ttrain/batch_loss: 3.552\n",
      "\ttrain/batch_acc: 37.671\n",
      "\ttrain/smooth_train_acc: 33.399\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 2200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.920\n",
      "\ttrain/batch_acc: 47.260\n",
      "\ttrain/smooth_train_acc: 33.969\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 2400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.466\n",
      "\ttrain/batch_acc: 37.671\n",
      "\ttrain/smooth_train_acc: 34.490\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 2600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 3.199\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 34.946\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 2800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.386\n",
      "\ttrain/batch_acc: 36.986\n",
      "\ttrain/smooth_train_acc: 35.381\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 3000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.217\n",
      "\ttrain/batch_acc: 34.247\n",
      "\ttrain/smooth_train_acc: 35.779\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 3200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.585\n",
      "\ttrain/batch_acc: 39.041\n",
      "\ttrain/smooth_train_acc: 36.162\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 3400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 3.524\n",
      "\ttrain/batch_acc: 36.301\n",
      "\ttrain/smooth_train_acc: 36.504\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 3600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.794\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 36.827\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 3800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 3.195\n",
      "\ttrain/batch_acc: 42.466\n",
      "\ttrain/smooth_train_acc: 37.115\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 4000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.276\n",
      "\ttrain/batch_acc: 42.466\n",
      "\ttrain/smooth_train_acc: 37.401\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 4200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.011\n",
      "\ttrain/batch_acc: 40.411\n",
      "\ttrain/smooth_train_acc: 37.652\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 4400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 3.210\n",
      "\ttrain/batch_acc: 44.521\n",
      "\ttrain/smooth_train_acc: 37.923\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 4600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 3.360\n",
      "\ttrain/batch_acc: 37.671\n",
      "\ttrain/smooth_train_acc: 38.186\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 4800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.735\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 38.413\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 5000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.049\n",
      "\ttrain/batch_acc: 45.205\n",
      "\ttrain/smooth_train_acc: 38.633\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 5200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.065\n",
      "\ttrain/batch_acc: 47.260\n",
      "\ttrain/smooth_train_acc: 38.832\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 5400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.990\n",
      "\ttrain/batch_acc: 45.890\n",
      "\ttrain/smooth_train_acc: 39.030\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 5600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 3.302\n",
      "\ttrain/batch_acc: 41.781\n",
      "\ttrain/smooth_train_acc: 39.242\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 5800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 3.125\n",
      "\ttrain/batch_acc: 41.781\n",
      "\ttrain/smooth_train_acc: 39.425\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 6000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 3.293\n",
      "\ttrain/batch_acc: 41.096\n",
      "\ttrain/smooth_train_acc: 39.601\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 6200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.873\n",
      "\ttrain/batch_acc: 43.151\n",
      "\ttrain/smooth_train_acc: 39.784\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 6400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.820\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 39.947\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 6600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.721\n",
      "\ttrain/batch_acc: 48.630\n",
      "\ttrain/smooth_train_acc: 40.113\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 6800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.646\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 40.264\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 7000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.856\n",
      "\ttrain/batch_acc: 45.890\n",
      "\ttrain/smooth_train_acc: 40.423\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 7200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 3.097\n",
      "\ttrain/batch_acc: 45.205\n",
      "\ttrain/smooth_train_acc: 40.591\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 7400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.917\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 40.736\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 7600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.953\n",
      "\ttrain/batch_acc: 41.781\n",
      "\ttrain/smooth_train_acc: 40.865\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 7800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.850\n",
      "\ttrain/batch_acc: 42.466\n",
      "\ttrain/smooth_train_acc: 41.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "####### train logging #######\n",
      "\ttrain/current_step: 8000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.727\n",
      "\ttrain/batch_acc: 45.205\n",
      "\ttrain/smooth_train_acc: 41.135\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 8200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.781\n",
      "\ttrain/batch_acc: 45.890\n",
      "\ttrain/smooth_train_acc: 41.270\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_8242_2017-12-04_01_10_52.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 8400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 3.003\n",
      "\ttrain/batch_acc: 43.151\n",
      "\ttrain/smooth_train_acc: 41.401\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 8600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.536\n",
      "\ttrain/batch_acc: 45.205\n",
      "\ttrain/smooth_train_acc: 41.522\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 8800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.726\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 41.640\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 9000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.591\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 41.753\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 9200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.496\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 41.864\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 9400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.998\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 41.982\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 9600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.802\n",
      "\ttrain/batch_acc: 43.836\n",
      "\ttrain/smooth_train_acc: 42.093\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 9800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.642\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 42.193\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 10000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.857\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 42.288\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 10200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.776\n",
      "\ttrain/batch_acc: 44.521\n",
      "\ttrain/smooth_train_acc: 42.391\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 10400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.576\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 42.485\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 10600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.634\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 42.593\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 10800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.331\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 42.692\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 11000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.515\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 42.784\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 11200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 3.193\n",
      "\ttrain/batch_acc: 43.151\n",
      "\ttrain/smooth_train_acc: 42.873\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 11400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.438\n",
      "\ttrain/batch_acc: 48.630\n",
      "\ttrain/smooth_train_acc: 42.960\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 11600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.927\n",
      "\ttrain/batch_acc: 41.781\n",
      "\ttrain/smooth_train_acc: 43.040\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 11800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.502\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 43.129\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 12000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.077\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 43.216\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 12200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.348\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 43.303\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 12400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.656\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 43.382\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 12600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.843\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 43.466\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 12800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.320\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 43.545\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 13000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.834\n",
      "\ttrain/batch_acc: 47.260\n",
      "\ttrain/smooth_train_acc: 43.623\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 13200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.716\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 43.701\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 13400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.403\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 43.767\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 13600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 3.110\n",
      "\ttrain/batch_acc: 40.411\n",
      "\ttrain/smooth_train_acc: 43.834\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 13800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.685\n",
      "\ttrain/batch_acc: 48.630\n",
      "\ttrain/smooth_train_acc: 43.909\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 14000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.201\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 43.979\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 14200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.792\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 44.054\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 14400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.962\n",
      "\ttrain/batch_acc: 44.521\n",
      "\ttrain/smooth_train_acc: 44.122\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 14600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.570\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 44.192\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 14800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.756\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 44.258\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 15000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.536\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 44.327\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 15200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.445\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 44.396\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 15400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.495\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 44.463\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 15600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.811\n",
      "\ttrain/batch_acc: 45.205\n",
      "\ttrain/smooth_train_acc: 44.522\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 15800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.923\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 44.584\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 16000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.473\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 44.645\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 16200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.637\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 44.710\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 16400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.990\n",
      "\ttrain/batch_acc: 42.466\n",
      "\ttrain/smooth_train_acc: 44.770\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_16491_2017-12-04_03_10_52.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 16600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.889\n",
      "\ttrain/batch_loss: 2.372\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 44.834\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 16800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.597\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 44.893\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 17000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.594\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 44.948\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 17200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.317\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 45.003\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 17400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.826\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 45.058\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 17600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.606\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 45.116\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 17800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.561\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 45.169\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 18000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.520\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 45.223\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 18200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.496\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 45.277\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 18400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.433\n",
      "\ttrain/batch_acc: 44.521\n",
      "\ttrain/smooth_train_acc: 45.325\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 18600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.410\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 45.374\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 18800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.829\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 45.426\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 19000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.564\n",
      "\ttrain/batch_acc: 45.890\n",
      "\ttrain/smooth_train_acc: 45.476\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 19200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.459\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 45.527\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 19400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.658\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 45.578\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 19600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.545\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 45.628\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 19800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.365\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 45.677\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 20000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.160\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 45.724\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 20200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.703\n",
      "\ttrain/batch_acc: 48.630\n",
      "\ttrain/smooth_train_acc: 45.774\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 20400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.774\n",
      "\ttrain/batch_acc: 48.630\n",
      "\ttrain/smooth_train_acc: 45.818\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 20600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.568\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 45.865\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 20800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.452\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 45.913\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 21000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.420\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 45.958\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 21200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.544\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 46.006\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 21400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.704\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 46.050\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 21600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.375\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 46.096\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 21800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.01\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.440\n",
      "\ttrain/batch_acc: 47.260\n",
      "\ttrain/smooth_train_acc: 46.138\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 22000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.137\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 46.184\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 22200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.579\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 46.230\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 22400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.716\n",
      "\ttrain/batch_acc: 45.205\n",
      "\ttrain/smooth_train_acc: 46.281\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 22600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.509\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 46.327\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 22800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.688\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 46.371\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 23000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.576\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 46.411\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 23200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.717\n",
      "\ttrain/batch_acc: 48.630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/smooth_train_acc: 46.456\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 23400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.478\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 46.500\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 23600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.252\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 46.549\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 23800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.148\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 46.591\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 24000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.867\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 46.635\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 24200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.346\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 46.677\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 24400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.712\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 46.718\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 24600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.492\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 46.760\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_24741_2017-12-04_05_10_53.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 24800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 2.557\n",
      "\ttrain/batch_acc: 48.630\n",
      "\ttrain/smooth_train_acc: 46.801\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 25000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.011\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 46.841\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 25200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.509\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 46.878\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 25400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.854\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 46.920\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 25600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.468\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 46.962\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 25800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.412\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 47.004\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 26000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 1.968\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 47.044\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 26200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.596\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 47.087\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 26400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.560\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 47.129\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 26600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.236\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 47.169\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 26800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.515\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 47.209\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 27000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.503\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 47.249\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 27200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.934\n",
      "\ttrain/batch_acc: 44.521\n",
      "\ttrain/smooth_train_acc: 47.285\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 27400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 1.857\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 47.320\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 27600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.210\n",
      "\ttrain/batch_acc: 60.274\n",
      "\ttrain/smooth_train_acc: 47.356\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 27800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.471\n",
      "\ttrain/batch_acc: 44.521\n",
      "\ttrain/smooth_train_acc: 47.393\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 28000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.313\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 47.430\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 28200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.245\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 47.467\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 28400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.545\n",
      "\ttrain/batch_acc: 48.630\n",
      "\ttrain/smooth_train_acc: 47.502\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 28600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.612\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 47.540\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 28800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.603\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 47.573\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 29000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.307\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 47.607\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 29200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.409\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 47.644\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 29400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.059\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 47.678\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 29600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.271\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 47.714\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 29800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.555\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 47.747\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 30000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.333\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 47.777\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 30200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.376\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 47.810\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 30400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.613\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 47.845\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 30600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.105\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 47.876\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 30800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/batch_acc: 48.630\n",
      "\ttrain/smooth_train_acc: 47.907\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 31000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.614\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 47.940\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 31200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 1.715\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 47.975\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 31400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.658\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 48.004\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 31600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.151\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 48.033\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 31800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.391\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 48.065\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 32000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.486\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 48.090\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 32200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.256\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 48.120\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 32400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.162\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 48.151\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 32600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.374\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 48.180\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 32800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.507\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 48.211\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_32998_2017-12-04_07_10_53.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 33000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.889\n",
      "\ttrain/batch_loss: 2.284\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 48.240\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 33200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.275\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 48.266\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 33400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.572\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 48.294\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 33600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.270\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 48.327\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 33800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.272\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 48.354\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 34000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.328\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 48.381\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 34200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.407\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 48.410\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 34400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.365\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 48.436\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 34600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.109\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 48.464\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 34800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.297\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 48.494\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 35000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.277\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 48.519\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 35200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.075\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 48.549\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 35400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.296\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 48.576\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 35600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.307\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 48.603\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 35800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.440\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 48.628\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 36000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.264\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 48.654\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 36200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.816\n",
      "\ttrain/batch_acc: 47.945\n",
      "\ttrain/smooth_train_acc: 48.679\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 36400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.235\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 48.706\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 36600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 1.867\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 48.730\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 36800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.232\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 48.755\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 37000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.204\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 48.782\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 37200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.413\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 48.811\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 37400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.174\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 48.834\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 37600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.398\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 48.860\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 37800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.869\n",
      "\ttrain/batch_loss: 2.332\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 48.883\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 38000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.483\n",
      "\ttrain/batch_acc: 47.260\n",
      "\ttrain/smooth_train_acc: 48.906\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 38200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.049\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 48.928\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 38400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/batch_loss: 2.269\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 48.950\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 38600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.645\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 48.974\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 38800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.174\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 48.997\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 39000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.419\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 49.019\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 39200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.429\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 49.044\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 39400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.045\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 49.066\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 39600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.237\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 49.091\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 39800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 1.527\n",
      "\ttrain/batch_acc: 69.178\n",
      "\ttrain/smooth_train_acc: 49.117\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 40000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.163\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 49.140\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 40200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.307\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 49.162\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 40400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.268\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 49.187\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 40600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.305\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 49.208\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 40800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.325\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 49.231\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 41000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 1.838\n",
      "\ttrain/batch_acc: 61.644\n",
      "\ttrain/smooth_train_acc: 49.253\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 41200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.056\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 49.274\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_41256_2017-12-04_09_10_54.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 41400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.891\n",
      "\ttrain/batch_loss: 2.777\n",
      "\ttrain/batch_acc: 43.151\n",
      "\ttrain/smooth_train_acc: 49.297\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 41600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 1.939\n",
      "\ttrain/batch_acc: 63.014\n",
      "\ttrain/smooth_train_acc: 49.318\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 41800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.089\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 49.338\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 42000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.389\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 49.359\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 42200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.622\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 49.378\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 42400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.642\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 49.399\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 42600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.699\n",
      "\ttrain/batch_acc: 46.575\n",
      "\ttrain/smooth_train_acc: 49.419\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 42800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.00875\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.532\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 49.440\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 43000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.363\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 49.461\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 43200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.254\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 49.483\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 43400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.214\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 49.508\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 43600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 1.958\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 49.532\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 43800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.006\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 49.554\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 44000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 1.950\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 49.579\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 44200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.242\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 49.601\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 44400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.383\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 49.625\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 44600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.132\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 49.647\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 44800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 1.978\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 49.672\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 45000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.479\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 49.692\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 45200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.847\n",
      "\ttrain/batch_acc: 44.521\n",
      "\ttrain/smooth_train_acc: 49.713\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 45400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.636\n",
      "\ttrain/batch_acc: 43.836\n",
      "\ttrain/smooth_train_acc: 49.735\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 45600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.523\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 49.758\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 45800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.178\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 49.780\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 46000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/batch_loss: 2.333\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 49.803\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 46200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.529\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 49.823\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 46400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.420\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 49.844\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 46600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.149\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 49.867\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 46800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.478\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 49.887\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 47000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.414\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 49.909\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 47200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.615\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 49.932\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 47400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.014\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 49.954\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 47600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.168\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 49.974\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 47800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 1.888\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 49.995\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 48000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.066\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 50.015\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 48200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 2.196\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 50.037\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 48400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.317\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 50.056\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 48600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 1.889\n",
      "\ttrain/batch_acc: 62.329\n",
      "\ttrain/smooth_train_acc: 50.075\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 48800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.880\n",
      "\ttrain/batch_loss: 2.300\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 50.096\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 49000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 1.817\n",
      "\ttrain/batch_acc: 62.329\n",
      "\ttrain/smooth_train_acc: 50.115\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 49200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 1.950\n",
      "\ttrain/batch_acc: 63.699\n",
      "\ttrain/smooth_train_acc: 50.135\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 49400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.580\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 50.153\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_49485_2017-12-04_11_10_54.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 49600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.894\n",
      "\ttrain/batch_loss: 1.853\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 50.171\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 49800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 2.426\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 50.192\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 50000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 2.083\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 50.209\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 50200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 1.844\n",
      "\ttrain/batch_acc: 63.014\n",
      "\ttrain/smooth_train_acc: 50.229\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 50400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.879\n",
      "\ttrain/batch_loss: 2.292\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 50.250\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 50600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.879\n",
      "\ttrain/batch_loss: 2.183\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 50.270\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 50800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 1.932\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 50.289\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 51000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.185\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 50.305\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 51200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 1.929\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 50.325\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 51400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 1.993\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 50.343\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 51600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 2.292\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 50.360\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 51800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.879\n",
      "\ttrain/batch_loss: 2.320\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 50.378\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 52000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.138\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 50.396\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 52200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.424\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 50.414\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 52400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.296\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 50.429\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 52600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 1.537\n",
      "\ttrain/batch_acc: 67.808\n",
      "\ttrain/smooth_train_acc: 50.448\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 52800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.282\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 50.465\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 53000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.367\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 50.483\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 53200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.271\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 50.499\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 53400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 1.885\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 50.517\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 53600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 50.536\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 53800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.199\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 50.554\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 54000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 1.918\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 50.571\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 54200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.527\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 50.587\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 54400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.117\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 50.603\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 54600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.172\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 50.621\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 54800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.010\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 50.638\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 55000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.088\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 50.653\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 55200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.145\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 50.669\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 55400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.131\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 50.685\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 55600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.486\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 50.704\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 55800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.320\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 50.720\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 56000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.037\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 50.735\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 56200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.361\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 50.752\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 56400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.158\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 50.768\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 56600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.351\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 50.784\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 56800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.191\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 50.800\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 57000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 1.955\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 50.818\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 57200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 1.875\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 50.834\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 57400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 1.784\n",
      "\ttrain/batch_acc: 63.699\n",
      "\ttrain/smooth_train_acc: 50.849\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 57600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.264\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 50.865\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_57695_2017-12-04_13_10_55.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 57800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.894\n",
      "\ttrain/batch_loss: 2.448\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 50.879\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 58000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.441\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 50.895\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 58200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.247\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 50.910\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 58400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.534\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 50.925\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 58600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.216\n",
      "\ttrain/batch_acc: 52.055\n",
      "\ttrain/smooth_train_acc: 50.940\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 58800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 1.899\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 50.955\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 59000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.182\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 50.969\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 59200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.208\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 50.985\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 59400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.426\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 51.001\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 59600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.524\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 51.016\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 59800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 1.741\n",
      "\ttrain/batch_acc: 62.329\n",
      "\ttrain/smooth_train_acc: 51.031\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 60000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.878\n",
      "\ttrain/batch_loss: 2.470\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 51.046\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 60200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.240\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 51.061\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 60400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.036\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 51.075\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 60600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.183\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 51.090\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 60800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.080\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 51.106\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 61000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.048\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 51.119\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 61200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.445\n",
      "\ttrain/batch_acc: 54.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/smooth_train_acc: 51.133\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 61400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 1.856\n",
      "\ttrain/batch_acc: 61.644\n",
      "\ttrain/smooth_train_acc: 51.147\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 61600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.880\n",
      "\ttrain/batch_loss: 2.210\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 51.160\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 61800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 1.012\n",
      "\ttrain/batch_loss: 1.930\n",
      "\ttrain/batch_acc: 62.329\n",
      "\ttrain/smooth_train_acc: 51.173\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 62000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.868\n",
      "\ttrain/batch_loss: 2.345\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 51.189\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 62200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.859\n",
      "\ttrain/batch_loss: 2.081\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 51.201\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 62400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.047\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 51.214\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 62600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.857\n",
      "\ttrain/batch_loss: 2.296\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 51.228\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 62800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0075\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.059\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 51.242\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 63000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0055000000000000005\n",
      "\ttrain/sec_per_step: 0.857\n",
      "\ttrain/batch_loss: 2.284\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 51.253\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 63200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0055000000000000005\n",
      "\ttrain/sec_per_step: 0.857\n",
      "\ttrain/batch_loss: 2.344\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 51.269\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 63400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0055000000000000005\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.180\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 51.284\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 63600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0055000000000000005\n",
      "\ttrain/sec_per_step: 0.857\n",
      "\ttrain/batch_loss: 2.188\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 51.300\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 63800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0055000000000000005\n",
      "\ttrain/sec_per_step: 0.857\n",
      "\ttrain/batch_loss: 1.756\n",
      "\ttrain/batch_acc: 62.329\n",
      "\ttrain/smooth_train_acc: 51.317\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 64000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.855\n",
      "\ttrain/batch_loss: 2.162\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 51.332\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 64200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.855\n",
      "\ttrain/batch_loss: 2.163\n",
      "\ttrain/batch_acc: 50.000\n",
      "\ttrain/smooth_train_acc: 51.349\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 64400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.855\n",
      "\ttrain/batch_loss: 1.837\n",
      "\ttrain/batch_acc: 64.384\n",
      "\ttrain/smooth_train_acc: 51.367\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 64600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.855\n",
      "\ttrain/batch_loss: 2.834\n",
      "\ttrain/batch_acc: 43.836\n",
      "\ttrain/smooth_train_acc: 51.387\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 64800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 1.994\n",
      "\ttrain/batch_acc: 60.274\n",
      "\ttrain/smooth_train_acc: 51.405\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 65000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.049\n",
      "\ttrain/batch_acc: 61.644\n",
      "\ttrain/smooth_train_acc: 51.423\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 65200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.103\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 51.442\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 65400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.857\n",
      "\ttrain/batch_loss: 1.948\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 51.461\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 65600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.865\n",
      "\ttrain/batch_loss: 2.087\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 51.480\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 65800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.206\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 51.500\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_65959_2017-12-04_15_10_55.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 66000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 2.138\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 51.518\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 66200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.150\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 51.535\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 66400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.068\n",
      "\ttrain/batch_acc: 55.479\n",
      "\ttrain/smooth_train_acc: 51.554\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 66600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.870\n",
      "\ttrain/batch_loss: 2.046\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 51.572\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 66800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 1.893\n",
      "\ttrain/batch_acc: 64.384\n",
      "\ttrain/smooth_train_acc: 51.590\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 67000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.871\n",
      "\ttrain/batch_loss: 2.495\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 51.608\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 67200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.278\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 51.628\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 67400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 1.021\n",
      "\ttrain/batch_loss: 1.830\n",
      "\ttrain/batch_acc: 61.644\n",
      "\ttrain/smooth_train_acc: 51.647\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 67600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.901\n",
      "\ttrain/batch_loss: 2.498\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 51.664\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 67800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.876\n",
      "\ttrain/batch_loss: 2.144\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 51.682\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 68000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.254\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 51.699\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 68200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 2.389\n",
      "\ttrain/batch_acc: 52.740\n",
      "\ttrain/smooth_train_acc: 51.716\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 68400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.884\n",
      "\ttrain/batch_loss: 2.013\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 51.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "####### train logging #######\n",
      "\ttrain/current_step: 68600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.884\n",
      "\ttrain/batch_loss: 2.427\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 51.750\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 68800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.004000000000000001\n",
      "\ttrain/sec_per_step: 0.884\n",
      "\ttrain/batch_loss: 2.283\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 51.768\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 69000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.885\n",
      "\ttrain/batch_loss: 2.153\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 51.785\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 69200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 2.396\n",
      "\ttrain/batch_acc: 47.260\n",
      "\ttrain/smooth_train_acc: 51.803\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 69400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 2.591\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 51.821\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 69600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.885\n",
      "\ttrain/batch_loss: 2.205\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 51.838\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 69800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 1.950\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 51.855\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 70000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.888\n",
      "\ttrain/batch_loss: 2.040\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 51.873\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 70200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.889\n",
      "\ttrain/batch_loss: 2.041\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 51.891\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 70400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.888\n",
      "\ttrain/batch_loss: 2.112\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 51.909\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 70600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.888\n",
      "\ttrain/batch_loss: 2.002\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 51.926\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 70800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.888\n",
      "\ttrain/batch_loss: 1.862\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 51.944\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 71000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 1.925\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 51.961\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 71200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.892\n",
      "\ttrain/batch_loss: 1.962\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 51.978\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 71400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.894\n",
      "\ttrain/batch_loss: 1.719\n",
      "\ttrain/batch_acc: 69.863\n",
      "\ttrain/smooth_train_acc: 51.995\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 71600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.894\n",
      "\ttrain/batch_loss: 1.912\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.013\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 71800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.888\n",
      "\ttrain/batch_loss: 1.823\n",
      "\ttrain/batch_acc: 60.274\n",
      "\ttrain/smooth_train_acc: 52.030\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 72000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 1.983\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.048\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 72200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 2.029\n",
      "\ttrain/batch_acc: 61.644\n",
      "\ttrain/smooth_train_acc: 52.065\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 72400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.882\n",
      "\ttrain/batch_loss: 1.776\n",
      "\ttrain/batch_acc: 65.753\n",
      "\ttrain/smooth_train_acc: 52.083\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 72600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.888\n",
      "\ttrain/batch_loss: 1.834\n",
      "\ttrain/batch_acc: 63.014\n",
      "\ttrain/smooth_train_acc: 52.101\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 72800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.894\n",
      "\ttrain/batch_loss: 1.975\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.119\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 73000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.893\n",
      "\ttrain/batch_loss: 2.074\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 52.136\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 73200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.893\n",
      "\ttrain/batch_loss: 2.135\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 52.153\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 73400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.890\n",
      "\ttrain/batch_loss: 2.079\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 52.170\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 73600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.890\n",
      "\ttrain/batch_loss: 2.024\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 52.188\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 73800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.886\n",
      "\ttrain/batch_loss: 1.892\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 52.206\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 74000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 2.234\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 52.223\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_74065_2017-12-04_17_10_55.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 74200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.895\n",
      "\ttrain/batch_loss: 2.390\n",
      "\ttrain/batch_acc: 54.110\n",
      "\ttrain/smooth_train_acc: 52.240\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 74400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.884\n",
      "\ttrain/batch_loss: 2.292\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 52.255\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 74600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.887\n",
      "\ttrain/batch_loss: 2.031\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 52.273\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 74800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.883\n",
      "\ttrain/batch_loss: 1.881\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 52.292\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 75000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.872\n",
      "\ttrain/batch_loss: 1.818\n",
      "\ttrain/batch_acc: 60.274\n",
      "\ttrain/smooth_train_acc: 52.309\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 75200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.064\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 52.326\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 75400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.877\n",
      "\ttrain/batch_loss: 1.728\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 52.343\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 75600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 1.880\n",
      "\ttrain/batch_acc: 63.014\n",
      "\ttrain/smooth_train_acc: 52.359\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 75800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.013\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.376\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 76000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.874\n",
      "\ttrain/batch_loss: 2.177\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 52.391\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 76200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.216\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 52.407\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 76400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.295\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 52.425\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 76600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.873\n",
      "\ttrain/batch_loss: 2.092\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 52.440\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 76800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.882\n",
      "\ttrain/batch_loss: 2.121\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 52.457\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 77000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.885\n",
      "\ttrain/batch_loss: 1.947\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 52.474\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 77200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.912\n",
      "\ttrain/batch_loss: 1.793\n",
      "\ttrain/batch_acc: 64.384\n",
      "\ttrain/smooth_train_acc: 52.490\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 77400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.910\n",
      "\ttrain/batch_loss: 1.830\n",
      "\ttrain/batch_acc: 60.274\n",
      "\ttrain/smooth_train_acc: 52.506\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 77600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.875\n",
      "\ttrain/batch_loss: 2.155\n",
      "\ttrain/batch_acc: 50.685\n",
      "\ttrain/smooth_train_acc: 52.523\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 77800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.858\n",
      "\ttrain/batch_loss: 1.837\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 52.539\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 78000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.858\n",
      "\ttrain/batch_loss: 1.913\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 52.554\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 78200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.859\n",
      "\ttrain/batch_loss: 1.792\n",
      "\ttrain/batch_acc: 63.014\n",
      "\ttrain/smooth_train_acc: 52.571\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 78400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.859\n",
      "\ttrain/batch_loss: 2.175\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 52.586\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 78600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.859\n",
      "\ttrain/batch_loss: 2.011\n",
      "\ttrain/batch_acc: 59.589\n",
      "\ttrain/smooth_train_acc: 52.601\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 78800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.859\n",
      "\ttrain/batch_loss: 1.905\n",
      "\ttrain/batch_acc: 62.329\n",
      "\ttrain/smooth_train_acc: 52.616\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 79000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.857\n",
      "\ttrain/batch_loss: 2.045\n",
      "\ttrain/batch_acc: 57.534\n",
      "\ttrain/smooth_train_acc: 52.631\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 79200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 1.860\n",
      "\ttrain/batch_acc: 60.274\n",
      "\ttrain/smooth_train_acc: 52.645\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 79400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.354\n",
      "\ttrain/batch_acc: 49.315\n",
      "\ttrain/smooth_train_acc: 52.659\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 79600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.080\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 52.674\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 79800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.858\n",
      "\ttrain/batch_loss: 2.397\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 52.689\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 80000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.858\n",
      "\ttrain/batch_loss: 1.713\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 52.704\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 80200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.859\n",
      "\ttrain/batch_loss: 1.988\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.719\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 80400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.858\n",
      "\ttrain/batch_loss: 1.694\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 52.733\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 80600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.859\n",
      "\ttrain/batch_loss: 1.815\n",
      "\ttrain/batch_acc: 63.699\n",
      "\ttrain/smooth_train_acc: 52.748\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 80800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.858\n",
      "\ttrain/batch_loss: 2.102\n",
      "\ttrain/batch_acc: 56.849\n",
      "\ttrain/smooth_train_acc: 52.763\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 81000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.859\n",
      "\ttrain/batch_loss: 2.035\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.777\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 81200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.858\n",
      "\ttrain/batch_loss: 2.205\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 52.790\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 81400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.092\n",
      "\ttrain/batch_acc: 53.425\n",
      "\ttrain/smooth_train_acc: 52.804\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 81600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 2.147\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 52.818\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 81800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 1.951\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 52.833\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 82000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.856\n",
      "\ttrain/batch_loss: 1.856\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.846\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 82200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.869\n",
      "\ttrain/batch_loss: 2.125\n",
      "\ttrain/batch_acc: 58.904\n",
      "\ttrain/smooth_train_acc: 52.862\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_82352_2017-12-04_19_10_55.pth.\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 82400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.897\n",
      "\ttrain/batch_loss: 1.672\n",
      "\ttrain/batch_acc: 60.959\n",
      "\ttrain/smooth_train_acc: 52.876\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 82600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.885\n",
      "\ttrain/batch_loss: 1.932\n",
      "\ttrain/batch_acc: 61.644\n",
      "\ttrain/smooth_train_acc: 52.891\n",
      "####### train logging #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ttrain/current_step: 82800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.885\n",
      "\ttrain/batch_loss: 2.156\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 52.907\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 83000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.886\n",
      "\ttrain/batch_loss: 2.241\n",
      "\ttrain/batch_acc: 51.370\n",
      "\ttrain/smooth_train_acc: 52.920\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 83200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.883\n",
      "\ttrain/batch_loss: 1.964\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.934\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 83400, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.884\n",
      "\ttrain/batch_loss: 1.970\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 52.948\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 83600, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.884\n",
      "\ttrain/batch_loss: 2.218\n",
      "\ttrain/batch_acc: 58.219\n",
      "\ttrain/smooth_train_acc: 52.961\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 83800, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.886\n",
      "\ttrain/batch_loss: 2.119\n",
      "\ttrain/batch_acc: 56.164\n",
      "\ttrain/smooth_train_acc: 52.976\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 84000, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.888\n",
      "\ttrain/batch_loss: 1.966\n",
      "\ttrain/batch_acc: 62.329\n",
      "\ttrain/smooth_train_acc: 52.990\n",
      "####### train logging #######\n",
      "\ttrain/current_step: 84200, cur_epoch: 0/84259\n",
      "\ttrain/current_lr: 0.0032500000000000007\n",
      "\ttrain/sec_per_step: 0.885\n",
      "\ttrain/batch_loss: 2.209\n",
      "\ttrain/batch_acc: 54.795\n",
      "\ttrain/smooth_train_acc: 53.004\n",
      "epoch 0 finished.\n",
      "model saved: /media/rs/0E06CD1706CD0127/Kapok/kaggle/pytorch/cdiscount/logs_resnet/Resnet_pytorch_state_84259_2017-12-04_19_39_05.pth.\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 201/2175\n",
      "\tvalidation/sec_per_step: 0.384\n",
      "\tvalidation/batch_acc: 50.000\n",
      "\tvalidation/smooth_validation_acc: 61.324\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 401/2175\n",
      "\tvalidation/sec_per_step: 0.073\n",
      "\tvalidation/batch_acc: 62.500\n",
      "\tvalidation/smooth_validation_acc: 60.906\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 601/2175\n",
      "\tvalidation/sec_per_step: 0.074\n",
      "\tvalidation/batch_acc: 56.250\n",
      "\tvalidation/smooth_validation_acc: 60.642\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 801/2175\n",
      "\tvalidation/sec_per_step: 0.075\n",
      "\tvalidation/batch_acc: 65.625\n",
      "\tvalidation/smooth_validation_acc: 60.801\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 1001/2175\n",
      "\tvalidation/sec_per_step: 0.074\n",
      "\tvalidation/batch_acc: 65.625\n",
      "\tvalidation/smooth_validation_acc: 60.816\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 1201/2175\n",
      "\tvalidation/sec_per_step: 0.074\n",
      "\tvalidation/batch_acc: 59.375\n",
      "\tvalidation/smooth_validation_acc: 60.597\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 1401/2175\n",
      "\tvalidation/sec_per_step: 0.074\n",
      "\tvalidation/batch_acc: 62.500\n",
      "\tvalidation/smooth_validation_acc: 60.601\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 1601/2175\n",
      "\tvalidation/sec_per_step: 0.075\n",
      "\tvalidation/batch_acc: 68.750\n",
      "\tvalidation/smooth_validation_acc: 60.653\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 1801/2175\n",
      "\tvalidation/sec_per_step: 0.074\n",
      "\tvalidation/batch_acc: 62.500\n",
      "\tvalidation/smooth_validation_acc: 60.605\n",
      "####### validation logging #######\n",
      "\tvalidation/current_step: 2001/2175\n",
      "\tvalidation/sec_per_step: 0.072\n",
      "\tvalidation/batch_acc: 62.500\n",
      "\tvalidation/smooth_validation_acc: 60.588\n",
      "epoch 1 of 12 start...\n",
      "start from 0 of epoch 1.\n",
      "Error raised.\n",
      "Training did't finished.\n",
      "Done: 2017-12-04_19_42_40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "torch_saver = Saver(LOG_DIR, 'Resnet', 5)\n",
    "\n",
    "smoother = CriterionSmooth()\n",
    "smoother.register_smooth('train_correct', -1.)\n",
    "smoother.register_smooth('train_total', -1.)\n",
    "smoother.register_smooth('val_correct', -1.)\n",
    "smoother.register_smooth('val_total', -1.)\n",
    "\n",
    "timer_holder = TimeRecorder()\n",
    "timer_holder.register_event('save_time', save_time_interval)\n",
    "timer_holder.register_event('log_time', log_step_interval, False)\n",
    "timer_holder.register_event('eval_log_time', log_step_interval, False)\n",
    "\n",
    "cdiscount_net = torch_resnet.resnet152(pretrained=False, num_classes=NUM_CLASS)\n",
    "#print(cdiscount_net)\n",
    "if torch.cuda.is_available():\n",
    "    cdiscount_net = cdiscount_net.cuda()\n",
    "\n",
    "for param in cdiscount_net.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in cdiscount_net.bn1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in cdiscount_net.layer1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in cdiscount_net.layer2.parameters():\n",
    "    param.requires_grad = False\n",
    "# for param in cdiscount_net.layer3.parameters():\n",
    "#     param.requires_grad = False\n",
    "            \n",
    "#criterion = nn.NLLLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#optimizer = optim.SGD(cdiscount_net.parameters(), lr=initial_learning_rate, momentum=momentum, weight_decay=0.0001)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cdiscount_net.parameters()), lr=initial_learning_rate, momentum=momentum, weight_decay=0.0001)\n",
    "\n",
    "if True:\n",
    "    cdiscount_net = load_pretrain_file(cdiscount_net, PRETRAINED_MODEL_PATH, ['fc'])\n",
    "    others_state = None\n",
    "else:\n",
    "    others_state = torch_saver.restore_from_checkpoint(cdiscount_net, step=None)\n",
    "start_iter = 0\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "if others_state is not None:\n",
    "    start_iter  = others_state['iter']\n",
    "    start_epoch = others_state['epoch']\n",
    "    global_step  = others_state['global_step']\n",
    "    optimizer.load_state_dict(others_state['optimizer'])\n",
    "\n",
    "label_mapping = LabelMapping(CATEGORY_NAME_PATH)\n",
    "train_set = CdiscountDataset(tf_sess, TRAIN_PATH, 'output_file', label_mapping, TOTAL_EXAMPLES, NUM_CLASS, 10000, BATCH_SIZE, True)\n",
    "val_set = CdiscountDataset(tf_sess, VAL_PATH, 'output_file', label_mapping, VAL_EXAMPLES, NUM_CLASS, 12000, VAL_BATCH_SIZE, False)\n",
    "\n",
    "train_initializer = train_set.create_dataset()\n",
    "val_initializer = val_set.create_dataset()\n",
    "    \n",
    "tf_sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer()))\n",
    "#tf_sess.run(train_initializer)\n",
    "#print(tf_sess.run(train_set.get_next()))\n",
    "#tf_sess.run(train_set[0])\n",
    "\n",
    "# train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers = 4, drop_last=False)\n",
    "# val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers = 4, drop_last=False)\n",
    "\n",
    "# initialize tensorboard_logger\n",
    "summaries_dir = LOG_DIR + \"/tensorboard_logger_{}\".format(datetime.now().strftime('%Y-%m-%d_%H_%M_%S'))\n",
    "configure(summaries_dir, flush_secs=120)\n",
    "# summary_writer = tf.summary.FileWriter(summaries_dir, tf_sess.graph)\n",
    "\n",
    "# merged_summary = tf.summary.merge_all()\n",
    "# summary = tf_sess.run(merged_summary)\n",
    "# summary_writer.add_summary(summary, 0)\n",
    "        \n",
    "log.info(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "#target_onehot = torch.FloatTensor(batch_size, 4)\n",
    "timer_holder.reset_event('log_time', global_step)\n",
    "MDEBUG = False\n",
    "try:\n",
    "#if True:\n",
    "    for epoch in range(NUM_EPOCHES):\n",
    "        if epoch < start_epoch: continue\n",
    "        log.info('epoch {} of {} start...'.format(epoch, NUM_EPOCHES))\n",
    "        log.info('start from {} of epoch {}.'.format(start_iter, epoch))\n",
    "        cdiscount_net.train()\n",
    "        tf_sess.run(train_initializer)\n",
    "        for index in range(start_iter, len(train_set)):\n",
    "            try:\n",
    "                data = tf_sess.run(train_set.get_next())\n",
    "                #next_example, next_label, next_level0_label, next_level1_label = [torch.from_numpy(array) for array in data]\n",
    "                next_example, next_label, next_level0_label, next_level1_label = torch.FloatTensor(data[0]), torch.LongTensor(data[1]), torch.LongTensor(data[2]), torch.LongTensor(data[3])\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                example_in = Variable(next_example).cuda()\n",
    "                label_in = Variable(next_label).cuda()\n",
    "                level0_label_in = Variable(next_level0_label).cuda()\n",
    "                level1_label_in = Variable(next_level1_label).cuda()\n",
    "            else:\n",
    "                example_in = Variable(next_example)\n",
    "                label_in = Variable(next_label)\n",
    "                level0_label_in = Variable(next_level0_label)\n",
    "                level1_label_in = Variable(next_level1_label)\n",
    "\n",
    "            label_in = torch.topk(label_in, 1)[-1].view(-1)\n",
    "\n",
    "            target_logits = cdiscount_net(example_in)\n",
    "            target_softmax = nn.Softmax()(target_logits)\n",
    "            loss = criterion(target_logits, label_in)\n",
    "            _, batch_top1 = torch.topk(target_softmax, 1)\n",
    "            batch_top1 = batch_top1.type(torch.LongTensor)\n",
    "            if torch.cuda.is_available():\n",
    "                batch_top1 = batch_top1.cuda()\n",
    "\n",
    "            num_correct = smoother.push_new_value('train_correct', (label_in == batch_top1.view(-1)).sum().data[0])\n",
    "            smoother.push_new_value('train_total', BATCH_SIZE)\n",
    "    #         target_log_softmax = cdiscount_net(msno_in, artist_in)\n",
    "    #         loss = criterion(target_log_softmax, label_in)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_acc  = num_correct/BATCH_SIZE\n",
    "            batch_loss = loss.data[0]\n",
    "\n",
    "            smooth_train_acc = smoother.smooth_value('train_correct')/smoother.smooth_value('train_total')\n",
    "\n",
    "            if global_step%1000 == 0:\n",
    "                try:\n",
    "                    cur_readed_lr = read_learning_rate(global_step, num_steps_per_train_epoch)\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = cur_readed_lr\n",
    "                except:\n",
    "                    log.info('!!!Error Raised When Read Learning Rate!!!')\n",
    "            if timer_holder.check_for_me('log_time', global_step):\n",
    "                time_passed = timer_holder.get_ticks_passed()\n",
    "                log_value('train/batch_loss', batch_loss, global_step)\n",
    "                log_value('train/sec_per_step', time_passed/log_step_interval, global_step)\n",
    "                log_value('train/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "                log_value('train/batch_acc', batch_acc, global_step)\n",
    "                log_value('train/smooth_train_acc', smooth_train_acc, global_step)\n",
    "                # to stdout\n",
    "                log.info('####### train logging #######')\n",
    "                log.info('\\ttrain/current_step: {}/{}, cur_epoch: {}'.format(global_step, num_steps_per_train_epoch, epoch))\n",
    "                log.info('\\ttrain/current_lr: {}'.format(optimizer.param_groups[0]['lr']))\n",
    "                log.info('\\ttrain/sec_per_step: {:.3f}'.format(time_passed/log_step_interval))\n",
    "                log.info('\\ttrain/batch_loss: {:.3f}'.format(batch_loss))\n",
    "                log.info('\\ttrain/batch_acc: {:.3f}, smooth_acc: {:.3f}'.format(batch_acc*100, smooth_train_acc*100))\n",
    "                if MDEBUG: break\n",
    "            if timer_holder.check_for_me('save_time'):\n",
    "                torch_saver.save_checkpoint(cdiscount_net.state_dict(), {'optimizer': optimizer.state_dict(),\n",
    "                                                        'iter': index, 'epoch': epoch, 'global_step': global_step }, global_step, is_best=False)\n",
    "\n",
    "        # reset start_iter\n",
    "        start_iter = 0\n",
    "        log.info('epoch {} finished.'.format(epoch)) \n",
    "        # save model after each epoch\n",
    "        torch_saver.save_checkpoint(cdiscount_net.state_dict(), {'optimizer': optimizer.state_dict(),\n",
    "                                                        'iter': 0, 'epoch': epoch+1, 'global_step': global_step }, global_step, is_best=False)\n",
    "        timer_holder.reset_event('save_time')\n",
    "\n",
    "\n",
    "        # check on validation every epoches            \n",
    "        cdiscount_net.eval()\n",
    "        tf_sess.run(val_initializer)\n",
    "        timer_holder.reset_event('eval_log_time', 1)\n",
    "        #if False:\n",
    "        for index in range(len(val_set)):\n",
    "            try:\n",
    "                data = tf_sess.run(val_set.get_next())\n",
    "                #next_example, next_label, next_level0_label, next_level1_label = [torch.from_numpy(array) for array in data]\n",
    "                next_example, next_label, next_level0_label, next_level1_label = torch.FloatTensor(data[0]), torch.LongTensor(data[1]), torch.LongTensor(data[2]), torch.LongTensor(data[3])\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                example_in = Variable(next_example).cuda()\n",
    "                label_in = Variable(next_label).cuda()\n",
    "                level0_label_in = Variable(next_level0_label).cuda()\n",
    "                level1_label_in = Variable(next_level1_label).cuda()\n",
    "            else:\n",
    "                example_in = Variable(next_example)\n",
    "                label_in = Variable(next_label)\n",
    "                level0_label_in = Variable(next_level0_label)\n",
    "                level1_label_in = Variable(next_level1_label)\n",
    "\n",
    "            label_in = torch.topk(label_in, 1)[-1].view(-1)\n",
    "\n",
    "            _, batch_top1 = torch.topk(nn.Softmax()(cdiscount_net(example_in)), 1)\n",
    "            batch_top1 = batch_top1.type(torch.LongTensor)\n",
    "            if torch.cuda.is_available():\n",
    "                batch_top1 = batch_top1.cuda()\n",
    "            num_correct = smoother.push_new_value('val_correct', (label_in == batch_top1.view(-1)).sum().data[0])\n",
    "            smoother.push_new_value('val_total', VAL_BATCH_SIZE)        \n",
    "\n",
    "            batch_acc  = num_correct/VAL_BATCH_SIZE\n",
    "\n",
    "            smooth_validation_acc = smoother.smooth_value('val_correct')/smoother.smooth_value('val_total')\n",
    "\n",
    "            if timer_holder.check_for_me('eval_log_time', index):\n",
    "                time_passed = timer_holder.get_ticks_passed()\n",
    "                log_value('validation/batch_acc', batch_acc, global_step+index)\n",
    "                log_value('validation/smooth_validation_acc', smooth_validation_acc, global_step+index)\n",
    "                log.info('####### validation logging #######')\n",
    "                log.info('\\tvalidation/current_step: {}/{}'.format(index, num_steps_per_val_epoch))\n",
    "                log.info('\\tvalidation/sec_per_step: {:.3f}'.format(time_passed/log_step_interval))\n",
    "                log.info('\\tvalidation/batch_acc: {:.3f}, smooth_acc: {:.3f}'.format(batch_acc*100, smooth_validation_acc*100))\n",
    "                if MDEBUG: break\n",
    "except:\n",
    "    # use tensorflow to fill the GPU no matter what reason caused\n",
    "    log.info(\"Error raised.\\r\\nTraining did't finished.\")\n",
    "    tf_sess.close()\n",
    "    get_ipython().magic('env CUDA_VISIBLE_DEVICES = {}'.format(', '.join(map(str, GPU_ID))))\n",
    "    import tensorflow as tf\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1.)\n",
    "    tf_sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n",
    "tf_sess.close()    \n",
    "log.info('Done: {}'.format(datetime.now().strftime('%Y-%m-%d_%H_%M_%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
