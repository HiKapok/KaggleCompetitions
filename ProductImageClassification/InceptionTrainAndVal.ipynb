{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# Running %env without any arguments\n",
    "# lists all environment variables\n",
    "\n",
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "%env CUDA_VISIBLE_DEVICES = 1\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "import bson                       # this is installed with the pymongo package\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imsave\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging\n",
    "import os.path\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.python.slim.nets import inception\n",
    "import inception_preprocessing\n",
    "import logging\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/'\n",
    "PRETRAINED_MODEL_PATH = DATASET_PATH + 'logs/before/inception_v3_model.ckpt-917169'\n",
    "LOG_PATH = DATASET_PATH + 'logs/'\n",
    "TRAIN_PATH = DATASET_PATH + 'Split1/Train/'\n",
    "VAL_PATH = DATASET_PATH + 'Split1/Validation/'\n",
    "TEST_PATH = DATASET_PATH + 'Test/'\n",
    "CATEGORY_NAME_PATH = DATASET_PATH + 'category_names.csv'\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_WIDTH = 180\n",
    "IMAGE_HEIGHT = 180\n",
    "NUM_CLASS = 5270\n",
    "# validation examples num: 2319624\n",
    "# train examples num: 10051704\n",
    "# total step: 157057\n",
    "TOTAL_EXAMPLES = 10051704\n",
    "# validation num = 2319624\n",
    "NUM_EPOCHES = 7\n",
    "INPUT_THREADS = 6\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "# initial_learning_rate = 0.000003#0.00001\n",
    "# learning_rate_decay_factor = 0.94\n",
    "initial_learning_rate = 0.001#0.00001\n",
    "learning_rate_decay_factor = 0.8\n",
    "num_epochs_before_decay = 1\n",
    "momentum = 0.4\n",
    "#Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "num_steps_per_epoch = TOTAL_EXAMPLES / BATCH_SIZE\n",
    "decay_steps = int(num_epochs_before_decay * num_steps_per_epoch / 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get TF logger\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(DATASET_PATH + 'tensorflow_inception_train.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "fh.setFormatter(formatter)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MiniDataSet(object):\n",
    "    def __init__(self, file_path_pattern, category_level_csv, num_examples, num_classes, is_training = True, min_after_dequeue=1000, batch_size = BATCH_SIZE, num_epochs = NUM_EPOCHES, num_reader = INPUT_THREADS):\n",
    "        super(MiniDataSet, self).__init__()\n",
    "        self._num_examples = num_examples\n",
    "        self._num_classes = num_classes\n",
    "        self._file_path_pattern = file_path_pattern\n",
    "        self._category_level_csv = category_level_csv\n",
    "        self._num_reader = num_reader\n",
    "        self._batch_size = batch_size\n",
    "        self._num_epochs = num_epochs\n",
    "        self._min_after_dequeue = min_after_dequeue\n",
    "        self._is_training = is_training\n",
    "        \n",
    "    def get_category_description_from_csv(self, level = 0):\n",
    "        category_map = dict()\n",
    "        csv = pd.read_csv(self._category_level_csv).values\n",
    "        for row in csv:  \n",
    "            category_id, levels = row[0], row[1:]\n",
    "            category_map[category_id] = levels[level]\n",
    "        return category_map\n",
    "\n",
    "    def create_dataset(self):\n",
    "        opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "        reader = lambda : tf.TFRecordReader(options=opts)\n",
    "        keys_to_features = {\n",
    "            'img_raw': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "            'product_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            # notice that we don't have this feature in our TFRecord, so always default provided\n",
    "            'format': tf.FixedLenFeature([], tf.string, default_value='jpg'),\n",
    "            'category_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64))\n",
    "        }\n",
    "\n",
    "        items_to_handlers = {\n",
    "            # automated decode image from features in FixedLenFeature\n",
    "            'image': slim.tfexample_decoder.Image(image_key='img_raw', format_key='format'),\n",
    "            'label': slim.tfexample_decoder.Tensor('category_id'),\n",
    "        }\n",
    "\n",
    "        decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "        labels_to_name_dict = self.get_category_description_from_csv()\n",
    "\n",
    "        self._dataset = slim.dataset.Dataset(\n",
    "            data_sources = self._file_path_pattern,\n",
    "            decoder = decoder,\n",
    "            reader = reader,\n",
    "            # num_readers = 8,\n",
    "            num_samples = self._num_examples,\n",
    "            #num_classes = self._num_classes,\n",
    "            #labels_to_name = labels_to_name_dict,\n",
    "            items_to_descriptions = None)\n",
    "        \n",
    "        # notice that DatasetDataProvider can automate shuffle the examples by ParallelReader using its RandomShuffleQueue\n",
    "        self._data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            self._dataset,\n",
    "            num_readers = self._num_reader,\n",
    "            shuffle = True, # default is True\n",
    "            num_epochs = self._num_epochs,\n",
    "            common_queue_capacity = self._min_after_dequeue + 3 * self._batch_size,\n",
    "            common_queue_min = self._min_after_dequeue,\n",
    "            scope = self._is_training and 'train_files' or 'validation_files')\n",
    "        \n",
    "        return self._data_provider.get(['image', 'label'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_inception(input_image, is_training = True):\n",
    "    # inception_v3.default_image_size = 299\n",
    "    return inception_preprocessing.preprocess_image(input_image, 299, 299, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cvt_csv2tfrecord():\n",
    "    count = 0\n",
    "    category_map = dict()\n",
    "    csv = pd.read_csv(CATEGORY_NAME_PATH).values\n",
    "    for row in csv:  \n",
    "        category_id, _ = row[0], row[1:]\n",
    "        category_map[category_id] = count\n",
    "        count += 1\n",
    "    return category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_process(org_label, map_table, num_classes):\n",
    "    return tf.one_hot(map_table.lookup(tf.as_string(org_label)), num_classes, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def_graph = tf.Graph()\n",
    "with def_graph.as_default() as graph:\n",
    "    def train_step(input_examples, one_hot_labels):   \n",
    "        with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "            # here logits is the pre-softmax activations\n",
    "            logits, end_points = inception.inception_v3(\n",
    "                input_examples,\n",
    "                num_classes = NUM_CLASS,\n",
    "                is_training = True)\n",
    "            # we retrain for diferrent num classes\n",
    "            # and don't define any Variables before get_variables_to_restore\n",
    "            \n",
    "#             variables_to_exclude = []\n",
    "#             #variables_to_exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\n",
    "#             for var in slim.get_model_variables():\n",
    "#                 print(var.op.name)\n",
    "#                 if var.op.name.strip().endswith('*Momentum'):\n",
    "#                     print(var.op.name)\n",
    "#                     variables_to_exclude.append(var)\n",
    "\n",
    "#             variables = tf.contrib.framework.get_model_variables()\n",
    "#             restore_variables = tf.contrib.framework.filter_variables(\n",
    "#                 variables, include_patterns=None, exclude_patterns=['Momentum', 'momentum'])\n",
    "\n",
    "            variables_to_restore = slim.get_variables_to_restore(exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits'])\n",
    "            #variables_to_restore_from_checkpoint = slim.get_variables_to_restore(exclude = variables_to_exclude)\n",
    "            # Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced, e.x. label smothing\n",
    "            loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
    "            total_loss = tf.losses.get_total_loss()    # obtain the regularization losses as well\n",
    "\n",
    "            # Create the global step for monitoring the learning_rate and training.\n",
    "            # since supervisor will also create one global_step, so we create n advance in order to feed into exponential_decay\n",
    "            global_step = tf.train.get_or_create_global_step(graph = graph)\n",
    "\n",
    "            #Define your exponentially decaying learning rate\n",
    "            lr = tf.train.exponential_decay(\n",
    "                learning_rate = initial_learning_rate,\n",
    "                global_step = global_step,\n",
    "                decay_steps = decay_steps,\n",
    "                decay_rate = learning_rate_decay_factor,\n",
    "                staircase = True)\n",
    "\n",
    "            #Now we can define the optimizer that takes on the learning rate\n",
    "            #optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate = lr, momentum=momentum)\n",
    "            \n",
    "\n",
    "            #Create the train_op.\n",
    "            train_op = slim.learning.create_train_op(total_loss, optimizer, summarize_gradients=False)\n",
    "\n",
    "            #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "            predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "            probabilities = end_points['Predictions']\n",
    "            accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, tf.argmax(one_hot_labels, 1))\n",
    "            metrics_op = tf.group(accuracy_update)\n",
    "\n",
    "\n",
    "            #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "            tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "            tf.summary.scalar('accuracy', accuracy)\n",
    "            tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "            return train_op, global_step, metrics_op, variables_to_restore, predictions, lr, accuracy, total_loss\n",
    "\n",
    "    def validation_step(input_examples, one_hot_labels):   \n",
    "        with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "            # here logits is the pre-softmax activations\n",
    "            logits, end_points = inception.inception_v3(\n",
    "                input_examples,\n",
    "                num_classes = NUM_CLASS,\n",
    "                is_training=False, reuse=True)\n",
    "\n",
    "            #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "            predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "            probabilities = end_points['Predictions']\n",
    "            accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, tf.argmax(one_hot_labels, 1))\n",
    "            metrics_op = tf.group(accuracy_update)\n",
    "\n",
    "            #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "            tf.summary.scalar('validation/accuracy', accuracy)\n",
    "\n",
    "            return metrics_op, accuracy, predictions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with def_graph.as_default() as graph:\n",
    "    def init_dataset(file_path_pattern, mapping_table, is_training = True):\n",
    "        dataset = MiniDataSet(file_path_pattern, CATEGORY_NAME_PATH, TOTAL_EXAMPLES, NUM_CLASS, is_training = is_training)\n",
    "        org_image, org_label = dataset.create_dataset()\n",
    "        image = preprocess_for_inception(org_image, is_training) # final image to train\n",
    "\n",
    "        label = one_hot_process(org_label, mapping_table, NUM_CLASS) # final label for training\n",
    "        # no need for shuffle, DatasetDataProvider do this for us\n",
    "        batch_images, batch_labels = tf.train.batch([image, label], BATCH_SIZE,\\\n",
    "                                            num_threads = INPUT_THREADS,\\\n",
    "                                            capacity = 1000 + 3 * BATCH_SIZE,\\\n",
    "                                            allow_smaller_final_batch = is_training, name = is_training and 'train_batch' or 'validation_batch')\n",
    "        \n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/training/python/training/training.py:412: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "INFO:tensorflow:Restoring parameters from /media/rs/0E06CD1706CD0127/Kapok/kaggle/logs/inception_v3_model.ckpt-1115408\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path /media/rs/0E06CD1706CD0127/Kapok/kaggle/logs/inception_v3_model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Current Speed: 0.860sec/batch\n",
      "INFO:tensorflow:Current Streaming Accuracy: 59.531%\n",
      "INFO:tensorflow:Current Loss: 2.646\n",
      "INFO:tensorflow:Epoch 8/7, Global Step: 1115423\n",
      "INFO:tensorflow:Current Learning Rate: 8.507064563900713e-08\n",
      "INFO:tensorflow:Current Speed: 0.808sec/batch\n",
      "INFO:tensorflow:Current Streaming Accuracy: 59.375%\n",
      "INFO:tensorflow:Current Loss: 2.679\n",
      "INFO:tensorflow:Epoch 8/7, Global Step: 1115433\n",
      "INFO:tensorflow:Current Learning Rate: 8.507064563900713e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3d1b999a9fb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                     \u001b[0mtime_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with def_graph.as_default() as graph:\n",
    "    mapping_strings = tf.constant( [ str(key) for key in cvt_csv2tfrecord().keys() ] )\n",
    "    mapping_table = tf.contrib.lookup.index_table_from_tensor(mapping=mapping_strings, default_value=0)\n",
    "    batch_images, batch_labels = init_dataset(TRAIN_PATH + \"output_file*.tfrecords\", mapping_table)\n",
    "    batch_val_images, batch_val_labels = init_dataset(VAL_PATH + \"test_output_file*.tfrecords\", mapping_table, False)\n",
    "    with tf.device('/gpu:0'):\n",
    "        train_op, global_step, metrics_op, variables_to_restore, pred_op, lr, accuracy, total_loss = train_step(batch_images, batch_labels)\n",
    "        val_metrics_op, val_accuracy, val_predictions, val_probabilities = validation_step(batch_val_images, batch_val_labels)\n",
    "        real_val_label = tf.argmax(batch_val_labels, 1)\n",
    "    \n",
    "     # Summarize all gradients\n",
    "#     for var in tf.trainable_variables():\n",
    "#         print(var.name[:-2])\n",
    "#         if 'InceptionV3/Conv2d_1a_3x3/weights' == var.name[:-2]:\n",
    "#             tf.summary.tensor_summary(var.name[:-2], var) \n",
    "                    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    # Create a saver that restores only the pre-trained variables.\n",
    "    # we have change optim, restore all param use pretrained mode\n",
    "    #pre_train_saver = tf.train.Saver(variables_to_restore)\n",
    "    \n",
    "    variables = slim.get_variables_to_restore()\n",
    "    restore_from_pretrained = tf.contrib.framework.filter_variables(\n",
    "        variables,\n",
    "        include_patterns=None,\n",
    "        exclude_patterns=['Momentum'])\n",
    "\n",
    "    pre_train_saver = tf.train.Saver(restore_from_pretrained)\n",
    "    # Define an init function that loads the pretrained checkpoint.\n",
    "    # sess is the managed session passed by Supervisor\n",
    "    def load_pretrain(sess):\n",
    "        pre_train_saver.restore(sess, PRETRAINED_MODEL_PATH)\n",
    "\n",
    "    # no need for specify local_variables_initializer and tables_initializer, Supervisor will do this via default local_init_op\n",
    "    # init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer())\n",
    "    init_op = tf.group(tf.global_variables_initializer())\n",
    "    # Pass the init function to the supervisor.\n",
    "    # - The init function is called _after_ the variables have been initialized by running the init_op.\n",
    "    # - use default tf.Saver() for ordinary save and restore\n",
    "    # - save checkpoint every 1.3 hours(4800)\n",
    "    # - manage summary in current process by ourselves for memory saving\n",
    "    # - no need to specify global_step, supervisor will find this automately\n",
    "    # - initialize order: checkpoint -> local_init_op -> init_op -> init_func\n",
    "    sv = tf.train.Supervisor(logdir=LOG_PATH, init_fn = load_pretrain, init_op = init_op, summary_op = None, save_model_secs=24000, checkpoint_basename='inception_v3_model.ckpt')\n",
    "    \n",
    "    final_loss = 0.\n",
    "    final_accuracy = 0.\n",
    "    training_state = True\n",
    "\n",
    "    config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    with sv.managed_session(config=config) as sess:\n",
    "    #with sv.prepare_or_wait_for_session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "\n",
    "        # Here sess was either initialized from the pre-trained-checkpoint or\n",
    "        # recovered from a checkpoint saved in a previous run of this code.\n",
    "        for step in range(int(num_steps_per_epoch * NUM_EPOCHES)):       \n",
    "            if sv.should_stop():\n",
    "                tf_logging.info('Supervisor emit finished!')\n",
    "                tf_logging.info('Current Loss: %s', loss)\n",
    "                tf_logging.info('Current Accuracy: %s', accuracy)\n",
    "                tf_logging.info('Saving current model to disk(maybe invalid).')\n",
    "                training_state = False\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "            if step % 1000 == 0:\n",
    "                with tf.device('/gpu:0'):\n",
    "                    _, _, _, summ = sess.run([train_op, global_step, metrics_op, summary_op])\n",
    "                sv.summary_computed(sess, summ)\n",
    "            else:\n",
    "                if step % 50 == 0:\n",
    "                    with tf.device('/gpu:0'):\n",
    "                        _, val_acc, val_pred, val_prob, real_label = sess.run([val_metrics_op, val_accuracy, val_predictions, val_probabilities, real_val_label])\n",
    "                    time_elapsed = time.time() - start_time\n",
    "                    tf_logging.info('Validation Speed: {:5.3f}sec/batch'.format(time_elapsed))\n",
    "                    tf_logging.info('Current Streaming ValAccuracy: {:5.3f}%'.format(val_acc*100.))\n",
    "                    tf_logging.info('Real Label: {}'.format(real_label))\n",
    "                    tf_logging.info('Pred Label: {}'.format(val_pred))\n",
    "                        \n",
    "                else:\n",
    "                    with tf.device('/gpu:0'):\n",
    "                        _, total_step, _, cur_loss, cur_acc, cur_lr = sess.run([train_op, global_step, metrics_op, total_loss, accuracy, lr])\n",
    "                    time_elapsed = time.time() - start_time\n",
    "                    if step % 10 == 0:\n",
    "                        final_loss = cur_loss\n",
    "                        final_accuracy = cur_acc\n",
    "                        tf_logging.info('Current Speed: {:5.3f}sec/batch'.format(time_elapsed))\n",
    "                        tf_logging.info('Current Streaming Accuracy: {:5.3f}%'.format(cur_acc*100.))\n",
    "                        tf_logging.info('Current Loss: {:5.3f}'.format(cur_loss))\n",
    "                        tf_logging.info('Epoch %s/%s, Global Step: %s', int(total_step / num_steps_per_epoch + 1), NUM_EPOCHES, total_step)\n",
    "                        tf_logging.info('Current Learning Rate: {}'.format(cur_lr))\n",
    "                \n",
    "                    \n",
    "        if training_state:\n",
    "            #We log the final training loss and accuracy\n",
    "            tf_logging.info('Final Loss: %s', final_loss)\n",
    "            tf_logging.info('Final Accuracy: %s', final_accuracy)\n",
    "            # Once all the training has been done, save the log files and checkpoint model\n",
    "            tf_logging.info('Finished training! Model saved.')\n",
    "        sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
