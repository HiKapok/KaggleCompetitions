{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\n"
     ]
    }
   ],
   "source": [
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "%env CUDA_VISIBLE_DEVICES = \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import bson                       # this is installed with the pymongo package\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "import multiprocessing as mp      # will come in handy due to the size of the data\n",
    "import os.path\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from itertools import compress\n",
    "from datetime import datetime\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/'\n",
    "TRAIN_PATH = DATASET_PATH + 'Split/Train/'\n",
    "VAL_PATH = DATASET_PATH + 'Split/Validation/'\n",
    "TEST_PATH = DATASET_PATH + 'Test/'\n",
    "if os.path.exists(TRAIN_PATH) is not True: os.makedirs(TRAIN_PATH)\n",
    "if os.path.exists(VAL_PATH) is not True: os.makedirs(VAL_PATH)\n",
    "BATCH_SIZE = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_examples(files):\n",
    "    filename_queue = tf.train.string_input_producer(files, num_epochs=1, shuffle=True) \n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "    reader = tf.TFRecordReader(options = opts)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    input_features = tf.parse_single_example(\n",
    "          serialized_example,\n",
    "          features={\n",
    "                #'height': tf.FixedLenFeature([], tf.int64),\n",
    "                #'width': tf.FixedLenFeature([], tf.int64),\n",
    "                'category_id': tf.FixedLenFeature([], tf.int64),\n",
    "                'product_id': tf.FixedLenFeature([], tf.int64),\n",
    "                'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "          })\n",
    "    # only part of the dictionary are needed\n",
    "    return { 'img_raw' : input_features['img_raw'], 'product_id' : input_features['product_id'], 'category_id' : input_features['category_id'] }\n",
    "    #return input_features['img_raw'], input_features['product_id'], input_features['category_id']\n",
    "    #return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_pipeline(filenames, batch_size, read_threads = 8):\n",
    "    filename_queue = create_examples(filenames)\n",
    "    example_list = [ filename_queue for _ in range(read_threads)]\n",
    "    #print(example_list)\n",
    "    min_after_dequeue = 2000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    return tf.train.shuffle_batch_join(\n",
    "        example_list, batch_size = batch_size, capacity = capacity,\n",
    "        min_after_dequeue = min_after_dequeue, allow_smaller_final_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_num_of_examples(file_path, file_prefix):\n",
    "    # Count the total number of examples in all of these shard \n",
    "    num_samples = 0\n",
    "    tfrecords_to_count = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.startswith(file_prefix)]\n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file, options = opts):\n",
    "            num_samples += 1\n",
    "    return num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_num_of_examples2(file_path, file_prefix):\n",
    "    # Count the total number of examples in all of these shard \n",
    "    num_samples = 0\n",
    "    tfrecords_to_count = [os.path.join(file_path, file) for file in os.listdir(file_path) if file_prefix in file]\n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file, options = opts):\n",
    "            num_samples += 1\n",
    "    return num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def histogram_of_category(file_path, file_prefix, sess):\n",
    "    # Count the total number of examples in all of these shard \n",
    "    hist = dict()\n",
    "    temp_list= list()\n",
    "    tfrecords_to_count = [os.path.join(file_path, file) for file in os.listdir(file_path) if file.startswith(file_prefix)]\n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file, options = opts):\n",
    "            input_features = tf.parse_single_example(\n",
    "              record,\n",
    "              features={\n",
    "                    'category_id': tf.FixedLenFeature([], tf.int64),\n",
    "                    'product_id': tf.FixedLenFeature([], tf.int64),\n",
    "                    'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "              })\n",
    "            temp_list.append(input_features['category_id'].eval(session=sess))\n",
    "\n",
    "    for elements in temp_list:\n",
    "        hist[elements]=temp_list.count(elements)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_train_val(filenames, outpath_train, outpath_test, batch_size, val_per, out_file_num = 500):\n",
    "    tfrecords_filename = [outpath_train + 'output_file{:d}.tfrecords'.format(index + 1) for index in range(out_file_num)]\n",
    "    test_out_file_num = int(out_file_num*val_per)\n",
    "    tfrecords_test_filename = [outpath_test + 'test_output_file{:d}.tfrecords'.format(index + 1) for index in range(test_out_file_num)]\n",
    "    # create a partition vector\n",
    "    partitions = [0] * batch_size\n",
    "    test_set_size = int(batch_size * val_per)\n",
    "    partitions[:test_set_size] = [1] * test_set_size \n",
    "    #print(partitions)\n",
    "    #total_fold = int(8)\n",
    "\n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "\n",
    "    try:\n",
    "        writer_list = [tf.python_io.TFRecordWriter(file_name, options = opts) for file_name in tfrecords_filename]\n",
    "    except Exception as e:\n",
    "        print('writer_list create failed!')\n",
    "        if not writer_list:\n",
    "            for f in writer_list:\n",
    "                f.close()\n",
    "        return\n",
    "    try:\n",
    "        test_writer_list = [tf.python_io.TFRecordWriter(file_name, options = opts) for file_name in tfrecords_test_filename]\n",
    "    except Exception as e:\n",
    "        print('test_writer_list create failed!')\n",
    "        if not test_writer_list:\n",
    "            for f in test_writer_list:\n",
    "                f.close()\n",
    "        return\n",
    "    \n",
    "    files = tf.train.match_filenames_once(filenames)\n",
    "    all_examples = input_pipeline(files, batch_size)\n",
    "    \n",
    "    partitions_tensor = tf.constant(partitions)\n",
    "    shuff = tf.random_shuffle(partitions_tensor)\n",
    "    train_examples = dict()\n",
    "    test_examples = dict()\n",
    "    # split train and test examples from the mask\n",
    "    for key, value in all_examples.items():\n",
    "        #temp_list = tf.split(value, total_fold)\n",
    "        #train_examples[key] = tf.concat(temp_list[0:-1], 0)\n",
    "        #test_examples[key] = temp_list[-1]\n",
    "        train_examples[key], test_examples[key] = tf.dynamic_partition(value, shuff, 2)#tf.cond(value.get_shape().as_list()[0] < batch_size, \\\n",
    "                                           # lambda : tf.dynamic_partition(value, shuff, 2), lambda : tf.split(value, 2))\n",
    "         \n",
    "    # Create the graph, etc.\n",
    "    # initialize local variables, like local counter epochs\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    # Create a session for running operations in the Graph.\n",
    "    sess = tf.Session()\n",
    "    # Initialize the variables (like the epoch counter).\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Start input enqueue threads.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    # batch iteration count, use for select different output file\n",
    "    count = 0\n",
    "\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            cur_train_writer = writer_list[count % out_file_num]\n",
    "            cur_test_writer = test_writer_list[count % test_out_file_num]\n",
    "           \n",
    "            try:\n",
    "                # Run training steps or whatever\n",
    "                feeded_train_list, feeded_test_list = sess.run([[tensors for tensors in train_examples.values()], [tensors for tensors in test_examples.values()]])\n",
    "                train_dictionary = dict(zip([key for key in train_examples.keys()], feeded_train_list))\n",
    "                # each run call runs one \"step\" of TensorFlow computation\n",
    "                # feeded_test_list = sess.run([tensors for tensors in test_examples.values()])\n",
    "                test_dictionary = dict(zip([key for key in test_examples.keys()], feeded_test_list))\n",
    "            # error dealed for less than a batch num data\n",
    "            # feed remaining into train\n",
    "            except tf.errors.InvalidArgumentError:\n",
    "                feeded_train_list = sess.run([tensors for tensors in all_examples.values()])\n",
    "                train_dictionary = dict(zip([key for key in all_examples.keys()], feeded_train_list))\n",
    "                test_dictionary = {}\n",
    "            finally:\n",
    "                # write here  \n",
    "                reshaped_test = [ dict(zip([key for key in test_dictionary.keys()], [test_dictionary[key][index] for key in test_dictionary.keys()])) for index in range(len(test_dictionary['img_raw'])) ]\n",
    "                reshaped_train = [ dict(zip([key for key in train_dictionary.keys()], [train_dictionary[key][index] for key in train_dictionary.keys()])) for index in range(len(train_dictionary['img_raw'])) ]\n",
    "                for item in reshaped_train:\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'img_raw': _bytes_feature(item['img_raw']),\n",
    "                        'product_id': _int64_feature(item['product_id']),\n",
    "                        'category_id': _int64_feature(item['category_id'])\n",
    "                    }))\n",
    "                    cur_train_writer.write(example.SerializeToString())\n",
    "                for item in reshaped_test:\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'img_raw': _bytes_feature(item['img_raw']),\n",
    "                        'product_id': _int64_feature(item['product_id']),\n",
    "                        'category_id': _int64_feature(item['category_id'])\n",
    "                    }))\n",
    "                    cur_test_writer.write(example.SerializeToString())\n",
    "                #[dict(zip([key for key in test_dictionary.keys()], feeded_train_list)) for key in test_dictionary.keys() for _ in range(len(test_dictionary[key])) ]\n",
    "                #print(test_dictionary['product_id'])\n",
    "                #print(test_dictionary['category_id'])\n",
    "#                 print(len(test_dictionary['img_raw']))\n",
    "#                 print(len(train_dictionary['img_raw']))\n",
    "#                 print(shuff.eval(session=sess))\n",
    "                count += 1\n",
    "                if count > 1000:\n",
    "                    break\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done splitting -- epoch limit reached')\n",
    "        print('last count: {}, roughly examples num: {}'.format(count, count * batch_size))\n",
    "        print('finished time: {}'.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    finally:\n",
    "        for f in writer_list:\n",
    "            f.close()\n",
    "        for f in test_writer_list:\n",
    "            f.close()\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "    \n",
    "    # Wait for threads to finish.\n",
    "    coord.join(threads) \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_train_val2(filenames, outpath_train, batch_size, out_file_num = 500):\n",
    "    tfrecords_filename = [outpath_train + 'output_file{:d}.tfrecords'.format(index + 1) for index in range(int(out_file_num/2))]\n",
    "    \n",
    "    #tfrecords_test_filename = [outpath_test + 'test_output_file{:d}.tfrecords'.format(index + 1) for index in range(test_out_file_num)]\n",
    "    tfrecords_test_filename = [outpath_train + 'output_file{:d}.tfrecords'.format(index + 1) for index in range(int(out_file_num/2), out_file_num)]\n",
    "    test_out_file_num = len(tfrecords_test_filename)\n",
    "    out_file_num = len(tfrecords_filename)\n",
    "    # create a partition vector\n",
    "    partitions = [0] * batch_size\n",
    "    test_set_size = int(batch_size * 0.5)\n",
    "    partitions[:test_set_size] = [1] * test_set_size \n",
    "    #print(partitions)\n",
    "    #total_fold = int(8)\n",
    "\n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "\n",
    "    try:\n",
    "        writer_list = [tf.python_io.TFRecordWriter(file_name, options = opts) for file_name in tfrecords_filename]\n",
    "    except Exception as e:\n",
    "        print('writer_list create failed!')\n",
    "        if not writer_list:\n",
    "            for f in writer_list:\n",
    "                f.close()\n",
    "        return\n",
    "    try:\n",
    "        test_writer_list = [tf.python_io.TFRecordWriter(file_name, options = opts) for file_name in tfrecords_test_filename]\n",
    "    except Exception as e:\n",
    "        print('test_writer_list create failed!')\n",
    "        if not test_writer_list:\n",
    "            for f in test_writer_list:\n",
    "                f.close()\n",
    "        return\n",
    "    \n",
    "    files = tf.train.match_filenames_once(filenames)\n",
    "    all_examples = input_pipeline(files, batch_size)\n",
    "    \n",
    "    partitions_tensor = tf.constant(partitions)\n",
    "    shuff = tf.random_shuffle(partitions_tensor)\n",
    "    train_examples = dict()\n",
    "    test_examples = dict()\n",
    "    # split train and test examples from the mask\n",
    "    for key, value in all_examples.items():\n",
    "        #temp_list = tf.split(value, total_fold)\n",
    "        #train_examples[key] = tf.concat(temp_list[0:-1], 0)\n",
    "        #test_examples[key] = temp_list[-1]\n",
    "        train_examples[key], test_examples[key] = tf.dynamic_partition(value, shuff, 2)#tf.cond(value.get_shape().as_list()[0] < batch_size, \\\n",
    "                                           # lambda : tf.dynamic_partition(value, shuff, 2), lambda : tf.split(value, 2))\n",
    "         \n",
    "    # Create the graph, etc.\n",
    "    # initialize local variables, like local counter epochs\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    # Create a session for running operations in the Graph.\n",
    "    sess = tf.Session()\n",
    "    # Initialize the variables (like the epoch counter).\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Start input enqueue threads.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    # batch iteration count, use for select different output file\n",
    "    count = 0\n",
    "\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            cur_train_writer = writer_list[count % out_file_num]\n",
    "            cur_test_writer = test_writer_list[count % test_out_file_num]\n",
    "           \n",
    "            try:\n",
    "                # Run training steps or whatever\n",
    "                feeded_train_list, feeded_test_list = sess.run([[tensors for tensors in train_examples.values()], [tensors for tensors in test_examples.values()]])\n",
    "                train_dictionary = dict(zip([key for key in train_examples.keys()], feeded_train_list))\n",
    "                # each run call runs one \"step\" of TensorFlow computation\n",
    "                # feeded_test_list = sess.run([tensors for tensors in test_examples.values()])\n",
    "                test_dictionary = dict(zip([key for key in test_examples.keys()], feeded_test_list))\n",
    "            # error dealed for less than a batch num data\n",
    "            # feed remaining into train\n",
    "            except tf.errors.InvalidArgumentError:\n",
    "                feeded_train_list = sess.run([tensors for tensors in all_examples.values()])\n",
    "                train_dictionary = dict(zip([key for key in all_examples.keys()], feeded_train_list))\n",
    "                test_dictionary = {}\n",
    "            finally:\n",
    "                # write here  \n",
    "                reshaped_test = [ dict(zip([key for key in test_dictionary.keys()], [test_dictionary[key][index] for key in test_dictionary.keys()])) for index in range(len(test_dictionary['img_raw'])) ]\n",
    "                reshaped_train = [ dict(zip([key for key in train_dictionary.keys()], [train_dictionary[key][index] for key in train_dictionary.keys()])) for index in range(len(train_dictionary['img_raw'])) ]\n",
    "                for item in reshaped_train:\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'img_raw': _bytes_feature(item['img_raw']),\n",
    "                        'product_id': _int64_feature(item['product_id']),\n",
    "                        'category_id': _int64_feature(item['category_id'])\n",
    "                    }))\n",
    "                    cur_train_writer.write(example.SerializeToString())\n",
    "                for item in reshaped_test:\n",
    "                    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'img_raw': _bytes_feature(item['img_raw']),\n",
    "                        'product_id': _int64_feature(item['product_id']),\n",
    "                        'category_id': _int64_feature(item['category_id'])\n",
    "                    }))\n",
    "                    cur_test_writer.write(example.SerializeToString())\n",
    "                #[dict(zip([key for key in test_dictionary.keys()], feeded_train_list)) for key in test_dictionary.keys() for _ in range(len(test_dictionary[key])) ]\n",
    "                #print(test_dictionary['product_id'])\n",
    "                #print(test_dictionary['category_id'])\n",
    "#                 print(len(test_dictionary['img_raw']))\n",
    "#                 print(len(train_dictionary['img_raw']))\n",
    "#                 print(shuff.eval(session=sess))\n",
    "                count += 1\n",
    "           \n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done splitting -- epoch limit reached')\n",
    "        print('last count: {}, roughly examples num: {}'.format(count, count * batch_size))\n",
    "        print('finished time: {}'.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    finally:\n",
    "        for f in writer_list:\n",
    "            f.close()\n",
    "        for f in test_writer_list:\n",
    "            f.close()\n",
    "        # When done, ask the threads to stop.\n",
    "        coord.request_stop()\n",
    "    \n",
    "    # Wait for threads to finish.\n",
    "    coord.join(threads) \n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split dataset into train and validation\n",
    "split_into_train_val(DATASET_PATH + \"Train/output_file*.tfrecords\", TRAIN_PATH, VAL_PATH, BATCH_SIZE, 0.2)\n",
    "#split_into_train_val2(DATASET_PATH + 'Split/org/Validation/' + \"test_output_file*.tfrecords\", VAL_PATH, BATCH_SIZE, out_file_num=500)\n",
    "#split_into_train_val2(DATASET_PATH + 'Split/org/Train/' + \"output_file*.tfrecords\", TRAIN_PATH, BATCH_SIZE, out_file_num=900)\n",
    "print('validation examples num: {}'.format(count_num_of_examples(VAL_PATH, 'test_output_file')))\n",
    "print('train examples num: {}'.format(count_num_of_examples(TRAIN_PATH, 'output_file')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train examples num: 69588\n"
     ]
    }
   ],
   "source": [
    "# calculate num of examples\n",
    "# print('validation examples num: {}'.format(count_num_of_examples(VAL_PATH, 'test_output_file')))\n",
    "print('train examples num: {}'.format(count_num_of_examples2(VAL_PATH, 'output_file')))\n",
    "#print('test examples num: {}'.format(count_num_of_examples(TEST_PATH, 'output_file',sess)))\n",
    "#print('total sampled examples num: {}'.format(count_num_of_examples('/media/rs/FC6CDC6F6CDC25E4/resample_dataset2/', 'output_file')))\n",
    "#print('total sampled examples num: {}'.format(count_num_of_examples('/media/rs/FC6CDC6F6CDC25E4/ResnetHardTrain/', 'output_file')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate category histogram\n",
    "init_op = tf.global_variables_initializer()\n",
    "# Create a session for running operations in the Graph.\n",
    "sess = tf.Session()\n",
    "# Initialize the variables (like the epoch counter).\n",
    "sess.run(init_op)\n",
    "train_hist = histogram_of_category(TRAIN_PATH, 'output_file', sess)\n",
    "val_hist = histogram_of_category(VAL_PATH, 'test_output_file', sess)\n",
    "print('validation category histogram: {}'.format(val_hist))\n",
    "print('train category histogram: {}'.format(train_hist))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init = tf.global_variables_initializer()\n",
    "# t1 = tf.constant([[2],[3], [4],[4]], dtype='int64')\n",
    "# t2 = tf.constant('fegegerg')\n",
    "# # Start training\n",
    "# with tf.Session() as sess:\n",
    "#     # Run the initializer\n",
    "#     sess.run(init)\n",
    "#     # t1.eval().tostring()\n",
    "#     t8 = tf.decode_raw(t2, tf.int64)\n",
    "\n",
    "#     t9 = t8.eval().tostring()\n",
    "#     print(t9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
