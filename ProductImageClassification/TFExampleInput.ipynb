{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0,1\n"
     ]
    }
   ],
   "source": [
    "# Running %env without any arguments\n",
    "# lists all environment variables\n",
    "\n",
    "# The line below sets the environment\n",
    "# variable CUDA_VISIBLE_DEVICES\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import time\n",
    "import bson                       # this is installed with the pymongo package\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread, imsave\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import tf_logging\n",
    "import os.path\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.python.slim.nets import inception\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "import inception_preprocessing\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = '/media/rs/0E06CD1706CD0127/Kapok/kaggle/'\n",
    "PRETRAINED_MODEL_PATH = DATASET_PATH + 'models/inception_v3.ckpt'\n",
    "LOG_PATH = DATASET_PATH + 'logs222/'\n",
    "TRAIN_PATH = DATASET_PATH + 'Split/Train/'\n",
    "VAL_PATH = DATASET_PATH + 'Split/Validation/'\n",
    "TEST_PATH = DATASET_PATH + 'Test/'\n",
    "CATEGORY_NAME_PATH = DATASET_PATH + 'category_names.csv'\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_WIDTH = 180\n",
    "IMAGE_HEIGHT = 180\n",
    "NUM_CLASS = 5270\n",
    "# validation examples num: 2319618\n",
    "# train examples num: 10051678\n",
    "# total step: 157057\n",
    "TOTAL_EXAMPLES = 10051678\n",
    "NUM_EPOCHES = 4\n",
    "INPUT_THREADS = 6\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "initial_learning_rate = 0.000009#0.00001\n",
    "learning_rate_decay_factor = 0.96\n",
    "num_epochs_before_decay = 1\n",
    "#Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "num_steps_per_epoch = TOTAL_EXAMPLES / BATCH_SIZE\n",
    "decay_steps = int(num_epochs_before_decay * num_steps_per_epoch / 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deprecated\n",
    "def read_my_file_format(filename_queue):\n",
    "    # image preprocess, i.e. crop etc.\n",
    "    example_image, product_id, category_id = preprocess_for_inception(tf.image.decode_jpeg(filename_queue['img_raw'])), filename_queue['product_id'], filename_queue['category_id']\n",
    "    return example_image, product_id, category_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deprecated\n",
    "def create_examples(filename_regex, num_epochs):\n",
    "    files = tf.train.match_filenames_once(filename_regex)\n",
    "    filename_queue = tf.train.string_input_producer(files, num_epochs=num_epochs, shuffle=True) \n",
    "    opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "    reader = tf.TFRecordReader(options=opts)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    input_features = tf.parse_single_example(\n",
    "          serialized_example,\n",
    "          features={\n",
    "              'img_raw': tf.FixedLenFeature([], tf.string),\n",
    "              'product_id': tf.FixedLenFeature([], tf.int64),\n",
    "              'category_id': tf.FixedLenFeature([], tf.int64)\n",
    "          })\n",
    "    return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deprecated\n",
    "def input_pipeline(filenames, batch_size, read_threads, num_epochs=None):\n",
    "    filename_queue = create_examples(filenames, num_epochs)\n",
    "    example_list = [read_my_file_format(filename_queue)\n",
    "                  for _ in range(read_threads)]\n",
    "    min_after_dequeue = 2000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, product_batch, category_batch = tf.train.shuffle_batch_join(\n",
    "      example_list, batch_size=batch_size, capacity=capacity,\n",
    "      min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, product_batch, category_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MiniDataSet(object):\n",
    "    def __init__(self, file_path_pattern, category_level_csv, num_examples, num_classes, min_after_dequeue=1000, batch_size = BATCH_SIZE, num_epochs = NUM_EPOCHES, num_reader = INPUT_THREADS):\n",
    "        super(MiniDataSet, self).__init__()\n",
    "        self._num_examples = num_examples\n",
    "        self._num_classes = num_classes\n",
    "        self._file_path_pattern = file_path_pattern\n",
    "        self._category_level_csv = category_level_csv\n",
    "        self._num_reader = num_reader\n",
    "        self._batch_size = batch_size\n",
    "        self._num_epochs = num_epochs\n",
    "        self._min_after_dequeue = min_after_dequeue\n",
    "        \n",
    "    def get_category_description_from_csv(self, level = 0):\n",
    "        category_map = dict()\n",
    "        csv = pd.read_csv(self._category_level_csv).values\n",
    "        for row in csv:  \n",
    "            category_id, levels = row[0], row[1:]\n",
    "            category_map[category_id] = levels[level]\n",
    "        return category_map\n",
    "\n",
    "    def create_dataset(self):\n",
    "        opts = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)\n",
    "        reader = lambda : tf.TFRecordReader(options=opts)\n",
    "        keys_to_features = {\n",
    "            'img_raw': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "            'product_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "            # notice that we don't have this feature in our TFRecord, so always default provided\n",
    "            'format': tf.FixedLenFeature([], tf.string, default_value='jpg'),\n",
    "            'category_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64))\n",
    "        }\n",
    "\n",
    "        items_to_handlers = {\n",
    "            # automated decode image from features in FixedLenFeature\n",
    "            'image': slim.tfexample_decoder.Image(image_key='img_raw', format_key='format'),\n",
    "            'label': slim.tfexample_decoder.Tensor('category_id'),\n",
    "        }\n",
    "\n",
    "        decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "        labels_to_name_dict = self.get_category_description_from_csv()\n",
    "\n",
    "        self._dataset = slim.dataset.Dataset(\n",
    "            data_sources = self._file_path_pattern,\n",
    "            decoder = decoder,\n",
    "            reader = reader,\n",
    "            # num_readers = 8,\n",
    "            num_samples = self._num_examples,\n",
    "            #num_classes = self._num_classes,\n",
    "            #labels_to_name = labels_to_name_dict,\n",
    "            items_to_descriptions = None)\n",
    "        \n",
    "        # notice that DatasetDataProvider can automate shuffle the examples by ParallelReader using its RandomShuffleQueue\n",
    "        self._data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "            self._dataset,\n",
    "            num_readers = self._num_reader,\n",
    "            shuffle = True, # default is True\n",
    "            num_epochs = self._num_epochs,\n",
    "            common_queue_capacity = self._min_after_dequeue + 3 * self._batch_size,\n",
    "            common_queue_min = self._min_after_dequeue)\n",
    "        \n",
    "        return self._data_provider.get(['image', 'label'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_for_inception(input_image, is_training = True):\n",
    "    # inception_v3.default_image_size = 299\n",
    "    return inception_preprocessing.preprocess_image(input_image, 299, 299, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cvt_csv2tfrecord():\n",
    "    count = 0\n",
    "    category_map = dict()\n",
    "    csv = pd.read_csv(CATEGORY_NAME_PATH).values\n",
    "    for row in csv:  \n",
    "        category_id, _ = row[0], row[1:]\n",
    "        category_map[category_id] = count\n",
    "        count += 1\n",
    "    return category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_process(org_label, map_table, num_classes):\n",
    "    return tf.one_hot(map_table.lookup(tf.as_string(org_label)), num_classes, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /media/rs/0E06CD1706CD0127/Kapok/kaggle/logs222/inception_v3_model.ckpt-0\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path /media/rs/0E06CD1706CD0127/Kapok/kaggle/logs222/inception_v3_model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.ResourceExhaustedError'>, OOM when allocating tensor with shape[128,17,17,192]\n",
      "\t [[Node: InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV3/InceptionV3/Mixed_6d/concat, InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/read)]]\n",
      "\t [[Node: total_loss/_7079 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_29249_total_loss\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n",
      "Caused by op 'InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution', defined at:\n",
      "  File \"/home/rs/.pyenv/versions/3.5.2/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/rs/.pyenv/versions/3.5.2/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-10-bcacf7fc7ba8>\", line 72, in <module>\n",
      "    train_op, global_step, metrics_op, variables_to_restore, pred_op, summary_op, lr, accuracy, total_loss = train_step(batch_images, batch_labels)\n",
      "  File \"<ipython-input-10-bcacf7fc7ba8>\", line 9, in train_step\n",
      "    is_training = True)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/nets/inception_v3.py\", line 576, in inception_v3\n",
      "    depth_multiplier=depth_multiplier)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/nets/inception_v3.py\", line 372, in inception_v3_base\n",
      "    net, depth(192), [1, 1], scope='Conv2d_0a_1x1')\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n",
      "    return func(*args, **current_args)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1027, in convolution\n",
      "    outputs = layer.apply(inputs)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n",
      "    return self.__call__(inputs, *args, **kwargs)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 450, in __call__\n",
      "    outputs = self.call(inputs, *args, **kwargs)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 158, in call\n",
      "    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 672, in convolution\n",
      "    op=op)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n",
      "    return op(input, num_spatial_dims, padding)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 664, in op\n",
      "    name=name)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 131, in _non_atrous_convolution\n",
      "    name=name)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n",
      "    data_format=data_format, name=name)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n",
      "    op_def=op_def)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n",
      "    original_op=self._default_original_op, op_def=op_def)\n",
      "  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,17,17,192]\n",
      "\t [[Node: InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV3/InceptionV3/Mixed_6d/concat, InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/read)]]\n",
      "\t [[Node: total_loss/_7079 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_29249_total_loss\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,17,17,192]\n\t [[Node: InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV3/InceptionV3/Mixed_6d/concat, InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/read)]]\n\t [[Node: total_loss/_7079 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_29249_total_loss\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution', defined at:\n  File \"/home/rs/.pyenv/versions/3.5.2/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/rs/.pyenv/versions/3.5.2/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-bcacf7fc7ba8>\", line 72, in <module>\n    train_op, global_step, metrics_op, variables_to_restore, pred_op, summary_op, lr, accuracy, total_loss = train_step(batch_images, batch_labels)\n  File \"<ipython-input-10-bcacf7fc7ba8>\", line 9, in train_step\n    is_training = True)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/nets/inception_v3.py\", line 576, in inception_v3\n    depth_multiplier=depth_multiplier)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/nets/inception_v3.py\", line 372, in inception_v3_base\n    net, depth(192), [1, 1], scope='Conv2d_0a_1x1')\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1027, in convolution\n    outputs = layer.apply(inputs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 450, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 158, in call\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 672, in convolution\n    op=op)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 664, in op\n    name=name)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 131, in _non_atrous_convolution\n    name=name)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,17,17,192]\n\t [[Node: InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV3/InceptionV3/Mixed_6d/concat, InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/read)]]\n\t [[Node: total_loss/_7079 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_29249_total_loss\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.2/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,17,17,192]\n\t [[Node: InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV3/InceptionV3/Mixed_6d/concat, InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/read)]]\n\t [[Node: total_loss/_7079 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_29249_total_loss\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bcacf7fc7ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;31m#                                                                               feed_dict={\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m#                                                                                             X: batch_images,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,17,17,192]\n\t [[Node: InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV3/InceptionV3/Mixed_6d/concat, InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/read)]]\n\t [[Node: total_loss/_7079 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_29249_total_loss\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op 'InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution', defined at:\n  File \"/home/rs/.pyenv/versions/3.5.2/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/rs/.pyenv/versions/3.5.2/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-bcacf7fc7ba8>\", line 72, in <module>\n    train_op, global_step, metrics_op, variables_to_restore, pred_op, summary_op, lr, accuracy, total_loss = train_step(batch_images, batch_labels)\n  File \"<ipython-input-10-bcacf7fc7ba8>\", line 9, in train_step\n    is_training = True)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/nets/inception_v3.py\", line 576, in inception_v3\n    depth_multiplier=depth_multiplier)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/nets/inception_v3.py\", line 372, in inception_v3_base\n    net, depth(192), [1, 1], scope='Conv2d_0a_1x1')\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1027, in convolution\n    outputs = layer.apply(inputs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/base.py\", line 450, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\", line 158, in call\n    data_format=utils.convert_data_format(self.data_format, self.rank + 2))\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 672, in convolution\n    op=op)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 338, in with_space_to_batch\n    return op(input, num_spatial_dims, padding)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 664, in op\n    name=name)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\", line 131, in _non_atrous_convolution\n    name=name)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 397, in conv2d\n    data_format=data_format, name=name)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/kapok/pyenv35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[128,17,17,192]\n\t [[Node: InceptionV3/InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](InceptionV3/InceptionV3/Mixed_6d/concat, InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/read)]]\n\t [[Node: total_loss/_7079 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_29249_total_loss\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    # define main train operation\n",
    "    def train_step(input_examples, one_hot_labels):   \n",
    "        with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "            # here logits is the pre-softmax activations\n",
    "            logits, end_points = inception.inception_v3(\n",
    "                input_examples,\n",
    "                num_classes = NUM_CLASS,\n",
    "                is_training = True)\n",
    "        # we retrain for diferrent num classes\n",
    "        # and don't define any Variables before get_variables_to_restore\n",
    "        variables_to_restore = slim.get_variables_to_restore(exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits'])\n",
    "#         loss = slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "#         total_loss = slim.losses.get_total_loss()\n",
    "        # Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced, e.x. label smothing\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
    "        total_loss = tf.losses.get_total_loss()    # obtain the regularization losses as well\n",
    "\n",
    "        # Create the global step for monitoring the learning_rate and training.\n",
    "        # since supervisor will also create one global_step, so we create n advance in order to feed into exponential_decay\n",
    "        global_step = get_or_create_global_step(graph = graph)\n",
    "\n",
    "        #Define your exponentially decaying learning rate\n",
    "        lr = tf.train.exponential_decay(\n",
    "            learning_rate = initial_learning_rate,\n",
    "            global_step = global_step,\n",
    "            decay_steps = decay_steps,\n",
    "            decay_rate = learning_rate_decay_factor,\n",
    "            staircase = True)\n",
    "\n",
    "        #Now we can define the optimizer that takes on the learning rate\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "\n",
    "        #Create the train_op.\n",
    "        train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "        #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "        predictions = tf.argmax(end_points['Predictions'], 1)\n",
    "        probabilities = end_points['Predictions']\n",
    "        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, tf.argmax(one_hot_labels, 1))\n",
    "        metrics_op = tf.group(accuracy_update)\n",
    "\n",
    "\n",
    "        #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "        tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "        tf.summary.scalar('learning_rate', lr)\n",
    "        my_summary_op = tf.summary.merge_all()\n",
    "\n",
    "        return train_op, global_step, metrics_op, variables_to_restore, predictions, my_summary_op, lr, accuracy, total_loss\n",
    "\n",
    "    mapping_strings = tf.constant( [ str(key) for key in cvt_csv2tfrecord().keys() ] )\n",
    "    mapping_table = tf.contrib.lookup.index_table_from_tensor(mapping=mapping_strings, default_value=0)\n",
    "    \n",
    "    # acctually we don't need a placeholder anymore\n",
    "    #X = tf.placeholder(tf.float32, [BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT, 3], name='input_image')\n",
    "    #Y = tf.placeholder(tf.int64, [BATCH_SIZE,], name='input_label')\n",
    "    #ids_Y = mapping_table.lookup(tf.as_string(Y))\n",
    "    # Perform one-hot-encoding of the labels\n",
    "    #one_hot_labels = tf.one_hot(ids_Y, NUM_CLASS, axis=-1)\n",
    "\n",
    "    dataset = MiniDataSet(TRAIN_PATH + \"output_file*.tfrecords\", CATEGORY_NAME_PATH, TOTAL_EXAMPLES, NUM_CLASS)\n",
    "    org_image, org_label = dataset.create_dataset()\n",
    "    image = preprocess_for_inception(org_image) # final image to train\n",
    "    \n",
    "    label = one_hot_process(org_label, mapping_table, NUM_CLASS) # final label for training\n",
    "    # no need for shuffle, DatasetDataProvider do this for us\n",
    "    batch_images, batch_labels = tf.train.batch([image, label], BATCH_SIZE,\\\n",
    "                                        num_threads = INPUT_THREADS,\\\n",
    "                                        capacity = 1000 + 3 * BATCH_SIZE,\\\n",
    "                                        allow_smaller_final_batch = True)\n",
    "    \n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        train_op, global_step, metrics_op, variables_to_restore, pred_op, summary_op, lr, accuracy, total_loss = train_step(batch_images, batch_labels)\n",
    "\n",
    "    # Create a saver that restores only the pre-trained variables.\n",
    "    pre_train_saver = tf.train.Saver(variables_to_restore)\n",
    "    # Define an init function that loads the pretrained checkpoint.\n",
    "    # sess is the managed session passed by Supervisor\n",
    "    def load_pretrain(sess):\n",
    "        pre_train_saver.restore(sess, PRETRAINED_MODEL_PATH)\n",
    "\n",
    "    # no need for specify local_variables_initializer and tables_initializer, Supervisor will do this via default local_init_op\n",
    "    # init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer())\n",
    "    init_op = tf.group(tf.global_variables_initializer())\n",
    "    # Pass the init function to the supervisor.\n",
    "    # - The init function is called _after_ the variables have been initialized by running the init_op.\n",
    "    # - use default tf.Saver() for ordinary save and restore\n",
    "    # - save checkpoint every 1.5 hours\n",
    "    # - manage summary in current process by ourselves for memory saving\n",
    "    # - no need to specify global_step, supervisor will find this automately\n",
    "    # - initialize order: checkpoint -> local_init_op -> init_op -> init_func\n",
    "    sv = tf.train.Supervisor(logdir=LOG_PATH, init_fn = load_pretrain, init_op = init_op, summary_op = None, save_model_secs=5400, checkpoint_basename='inception_v3_model.ckpt')\n",
    "    \n",
    "    final_loss = 0.\n",
    "    final_accuracy = 0.\n",
    "    training_state = True\n",
    "    with sv.managed_session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "    #with sv.prepare_or_wait_for_session(config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)) as sess:\n",
    "\n",
    "        # Here sess was either initialized from the pre-trained-checkpoint or\n",
    "        # recovered from a checkpoint saved in a previous run of this code.\n",
    "        for step in range(int(num_steps_per_epoch * NUM_EPOCHES)):       \n",
    "            if sv.should_stop():\n",
    "                tf_logging.info('Supervisor emit finished!')\n",
    "                tf_logging.info('Current Loss: %s', loss)\n",
    "                tf_logging.info('Current Accuracy: %s', accuracy)\n",
    "                tf_logging.info('Saving current model to disk(maybe invalid).')\n",
    "                training_state = False\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "            if step % 1000 == 0:\n",
    "                with tf.device('/gpu:0'):\n",
    "                    _, _, _, summ = sess.run([train_op, global_step, metrics_op, summary_op])#,\\\n",
    "#                                                                               feed_dict={\n",
    "#                                                                                             X: batch_images,\n",
    "#                                                                                             Y: batch_labels\n",
    "#                                                                                         })   \n",
    "                sv.summary_computed(sess, summ)\n",
    "            else:\n",
    "                with tf.device('/gpu:0'):\n",
    "                    _, total_step, _, cur_loss, cur_acc, cur_lr = sess.run([train_op, global_step, metrics_op, total_loss, accuracy, lr])\n",
    "                time_elapsed = time.time() - start_time\n",
    "                if step % 10 == 0:\n",
    "                    final_loss = cur_loss\n",
    "                    final_accuracy = cur_acc\n",
    "                    tf_logging.info('Current Speed: {:f}sec/batch'.format(time_elapsed))\n",
    "                    tf_logging.info('Current Streaming Accuracy: {}'.format(cur_acc))\n",
    "                    tf_logging.info('Current Loss: {}'.format(cur_loss))\n",
    "                    tf_logging.info('Epoch %s/%s, Global Step: %s', int(step / num_steps_per_epoch + 1), NUM_EPOCHES, total_step)\n",
    "                    tf_logging.info('Current Learning Rate: {}'.format(cur_lr))\n",
    "        if training_state:\n",
    "            #We log the final training loss and accuracy\n",
    "            tf_logging.info('Final Loss: %s', final_loss)\n",
    "            tf_logging.info('Final Accuracy: %s', final_accuracy)\n",
    "            # Once all the training has been done, save the log files and checkpoint model\n",
    "            tf_logging.info('Finished training! Model saved.')\n",
    "        sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping_strings = tf.constant( [ str(key) for key in cvt_csv2tfrecord().keys() ] )\n",
    "#mapping_table = tf.contrib.lookup.index_table_from_tensor(mapping=mapping_strings, default_value=0)\n",
    "\n",
    "# with tf.device('/cpu:0'):\n",
    "#     example_train, product_train, category_train = input_pipeline(TRAIN_PATH + \"output_file*.tfrecords\", BATCH_SIZE, 4)\n",
    "#     example_test, product_test, category_test = input_pipeline(TEST_PATH + \"output_file*.tfrecords\", BATCH_SIZE, 4)\n",
    "\n",
    "# with tf.device('/gpu:0'):\n",
    "#     train_op, global_step, metrics_op, probabilities = buildClassificationNetwork(X, ids_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create the graph, etc.\n",
    "\n",
    "# # Create a session for running operations in the Graph.\n",
    "# sess = tf.Session()\n",
    "# # Initialize the variables (like the epoch counter).\n",
    "# sess.run(init_op)\n",
    "# # initialize local variables, like local counter epochs\n",
    "   \n",
    "    \n",
    "# # Start input enqueue threads.\n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "# try:\n",
    "#     while not coord.should_stop():\n",
    "#         #Check the time for each sess run\n",
    "#         start_time = time.time()\n",
    "#         total_loss, global_step_count, cur_acc, _, _ = sess.run([train_op, global_step, metrics_op, probabilities], feed_dict={\n",
    "#             X: example_train,\n",
    "#             Y: category_train\n",
    "#         })\n",
    "#         time_elapsed = time.time() - start_time\n",
    "\n",
    "#         #Run the logging to print some results\n",
    "#         logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
    "        \n",
    "# except tf.errors.OutOfRangeError:\n",
    "#     print('Done training -- epoch limit reached')\n",
    "# finally:\n",
    "#     # When done, ask the threads to stop.\n",
    "#     coord.request_stop()\n",
    "\n",
    "# # Wait for threads to finish.\n",
    "# coord.join(threads)\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # create a partition vector\n",
    "# partitions = [0] * len(all_filepaths)\n",
    "# partitions[:test_set_size] = [1] * test_set_size\n",
    "# random.shuffle(partitions)\n",
    "\n",
    "# # partition our data into a test and train set according to our partition vector\n",
    "# train_images, test_images = tf.dynamic_partition(all_images, partitions, 2)\n",
    "# train_labels, test_labels = tf.dynamic_partition(all_labels, partitions, 2)\n",
    "\n",
    "# # create input queues\n",
    "# train_input_queue = tf.train.slice_input_producer(\n",
    "#                                     [train_images, train_labels],\n",
    "#                                     shuffle=False)\n",
    "# test_input_queue = tf.train.slice_input_producer(\n",
    "#                                     [test_images, test_labels],\n",
    "#                                     shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
